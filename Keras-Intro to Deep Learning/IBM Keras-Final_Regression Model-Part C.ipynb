{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Keras - Final Assignment - Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('concrete_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks clean, with no null values. We have to predict data of concrete strength\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[df.columns != 'Strength']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Strength']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Neural Network\n",
    "\n",
    "def regression_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    #compile the model\n",
    "    model.compile(optimizer='adam', loss ='mean_squared_error')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "Since the data is clean we can go ahead and split the data in train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols = X_train.shape[1]\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (721, 8)\n",
      "y_train: (721,)\n",
      "X_test: (309, 8)\n",
      "y_test: (309,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Normalize the Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train - X_train.mean()) / X_train.std()\n",
    "#X_train_norm\n",
    "X_test_norm = (X_test - X_test.mean()) / X_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "model = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 160us/step - loss: 24.6843 - val_loss: 47.0173\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.7017 - val_loss: 46.9551\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6596 - val_loss: 47.0148\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.7269 - val_loss: 46.9363\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.7276 - val_loss: 47.0053\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6664 - val_loss: 47.0147\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6572 - val_loss: 47.0081\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6317 - val_loss: 47.0454\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.7064 - val_loss: 47.1646\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6414 - val_loss: 46.9545\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6830 - val_loss: 46.8486\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6526 - val_loss: 47.0051\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 24.7199 - val_loss: 47.1199\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.6559 - val_loss: 46.9596\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 124us/step - loss: 24.6531 - val_loss: 46.8913\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 109us/step - loss: 24.6422 - val_loss: 47.0436\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.6706 - val_loss: 46.9635\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.7275 - val_loss: 47.1085\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6455 - val_loss: 47.0812\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6666 - val_loss: 46.8521\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6831 - val_loss: 46.9140\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6596 - val_loss: 46.9419\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.6758 - val_loss: 47.0518\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 183us/step - loss: 24.6369 - val_loss: 47.0164\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 140us/step - loss: 24.6446 - val_loss: 47.0128\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 113us/step - loss: 24.6432 - val_loss: 47.1372\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.6954 - val_loss: 46.8428\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6511 - val_loss: 46.9575\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.7145 - val_loss: 47.2978\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6846 - val_loss: 47.0063\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6473 - val_loss: 46.9420\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6952 - val_loss: 46.9568\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6552 - val_loss: 46.9660\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6444 - val_loss: 46.9766\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.6970 - val_loss: 47.0496\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6965 - val_loss: 46.8830\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.7042 - val_loss: 46.9876\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.6237 - val_loss: 47.0297\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6611 - val_loss: 46.8991\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6518 - val_loss: 46.9409\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.6815 - val_loss: 47.0850\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6480 - val_loss: 47.0296\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.7035 - val_loss: 47.0095\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6525 - val_loss: 47.0005\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6456 - val_loss: 47.0938\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6621 - val_loss: 46.9837\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6487 - val_loss: 46.9178\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6789 - val_loss: 47.0126\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.6359 - val_loss: 47.1225\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6636 - val_loss: 46.9991\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 120us/step - loss: 24.6451 - val_loss: 47.0588\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6639 - val_loss: 46.9543\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 113us/step - loss: 24.6314 - val_loss: 47.0559\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 114us/step - loss: 24.6745 - val_loss: 47.0405\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6950 - val_loss: 46.8895\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 139us/step - loss: 24.6259 - val_loss: 47.0602\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 267us/step - loss: 24.6515 - val_loss: 47.1122\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 207us/step - loss: 24.6370 - val_loss: 47.0172\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 155us/step - loss: 24.6505 - val_loss: 46.9662\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6480 - val_loss: 46.8717\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6439 - val_loss: 47.1198\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6705 - val_loss: 47.0009\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6390 - val_loss: 47.0733\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 281us/step - loss: 24.6796 - val_loss: 47.0949\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 167us/step - loss: 24.7113 - val_loss: 46.8594\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6595 - val_loss: 47.0506\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6677 - val_loss: 46.9337\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.7004 - val_loss: 46.9143\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 111us/step - loss: 24.6647 - val_loss: 46.9577\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6808 - val_loss: 46.9995\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 119us/step - loss: 24.6661 - val_loss: 47.1159\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.6837 - val_loss: 46.9097\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6572 - val_loss: 47.0047\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.6605 - val_loss: 47.1325\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 107us/step - loss: 24.6693 - val_loss: 46.9160\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 127us/step - loss: 24.6549 - val_loss: 47.0087\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 125us/step - loss: 24.6659 - val_loss: 47.1573\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 173us/step - loss: 24.6613 - val_loss: 46.9596\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.6648 - val_loss: 46.9469\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.7023 - val_loss: 47.0909\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.6815 - val_loss: 46.9007\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.7356 - val_loss: 47.0501\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.6915 - val_loss: 47.0014\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6463 - val_loss: 47.1429\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6674 - val_loss: 46.8929\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.7171 - val_loss: 47.1771\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 138us/step - loss: 24.7098 - val_loss: 46.8151\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6753 - val_loss: 47.1127\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.6167 - val_loss: 47.0289\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.7192 - val_loss: 46.9412\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6430 - val_loss: 46.9699\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6607 - val_loss: 47.0892\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.7514 - val_loss: 46.8799\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6415 - val_loss: 46.9669\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6464 - val_loss: 47.1385\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6828 - val_loss: 47.0810\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6745 - val_loss: 46.8665\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6719 - val_loss: 47.1398\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6545 - val_loss: 47.0019\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6446 - val_loss: 47.0292\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "\n",
    "history = model.fit(X_train_norm, y_train, validation_split=0.2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model MSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on test data\n",
    "\n",
    "predict_yhat = model.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240.64137803146687"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, predict_yhat)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  1\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 155us/step - loss: 24.6655 - val_loss: 47.0396\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.6715 - val_loss: 46.8948\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.6449 - val_loss: 47.0024\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 113us/step - loss: 24.6523 - val_loss: 47.0494\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.6662 - val_loss: 46.9376\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6417 - val_loss: 47.0128\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6835 - val_loss: 46.9409\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6954 - val_loss: 47.1339\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.6408 - val_loss: 46.9945\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 115us/step - loss: 24.6961 - val_loss: 47.0995\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 123us/step - loss: 24.6568 - val_loss: 46.9588\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 173us/step - loss: 24.6419 - val_loss: 46.9392\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 159us/step - loss: 24.6469 - val_loss: 46.9878\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.6133 - val_loss: 46.9811\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.6579 - val_loss: 47.0729\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6684 - val_loss: 46.9659\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.6485 - val_loss: 47.0749\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 134us/step - loss: 24.6429 - val_loss: 46.9762\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 141us/step - loss: 24.6742 - val_loss: 46.9247\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 125us/step - loss: 24.6160 - val_loss: 46.9659\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.6368 - val_loss: 47.0619\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 107us/step - loss: 24.6648 - val_loss: 46.9612\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 119us/step - loss: 24.6748 - val_loss: 47.0335\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 135us/step - loss: 24.6612 - val_loss: 46.9484\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 142us/step - loss: 24.6848 - val_loss: 46.9590\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 138us/step - loss: 24.6400 - val_loss: 47.1208\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 274us/step - loss: 24.6612 - val_loss: 47.0881\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 137us/step - loss: 24.6553 - val_loss: 47.0343\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 109us/step - loss: 24.6520 - val_loss: 47.0417\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.6555 - val_loss: 47.0233\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 115us/step - loss: 24.6371 - val_loss: 46.9849\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 103us/step - loss: 24.6676 - val_loss: 47.0172\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.6571 - val_loss: 47.0600\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 107us/step - loss: 24.6417 - val_loss: 47.0442\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.6328 - val_loss: 47.0324\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 106us/step - loss: 24.6695 - val_loss: 46.9588\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.6937 - val_loss: 47.1204\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 103us/step - loss: 24.6344 - val_loss: 46.8446\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.6783 - val_loss: 46.9372\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 120us/step - loss: 24.6641 - val_loss: 47.0797\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 153us/step - loss: 24.6330 - val_loss: 47.0120\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 135us/step - loss: 24.6230 - val_loss: 46.9526\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 24.6300 - val_loss: 47.0542\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 24.6678 - val_loss: 47.0100\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.6510 - val_loss: 46.9779\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 115us/step - loss: 24.6273 - val_loss: 47.0122\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 114us/step - loss: 24.6174 - val_loss: 46.9524\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 114us/step - loss: 24.6377 - val_loss: 46.8726\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.6769 - val_loss: 47.0483\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 134us/step - loss: 24.6662 - val_loss: 46.8978\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 120us/step - loss: 24.6365 - val_loss: 46.9950\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 139us/step - loss: 24.6402 - val_loss: 47.1300\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.6685 - val_loss: 47.0353\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.6677 - val_loss: 46.9311\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 120us/step - loss: 24.6529 - val_loss: 46.9616\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 24.6189 - val_loss: 47.0719\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 123us/step - loss: 24.6351 - val_loss: 47.1247\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.6459 - val_loss: 46.9908\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 24.6821 - val_loss: 47.1083\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 130us/step - loss: 24.7077 - val_loss: 46.9337\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 161us/step - loss: 24.6326 - val_loss: 47.0475\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 136us/step - loss: 24.6386 - val_loss: 46.9814\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 143us/step - loss: 24.6345 - val_loss: 47.0608\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 207us/step - loss: 24.6285 - val_loss: 47.0578\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 152us/step - loss: 24.6640 - val_loss: 47.0516\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 151us/step - loss: 24.6260 - val_loss: 46.8956\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 279us/step - loss: 24.6297 - val_loss: 47.1329\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 176us/step - loss: 24.6542 - val_loss: 47.1130\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 135us/step - loss: 24.6610 - val_loss: 47.1417\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 142us/step - loss: 24.6830 - val_loss: 46.8468\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.6143 - val_loss: 46.9784\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.6233 - val_loss: 47.0345\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.6449 - val_loss: 46.9990\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.6464 - val_loss: 47.0431\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 358us/step - loss: 24.6299 - val_loss: 47.0565\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 299us/step - loss: 24.6814 - val_loss: 46.8988\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.6502 - val_loss: 46.9942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.6203 - val_loss: 46.9334\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6149 - val_loss: 47.0395\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6358 - val_loss: 46.9885\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.6208 - val_loss: 46.9756\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6506 - val_loss: 47.0490\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 24.6667 - val_loss: 46.9968\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 285us/step - loss: 24.6416 - val_loss: 47.0270\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 390us/step - loss: 24.6379 - val_loss: 47.0487\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 124us/step - loss: 24.6290 - val_loss: 46.8707\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.6305 - val_loss: 47.0910\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.6499 - val_loss: 47.0175\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6460 - val_loss: 47.1302\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 138us/step - loss: 24.6552 - val_loss: 47.0949\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.6094 - val_loss: 47.0800\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6370 - val_loss: 46.9821\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6600 - val_loss: 46.9048\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 24.5984 - val_loss: 47.0909\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.6474 - val_loss: 47.1265\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 125us/step - loss: 24.6718 - val_loss: 46.9956\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.6512 - val_loss: 46.9568\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6778 - val_loss: 46.7990\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.6376 - val_loss: 47.1048\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6812 - val_loss: 47.1312\n",
      "\n",
      "Mean Squared Error for iteration1: 45.679677916773144\n",
      "\n",
      "Iteration:  2\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6097 - val_loss: 47.0750\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6486 - val_loss: 46.9814\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6410 - val_loss: 46.9708\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6937 - val_loss: 46.9659\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6432 - val_loss: 47.0412\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6490 - val_loss: 47.1076\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6799 - val_loss: 47.0343\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6674 - val_loss: 46.9886\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.7273 - val_loss: 47.2049\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6384 - val_loss: 46.9196\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6398 - val_loss: 46.9636\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6351 - val_loss: 46.9938\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6394 - val_loss: 47.0771\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6648 - val_loss: 46.9763\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6743 - val_loss: 46.8755\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6074 - val_loss: 47.0112\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6439 - val_loss: 47.1174\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6567 - val_loss: 46.9511\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.6298 - val_loss: 47.0704\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6803 - val_loss: 46.8819\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6639 - val_loss: 47.1613\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6246 - val_loss: 47.0964\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6296 - val_loss: 47.1415\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6561 - val_loss: 47.0958\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6789 - val_loss: 46.9852\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6287 - val_loss: 47.0174\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6478 - val_loss: 47.0835\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 610us/step - loss: 24.6525 - val_loss: 47.0147\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6038 - val_loss: 47.0423\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6292 - val_loss: 47.0287\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6890 - val_loss: 47.1032\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6535 - val_loss: 46.8788\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6616 - val_loss: 46.9850\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6105 - val_loss: 47.0222\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6280 - val_loss: 47.0911\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6390 - val_loss: 47.1154\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6335 - val_loss: 47.1159\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6175 - val_loss: 47.0720\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6545 - val_loss: 47.0028\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6360 - val_loss: 47.0265\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6320 - val_loss: 47.0810\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6356 - val_loss: 47.0153\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6562 - val_loss: 47.1422\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6346 - val_loss: 47.0521\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6547 - val_loss: 46.9830\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.6190 - val_loss: 47.1569\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.6300 - val_loss: 47.0670\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.6252 - val_loss: 47.0554\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.6069 - val_loss: 46.9386\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.6410 - val_loss: 46.9498\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6336 - val_loss: 47.1088\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6166 - val_loss: 47.0940\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6200 - val_loss: 47.0860\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6298 - val_loss: 46.9985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6245 - val_loss: 46.9876\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6317 - val_loss: 47.1110\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6244 - val_loss: 47.0135\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6891 - val_loss: 47.1869\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5986 - val_loss: 46.9841\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6383 - val_loss: 46.9456\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6354 - val_loss: 47.1325\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6149 - val_loss: 47.0432\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6328 - val_loss: 47.0648\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5958 - val_loss: 47.0833\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6336 - val_loss: 47.1288\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 569us/step - loss: 24.6195 - val_loss: 46.9844\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 376us/step - loss: 24.6174 - val_loss: 46.9970\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6562 - val_loss: 47.1793\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6484 - val_loss: 47.1168\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6325 - val_loss: 46.8892\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6487 - val_loss: 47.0289\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6549 - val_loss: 47.0097\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6067 - val_loss: 47.1364\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6651 - val_loss: 47.1011\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6546 - val_loss: 46.9900\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6709 - val_loss: 46.9981\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6233 - val_loss: 47.0704\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6136 - val_loss: 46.9786\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.6398 - val_loss: 47.1241\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6241 - val_loss: 47.0411\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.6737 - val_loss: 47.1361\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 103us/step - loss: 24.6126 - val_loss: 46.9861\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.6308 - val_loss: 47.0708\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.7129 - val_loss: 47.0048\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6393 - val_loss: 46.9184\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.7081 - val_loss: 47.1775\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6465 - val_loss: 46.8924\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6548 - val_loss: 47.1383\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6295 - val_loss: 47.0867\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6739 - val_loss: 46.8390\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6335 - val_loss: 47.1552\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6590 - val_loss: 47.2025\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6332 - val_loss: 47.1434\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6326 - val_loss: 47.0141\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6334 - val_loss: 47.0021\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6281 - val_loss: 47.1132\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6208 - val_loss: 47.0744\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6313 - val_loss: 47.0402\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6487 - val_loss: 46.9375\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6090 - val_loss: 46.9865\n",
      "\n",
      "Mean Squared Error for iteration2: 45.837371454364906\n",
      "\n",
      "Iteration:  3\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.6137 - val_loss: 47.1358\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.6661 - val_loss: 46.9552\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6002 - val_loss: 46.9392\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5978 - val_loss: 47.0619\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6068 - val_loss: 47.0515\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6130 - val_loss: 47.0582\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6051 - val_loss: 47.0739\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6300 - val_loss: 46.9560\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.7054 - val_loss: 47.2727\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6313 - val_loss: 47.0017\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.7040 - val_loss: 47.2584\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6129 - val_loss: 46.9916\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6171 - val_loss: 46.9617\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6505 - val_loss: 47.0298\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6038 - val_loss: 47.0979\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6323 - val_loss: 47.1003\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5912 - val_loss: 47.0595\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6264 - val_loss: 47.0556\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.6415 - val_loss: 46.9262\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6581 - val_loss: 46.9587\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - ETA: 0s - loss: 24.45 - 0s 152us/step - loss: 24.5937 - val_loss: 47.0740\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 289us/step - loss: 24.6425 - val_loss: 47.2351\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 145us/step - loss: 24.7228 - val_loss: 46.8954\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5932 - val_loss: 47.0933\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6560 - val_loss: 46.9376\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6064 - val_loss: 47.0452\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6058 - val_loss: 47.1190\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6607 - val_loss: 47.2175\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5983 - val_loss: 46.9227\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6218 - val_loss: 47.0182\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.6169 - val_loss: 46.9537\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6151 - val_loss: 47.0494\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6587 - val_loss: 47.1624\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6934 - val_loss: 46.9760\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.5993 - val_loss: 47.1201\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.6432 - val_loss: 47.1742\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.5857 - val_loss: 47.0502\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5986 - val_loss: 47.0565\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.5880 - val_loss: 46.9889\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5879 - val_loss: 47.0948\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6247 - val_loss: 47.1640\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6020 - val_loss: 47.1088\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6566 - val_loss: 46.9046\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6746 - val_loss: 47.1657\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5922 - val_loss: 46.9859\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5916 - val_loss: 47.0368\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6634 - val_loss: 47.0067\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6724 - val_loss: 46.9057\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5989 - val_loss: 47.1148\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6004 - val_loss: 47.1117\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6179 - val_loss: 47.1625\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5903 - val_loss: 47.0693\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6328 - val_loss: 47.0905\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6601 - val_loss: 47.0390\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6146 - val_loss: 47.1125\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6241 - val_loss: 47.1274\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6349 - val_loss: 46.9549\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6088 - val_loss: 46.9998\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5803 - val_loss: 47.0657\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5873 - val_loss: 47.0817\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5941 - val_loss: 47.0682\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6147 - val_loss: 46.9593\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6546 - val_loss: 47.1466\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6635 - val_loss: 47.2130\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6541 - val_loss: 47.0401\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5933 - val_loss: 46.9987\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6344 - val_loss: 47.0595\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6793 - val_loss: 46.9882\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6063 - val_loss: 47.0847\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5897 - val_loss: 47.0828\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5961 - val_loss: 47.0998\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6220 - val_loss: 46.9999\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6473 - val_loss: 47.1628\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5770 - val_loss: 47.0015\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6442 - val_loss: 46.9574\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6075 - val_loss: 47.1355\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6094 - val_loss: 47.0025\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6407 - val_loss: 47.0559\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6108 - val_loss: 47.0444\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6031 - val_loss: 47.0600\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6124 - val_loss: 47.0822\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6105 - val_loss: 47.0000\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5886 - val_loss: 47.0946\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6076 - val_loss: 47.1441\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.6102 - val_loss: 46.9560\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.6613 - val_loss: 46.9931\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6557 - val_loss: 47.2772\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.6492 - val_loss: 47.1132\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6209 - val_loss: 47.0256\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.5786 - val_loss: 47.0375\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5918 - val_loss: 47.0345\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5859 - val_loss: 47.1992\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5838 - val_loss: 47.1635\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5769 - val_loss: 47.0667\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6021 - val_loss: 46.9003\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5751 - val_loss: 47.0362\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5841 - val_loss: 47.1207\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6330 - val_loss: 47.0402\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5825 - val_loss: 47.0726\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5930 - val_loss: 47.0774\n",
      "\n",
      "Mean Squared Error for iteration3: 45.511435197922566\n",
      "\n",
      "Iteration:  4\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6667 - val_loss: 46.9347\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 177us/step - loss: 24.6222 - val_loss: 47.2276\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 170us/step - loss: 24.6380 - val_loss: 46.9669\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 275us/step - loss: 24.6148 - val_loss: 47.1072\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 319us/step - loss: 24.6151 - val_loss: 47.0381\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5948 - val_loss: 47.1059\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6260 - val_loss: 47.0840\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 88us/step - loss: 24.5975 - val_loss: 47.0621\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5820 - val_loss: 47.0707\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.6479 - val_loss: 47.2395\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 261us/step - loss: 24.6154 - val_loss: 47.0175\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 265us/step - loss: 24.6353 - val_loss: 47.0026\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.5820 - val_loss: 47.0373\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.6096 - val_loss: 47.0900\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.6224 - val_loss: 46.9521\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5767 - val_loss: 47.0929\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5914 - val_loss: 47.1321\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.5698 - val_loss: 47.0302\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.6164 - val_loss: 47.1569\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5817 - val_loss: 47.0357\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5857 - val_loss: 47.1038\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.6113 - val_loss: 47.0138\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5895 - val_loss: 46.9060\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.6049 - val_loss: 47.0111\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.5886 - val_loss: 47.0975\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.6228 - val_loss: 47.0957\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5741 - val_loss: 47.0541\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5930 - val_loss: 47.1123\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6266 - val_loss: 47.1807\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5937 - val_loss: 47.0247\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5776 - val_loss: 46.9360\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5748 - val_loss: 46.9576\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5978 - val_loss: 47.1700\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5648 - val_loss: 47.0245\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6427 - val_loss: 46.8888\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6245 - val_loss: 47.1643\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5854 - val_loss: 47.0598\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5967 - val_loss: 47.1847\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6087 - val_loss: 46.9895\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6057 - val_loss: 46.9326\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5824 - val_loss: 47.0489\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5904 - val_loss: 47.2477\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5784 - val_loss: 47.0655\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5774 - val_loss: 47.0454\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5825 - val_loss: 47.1505\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5765 - val_loss: 47.0892\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.6322 - val_loss: 47.0983\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6438 - val_loss: 47.1009\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 133us/step - loss: 24.5901 - val_loss: 47.0747\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 635us/step - loss: 24.5761 - val_loss: 47.0436\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 108us/step - loss: 24.7214 - val_loss: 47.2038\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5765 - val_loss: 46.9759\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5816 - val_loss: 46.9876\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6211 - val_loss: 47.0843\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5637 - val_loss: 46.9564\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5753 - val_loss: 47.1321\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5650 - val_loss: 47.1143\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6123 - val_loss: 46.9578\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6536 - val_loss: 47.3663\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6297 - val_loss: 46.9588\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 152us/step - loss: 24.6054 - val_loss: 46.9737\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.5799 - val_loss: 47.1446\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5820 - val_loss: 47.0510\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5974 - val_loss: 47.2025\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5810 - val_loss: 47.0651\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5860 - val_loss: 47.0720\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5919 - val_loss: 47.0657\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6147 - val_loss: 47.0805\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5784 - val_loss: 47.0235\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 161us/step - loss: 24.6091 - val_loss: 47.1827\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 274us/step - loss: 24.6075 - val_loss: 47.0888\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 500us/step - loss: 24.5508 - val_loss: 47.0689\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.6266 - val_loss: 47.2209\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5929 - val_loss: 47.1992\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5643 - val_loss: 46.9885\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6028 - val_loss: 47.0899\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6401 - val_loss: 47.1390\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5835 - val_loss: 47.0037\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5807 - val_loss: 47.1202\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6416 - val_loss: 47.0566\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6148 - val_loss: 47.1775\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5944 - val_loss: 47.0932\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5700 - val_loss: 47.2007\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 132us/step - loss: 24.5512 - val_loss: 47.0521\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.5554 - val_loss: 46.9770\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 88us/step - loss: 24.6255 - val_loss: 47.1405\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5803 - val_loss: 47.0131\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.6042 - val_loss: 47.0396\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5853 - val_loss: 47.1418\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6455 - val_loss: 47.1323\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5821 - val_loss: 46.9515\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5861 - val_loss: 47.1449\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.5741 - val_loss: 46.9726\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5843 - val_loss: 47.0950\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5808 - val_loss: 47.1708\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5777 - val_loss: 47.0285\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5733 - val_loss: 47.1235\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5698 - val_loss: 47.0549\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5512 - val_loss: 47.1652\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5635 - val_loss: 47.0699\n",
      "\n",
      "Mean Squared Error for iteration4: 45.5312667950636\n",
      "\n",
      "Iteration:  5\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5655 - val_loss: 47.1068\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6017 - val_loss: 47.1496\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5723 - val_loss: 47.0454\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6073 - val_loss: 46.9927\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5847 - val_loss: 47.2462\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5568 - val_loss: 47.1237\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5845 - val_loss: 47.0429\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6293 - val_loss: 46.9467\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5579 - val_loss: 47.1374\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6075 - val_loss: 47.1692\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5936 - val_loss: 47.2244\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5587 - val_loss: 47.0825\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5842 - val_loss: 47.0175\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.5607 - val_loss: 47.0771\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6103 - val_loss: 47.2519\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.5554 - val_loss: 47.0873\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5892 - val_loss: 47.0044\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6162 - val_loss: 47.1408\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5705 - val_loss: 47.0596\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5722 - val_loss: 47.1824\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5823 - val_loss: 47.1470\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5918 - val_loss: 47.2533\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5512 - val_loss: 47.1016\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5659 - val_loss: 47.0612\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5590 - val_loss: 47.0627\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5980 - val_loss: 47.0382\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5927 - val_loss: 47.0776\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.5716 - val_loss: 47.1469\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5803 - val_loss: 47.0524\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6088 - val_loss: 47.0387\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5820 - val_loss: 47.1885\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5751 - val_loss: 47.0587\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6316 - val_loss: 47.3051\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6317 - val_loss: 46.8653\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5714 - val_loss: 47.0317\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5603 - val_loss: 47.2163\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5857 - val_loss: 47.1934\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.6098 - val_loss: 47.0434\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.6036 - val_loss: 47.1075\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6017 - val_loss: 47.2540\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6054 - val_loss: 47.0828\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5736 - val_loss: 47.0615\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5965 - val_loss: 47.2089\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5740 - val_loss: 47.1185\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6138 - val_loss: 47.0114\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5504 - val_loss: 47.2646\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5778 - val_loss: 47.1487\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5623 - val_loss: 47.1853\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6050 - val_loss: 47.0790\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.6012 - val_loss: 47.1901\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5353 - val_loss: 47.1706\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5479 - val_loss: 47.0617\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5924 - val_loss: 46.9786\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.6077 - val_loss: 47.2661\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5852 - val_loss: 47.2780\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5606 - val_loss: 47.1490\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5648 - val_loss: 47.1638\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5524 - val_loss: 47.0231\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6027 - val_loss: 47.3108\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.5993 - val_loss: 47.2623\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5535 - val_loss: 47.1271\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5577 - val_loss: 47.0370\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.6078 - val_loss: 47.2003\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5608 - val_loss: 47.0423\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5683 - val_loss: 46.9886\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5864 - val_loss: 47.1271\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5998 - val_loss: 47.3213\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5810 - val_loss: 47.2095\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5643 - val_loss: 47.0741\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5505 - val_loss: 47.1922\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5639 - val_loss: 47.0346\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.6073 - val_loss: 47.0650\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.5859 - val_loss: 47.2563\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.5730 - val_loss: 47.1249\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5503 - val_loss: 47.2143\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5568 - val_loss: 47.0926\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5813 - val_loss: 47.2436\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5522 - val_loss: 47.2793\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5651 - val_loss: 47.2747\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6096 - val_loss: 47.1382\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5626 - val_loss: 47.1138\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5952 - val_loss: 47.3265\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5703 - val_loss: 47.3662\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.5323 - val_loss: 47.2235\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5450 - val_loss: 47.1117\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5547 - val_loss: 47.0374\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5385 - val_loss: 47.2069\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5600 - val_loss: 47.1550\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5792 - val_loss: 47.1505\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5351 - val_loss: 47.1845\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5508 - val_loss: 47.1858\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5543 - val_loss: 47.2197\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5785 - val_loss: 47.3328\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5487 - val_loss: 47.2014\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.6202 - val_loss: 47.0312\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5568 - val_loss: 47.2558\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5335 - val_loss: 47.1507\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5578 - val_loss: 47.2582\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5869 - val_loss: 47.0700\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5532 - val_loss: 47.2182\n",
      "\n",
      "Mean Squared Error for iteration5: 45.55753632031349\n",
      "\n",
      "Iteration:  6\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5355 - val_loss: 47.2653\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5576 - val_loss: 47.1503\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5882 - val_loss: 47.1218\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5257 - val_loss: 47.2614\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5384 - val_loss: 47.2186\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5863 - val_loss: 47.1984\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5462 - val_loss: 47.2459\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5361 - val_loss: 47.1782\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6002 - val_loss: 47.2860\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5531 - val_loss: 47.1185\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5696 - val_loss: 47.1947\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5362 - val_loss: 47.2481\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5571 - val_loss: 47.2361\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6203 - val_loss: 47.2793\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5589 - val_loss: 47.2006\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5603 - val_loss: 47.1429\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5607 - val_loss: 47.2387\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.5434 - val_loss: 47.2662\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5686 - val_loss: 47.2524\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5977 - val_loss: 47.1572\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5660 - val_loss: 47.1434\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5478 - val_loss: 47.2945\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5689 - val_loss: 47.3318\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5699 - val_loss: 47.1525\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5533 - val_loss: 47.2353\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5543 - val_loss: 47.2366\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.6013 - val_loss: 47.1382\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.5680 - val_loss: 47.2396\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.5697 - val_loss: 47.2636\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5783 - val_loss: 47.2511\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5318 - val_loss: 47.2230\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5483 - val_loss: 47.2567\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5828 - val_loss: 47.3260\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5683 - val_loss: 47.3600\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5528 - val_loss: 47.2442\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5643 - val_loss: 47.1345\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5516 - val_loss: 47.1368\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5412 - val_loss: 47.3432\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5438 - val_loss: 47.3479\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.5416 - val_loss: 47.1647\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5389 - val_loss: 47.2276\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5421 - val_loss: 47.2624\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5819 - val_loss: 47.3088\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5361 - val_loss: 47.2610\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5865 - val_loss: 47.3752\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6089 - val_loss: 47.1018\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5652 - val_loss: 47.3108\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5729 - val_loss: 47.3313\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5729 - val_loss: 47.2968\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5612 - val_loss: 47.2027\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5916 - val_loss: 47.3597\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5394 - val_loss: 47.1821\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5625 - val_loss: 47.3307\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5784 - val_loss: 47.3702\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5658 - val_loss: 47.1970\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5400 - val_loss: 47.2142\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5581 - val_loss: 47.3684\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5206 - val_loss: 47.2581\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5458 - val_loss: 47.3441\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5548 - val_loss: 47.3128\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5358 - val_loss: 47.2734\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5470 - val_loss: 47.2673\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5643 - val_loss: 47.1717\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.5225 - val_loss: 47.2394\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5699 - val_loss: 47.2437\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5909 - val_loss: 47.4875\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5737 - val_loss: 47.2153\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6091 - val_loss: 47.3687\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5343 - val_loss: 47.2564\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5560 - val_loss: 47.1272\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5436 - val_loss: 47.3453\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6006 - val_loss: 47.3621\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.5715 - val_loss: 47.1311\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.5541 - val_loss: 47.3302\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.6122 - val_loss: 47.1648\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5596 - val_loss: 47.3282\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5512 - val_loss: 47.1247\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5274 - val_loss: 47.2568\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5574 - val_loss: 47.3270\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5366 - val_loss: 47.3272\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5594 - val_loss: 47.2790\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5095 - val_loss: 47.2676\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5472 - val_loss: 47.2174\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5911 - val_loss: 47.3989\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5772 - val_loss: 47.2137\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5368 - val_loss: 47.4176\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5672 - val_loss: 47.4475\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5424 - val_loss: 47.3178\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5864 - val_loss: 47.1497\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5927 - val_loss: 47.2318\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5112 - val_loss: 47.3313\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5518 - val_loss: 47.2011\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5344 - val_loss: 47.3076\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5340 - val_loss: 47.4341\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5533 - val_loss: 47.2304\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5505 - val_loss: 47.2173\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5674 - val_loss: 47.2744\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5514 - val_loss: 47.3631\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5118 - val_loss: 47.2142\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5382 - val_loss: 47.2199\n",
      "\n",
      "Mean Squared Error for iteration6: 45.41896427406756\n",
      "\n",
      "Iteration:  7\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5924 - val_loss: 47.4016\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5229 - val_loss: 47.3372\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6092 - val_loss: 47.2117\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5364 - val_loss: 47.3808\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5414 - val_loss: 47.3397\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5922 - val_loss: 47.2416\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5731 - val_loss: 47.3482\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.5384 - val_loss: 47.4162\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5956 - val_loss: 47.0822\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5093 - val_loss: 47.3116\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5336 - val_loss: 47.3724\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5967 - val_loss: 47.2663\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5317 - val_loss: 47.4135\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5562 - val_loss: 47.2462\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5718 - val_loss: 47.4397\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5385 - val_loss: 47.2754\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.5483 - val_loss: 47.2807\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.5293 - val_loss: 47.3696\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4996 - val_loss: 47.3269\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5670 - val_loss: 47.2469\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5298 - val_loss: 47.3713\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5233 - val_loss: 47.3328\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5439 - val_loss: 47.3750\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5275 - val_loss: 47.2306\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5225 - val_loss: 47.3115\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5665 - val_loss: 47.4020\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5291 - val_loss: 47.3097\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5724 - val_loss: 47.1278\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5600 - val_loss: 47.2456\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5800 - val_loss: 47.5313\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4982 - val_loss: 47.2935\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.6377 - val_loss: 47.3379\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5608 - val_loss: 47.2050\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5354 - val_loss: 47.2264\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5465 - val_loss: 47.4126\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5502 - val_loss: 47.4973\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4951 - val_loss: 47.2914\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5153 - val_loss: 47.3051\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5492 - val_loss: 47.2124\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5584 - val_loss: 47.4036\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5091 - val_loss: 47.2964\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5544 - val_loss: 47.3231\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5586 - val_loss: 47.3680\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5394 - val_loss: 47.3463\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5412 - val_loss: 47.3266\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5182 - val_loss: 47.3474\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5250 - val_loss: 47.3042\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5711 - val_loss: 47.5180\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5208 - val_loss: 47.3625\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5606 - val_loss: 47.1740\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5644 - val_loss: 47.5132\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5692 - val_loss: 47.2315\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.5477 - val_loss: 47.2708\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.5005 - val_loss: 47.3832\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5055 - val_loss: 47.4068\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.5083 - val_loss: 47.4627\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5179 - val_loss: 47.2642\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5077 - val_loss: 47.3274\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5376 - val_loss: 47.3409\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5274 - val_loss: 47.4653\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5828 - val_loss: 47.2990\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5446 - val_loss: 47.5399\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5246 - val_loss: 47.3281\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5132 - val_loss: 47.4267\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5451 - val_loss: 47.4222\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5662 - val_loss: 47.2833\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5197 - val_loss: 47.3556\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5689 - val_loss: 47.3571\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5235 - val_loss: 47.2102\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5296 - val_loss: 47.3514\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5237 - val_loss: 47.3218\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5546 - val_loss: 47.4253\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5210 - val_loss: 47.4323\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5786 - val_loss: 47.3813\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5445 - val_loss: 47.4674\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5548 - val_loss: 47.3492\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5436 - val_loss: 47.3383\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5086 - val_loss: 47.4278\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5671 - val_loss: 47.5398\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5101 - val_loss: 47.2328\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4926 - val_loss: 47.2372\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5354 - val_loss: 47.2722\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5166 - val_loss: 47.3500\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.5202 - val_loss: 47.4414\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5341 - val_loss: 47.4343\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4949 - val_loss: 47.2917\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5059 - val_loss: 47.3483\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5347 - val_loss: 47.3277\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5268 - val_loss: 47.4470\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5092 - val_loss: 47.3093\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5152 - val_loss: 47.2829\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5419 - val_loss: 47.4297\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5742 - val_loss: 47.4856\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5190 - val_loss: 47.4590\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.5247 - val_loss: 47.3563\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4997 - val_loss: 47.3269\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 106us/step - loss: 24.5743 - val_loss: 47.1854\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5047 - val_loss: 47.3652\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5411 - val_loss: 47.4185\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5253 - val_loss: 47.2804\n",
      "\n",
      "Mean Squared Error for iteration7: 45.391878715573064\n",
      "\n",
      "Iteration:  8\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5518 - val_loss: 47.3828\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5207 - val_loss: 47.4130\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5395 - val_loss: 47.3403\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4954 - val_loss: 47.3410\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5452 - val_loss: 47.4684\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5011 - val_loss: 47.3645\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5321 - val_loss: 47.3164\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5625 - val_loss: 47.3268\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5311 - val_loss: 47.4145\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5299 - val_loss: 47.2762\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5029 - val_loss: 47.4191\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5363 - val_loss: 47.5252\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5233 - val_loss: 47.4039\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5400 - val_loss: 47.2810\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5186 - val_loss: 47.4241\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5364 - val_loss: 47.3310\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5267 - val_loss: 47.3845\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5129 - val_loss: 47.4515\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4917 - val_loss: 47.3997\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5603 - val_loss: 47.4751\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5666 - val_loss: 47.2583\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5097 - val_loss: 47.3383\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5551 - val_loss: 47.5205\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5149 - val_loss: 47.3792\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5211 - val_loss: 47.2202\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5480 - val_loss: 47.3896\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5246 - val_loss: 47.3886\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5623 - val_loss: 47.4223\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5474 - val_loss: 47.3572\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4971 - val_loss: 47.3875\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5605 - val_loss: 47.4413\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5204 - val_loss: 47.3516\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5501 - val_loss: 47.4035\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5185 - val_loss: 47.4898\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5288 - val_loss: 47.4648\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5349 - val_loss: 47.3114\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5034 - val_loss: 47.3584\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5353 - val_loss: 47.3278\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.6050 - val_loss: 47.4313\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5126 - val_loss: 47.2179\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5235 - val_loss: 47.3870\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5268 - val_loss: 47.3479\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 111us/step - loss: 24.5559 - val_loss: 47.5830\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4975 - val_loss: 47.3541\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5156 - val_loss: 47.2373\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5174 - val_loss: 47.4128\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5058 - val_loss: 47.3586\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.5218 - val_loss: 47.3572\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5354 - val_loss: 47.2994\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5130 - val_loss: 47.3426\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5217 - val_loss: 47.4031\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5389 - val_loss: 47.3765\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.5278 - val_loss: 47.4104\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5545 - val_loss: 47.3095\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5230 - val_loss: 47.5786\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.5007 - val_loss: 47.4451\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5198 - val_loss: 47.3146\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5310 - val_loss: 47.4309\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5124 - val_loss: 47.4472\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5083 - val_loss: 47.2588\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5283 - val_loss: 47.2346\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5236 - val_loss: 47.5067\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5086 - val_loss: 47.3258\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4856 - val_loss: 47.3527\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5456 - val_loss: 47.3581\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4994 - val_loss: 47.3255\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5145 - val_loss: 47.4717\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5146 - val_loss: 47.3750\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5424 - val_loss: 47.4771\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5297 - val_loss: 47.4474\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5380 - val_loss: 47.3386\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.5514 - val_loss: 47.3277\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5313 - val_loss: 47.4546\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5295 - val_loss: 47.2157\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.5129 - val_loss: 47.4243\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5024 - val_loss: 47.5014\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5205 - val_loss: 47.2832\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4987 - val_loss: 47.3878\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4920 - val_loss: 47.4060\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4934 - val_loss: 47.4312\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5275 - val_loss: 47.4148\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5284 - val_loss: 47.2932\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5395 - val_loss: 47.5432\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5327 - val_loss: 47.3421\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5002 - val_loss: 47.3216\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4890 - val_loss: 47.4247\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.5191 - val_loss: 47.2679\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.4820 - val_loss: 47.4107\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4956 - val_loss: 47.4654\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5375 - val_loss: 47.3723\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5734 - val_loss: 47.4813\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5412 - val_loss: 47.3574\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5370 - val_loss: 47.4543\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4973 - val_loss: 47.3371\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5231 - val_loss: 47.4281\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5241 - val_loss: 47.3979\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4951 - val_loss: 47.2678\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.4674 - val_loss: 47.4099\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4894 - val_loss: 47.4105\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5414 - val_loss: 47.3195\n",
      "\n",
      "Mean Squared Error for iteration8: 45.42809395954863\n",
      "\n",
      "Iteration:  9\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.5037 - val_loss: 47.4443\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5318 - val_loss: 47.4259\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4812 - val_loss: 47.2669\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5277 - val_loss: 47.4361\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4840 - val_loss: 47.4155\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5352 - val_loss: 47.4323\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5410 - val_loss: 47.3419\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5123 - val_loss: 47.2827\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5080 - val_loss: 47.5001\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5222 - val_loss: 47.4696\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5449 - val_loss: 47.4567\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4931 - val_loss: 47.3299\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4789 - val_loss: 47.3508\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5014 - val_loss: 47.3332\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5114 - val_loss: 47.3940\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5268 - val_loss: 47.5458\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4974 - val_loss: 47.3489\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5203 - val_loss: 47.3392\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.5108 - val_loss: 47.3258\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5192 - val_loss: 47.5185\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5526 - val_loss: 47.4114\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5129 - val_loss: 47.4555\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.6467 - val_loss: 47.3033\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4793 - val_loss: 47.4431\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5082 - val_loss: 47.4595\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4878 - val_loss: 47.3870\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4971 - val_loss: 47.3909\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4992 - val_loss: 47.3640\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5042 - val_loss: 47.3540\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4944 - val_loss: 47.2964\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4982 - val_loss: 47.3692\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.5035 - val_loss: 47.5212\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4797 - val_loss: 47.3519\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5025 - val_loss: 47.3455\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4998 - val_loss: 47.4031\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4827 - val_loss: 47.4087\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5799 - val_loss: 47.5290\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5005 - val_loss: 47.3687\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5032 - val_loss: 47.5194\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5109 - val_loss: 47.2931\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5380 - val_loss: 47.4064\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5290 - val_loss: 47.3675\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.5054 - val_loss: 47.4719\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5070 - val_loss: 47.3932\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5170 - val_loss: 47.3475\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5100 - val_loss: 47.4341\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4842 - val_loss: 47.4116\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4896 - val_loss: 47.4017\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 72us/step - loss: 24.4997 - val_loss: 47.3792\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5082 - val_loss: 47.4191\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4986 - val_loss: 47.3323\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.6305 - val_loss: 47.6173\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5225 - val_loss: 47.2313\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5378 - val_loss: 47.3996\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5157 - val_loss: 47.3412\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5226 - val_loss: 47.4419\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5652 - val_loss: 47.3170\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4806 - val_loss: 47.4452\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5339 - val_loss: 47.5927\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5076 - val_loss: 47.3853\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.4624 - val_loss: 47.2666\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5459 - val_loss: 47.1296\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5769 - val_loss: 47.5881\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5065 - val_loss: 47.2608\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5634 - val_loss: 47.5097\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5200 - val_loss: 47.3293\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4954 - val_loss: 47.3588\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5224 - val_loss: 47.4721\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4930 - val_loss: 47.3370\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4893 - val_loss: 47.2949\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4962 - val_loss: 47.4827\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4887 - val_loss: 47.3381\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4977 - val_loss: 47.3777\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5187 - val_loss: 47.4995\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5288 - val_loss: 47.2653\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5066 - val_loss: 47.3517\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5319 - val_loss: 47.4354\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 103us/step - loss: 24.5172 - val_loss: 47.4328\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5256 - val_loss: 47.4744\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5221 - val_loss: 47.4306\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5336 - val_loss: 47.4616\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5139 - val_loss: 47.3750\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4931 - val_loss: 47.3645\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4891 - val_loss: 47.4859\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5096 - val_loss: 47.3441\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5143 - val_loss: 47.5378\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4639 - val_loss: 47.3797\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5063 - val_loss: 47.4014\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.5587 - val_loss: 47.2766\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4908 - val_loss: 47.3662\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5110 - val_loss: 47.5461\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5006 - val_loss: 47.3843\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4932 - val_loss: 47.3180\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4812 - val_loss: 47.4693\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4802 - val_loss: 47.4833\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4996 - val_loss: 47.3707\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5290 - val_loss: 47.3830\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4599 - val_loss: 47.3576\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4776 - val_loss: 47.4189\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5114 - val_loss: 47.3238\n",
      "\n",
      "Mean Squared Error for iteration9: 45.35457627030926\n",
      "\n",
      "Iteration:  10\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.5214 - val_loss: 47.3570\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5625 - val_loss: 47.5041\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5263 - val_loss: 47.4923\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5531 - val_loss: 47.2682\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4845 - val_loss: 47.4165\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5368 - val_loss: 47.2557\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4913 - val_loss: 47.4789\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.5280 - val_loss: 47.3982\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4713 - val_loss: 47.5085\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5256 - val_loss: 47.3312\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.4819 - val_loss: 47.3982\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5016 - val_loss: 47.4018\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5560 - val_loss: 47.4259\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5205 - val_loss: 47.2776\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5060 - val_loss: 47.4990\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4859 - val_loss: 47.3937\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5141 - val_loss: 47.3567\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5468 - val_loss: 47.5350\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5351 - val_loss: 47.1958\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5055 - val_loss: 47.2717\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4751 - val_loss: 47.3883\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4892 - val_loss: 47.3259\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.4858 - val_loss: 47.3827\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.5018 - val_loss: 47.3459\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4617 - val_loss: 47.3288\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 76us/step - loss: 24.5244 - val_loss: 47.5055\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5148 - val_loss: 47.4370\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4889 - val_loss: 47.3413\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5100 - val_loss: 47.5300\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4789 - val_loss: 47.4165\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4828 - val_loss: 47.4995\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5389 - val_loss: 47.4097\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4863 - val_loss: 47.3629\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5050 - val_loss: 47.3297\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4925 - val_loss: 47.3051\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4898 - val_loss: 47.3274\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5104 - val_loss: 47.2360\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4894 - val_loss: 47.4214\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5053 - val_loss: 47.6272\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5035 - val_loss: 47.4701\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5300 - val_loss: 47.3959\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4696 - val_loss: 47.2721\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5033 - val_loss: 47.4984\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4735 - val_loss: 47.2909\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4817 - val_loss: 47.4772\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4975 - val_loss: 47.3401\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.5079 - val_loss: 47.2767\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4816 - val_loss: 47.4278\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4727 - val_loss: 47.3531\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4869 - val_loss: 47.3882\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4672 - val_loss: 47.4201\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.5065 - val_loss: 47.3338\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4694 - val_loss: 47.5172\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4881 - val_loss: 47.3554\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5212 - val_loss: 47.2324\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4734 - val_loss: 47.3457\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4788 - val_loss: 47.4242\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4920 - val_loss: 47.3575\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5513 - val_loss: 47.6184\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4731 - val_loss: 47.3668\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4965 - val_loss: 47.3520\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4948 - val_loss: 47.3552\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5136 - val_loss: 47.2622\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5735 - val_loss: 47.4156\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4927 - val_loss: 47.4176\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4770 - val_loss: 47.3129\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4835 - val_loss: 47.3313\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4841 - val_loss: 47.4597\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 108us/step - loss: 24.5401 - val_loss: 47.2350\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4702 - val_loss: 47.2792\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5112 - val_loss: 47.4086\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4891 - val_loss: 47.3919\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4754 - val_loss: 47.2900\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5008 - val_loss: 47.4127\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4796 - val_loss: 47.4113\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5314 - val_loss: 47.2039\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5058 - val_loss: 47.5067\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5286 - val_loss: 47.4957\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.4567 - val_loss: 47.3466\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4842 - val_loss: 47.3791\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5262 - val_loss: 47.3702\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4745 - val_loss: 47.3379\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4754 - val_loss: 47.4202\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4727 - val_loss: 47.3636\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5524 - val_loss: 47.4866\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5286 - val_loss: 47.3164\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4710 - val_loss: 47.3847\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4844 - val_loss: 47.3369\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4861 - val_loss: 47.2740\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4740 - val_loss: 47.3963\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4975 - val_loss: 47.2396\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4648 - val_loss: 47.4044\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4602 - val_loss: 47.4189\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.5717 - val_loss: 47.5556\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4980 - val_loss: 47.3046\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4740 - val_loss: 47.2669\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.5178 - val_loss: 47.4452\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5410 - val_loss: 47.4736\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4544 - val_loss: 47.3062\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5533 - val_loss: 47.5046\n",
      "\n",
      "Mean Squared Error for iteration10: 45.47541331091996\n",
      "\n",
      "Iteration:  11\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.4538 - val_loss: 47.2983\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4982 - val_loss: 47.2659\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.5104 - val_loss: 47.2293\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4675 - val_loss: 47.4637\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5102 - val_loss: 47.2839\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4637 - val_loss: 47.4361\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5032 - val_loss: 47.4214\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.5142 - val_loss: 47.3559\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4714 - val_loss: 47.3570\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5055 - val_loss: 47.4150\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4717 - val_loss: 47.4065\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4979 - val_loss: 47.1754\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4719 - val_loss: 47.3448\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.4771 - val_loss: 47.4401\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4807 - val_loss: 47.4246\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4690 - val_loss: 47.4286\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4975 - val_loss: 47.3474\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5237 - val_loss: 47.4847\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4782 - val_loss: 47.2767\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4815 - val_loss: 47.4461\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.4639 - val_loss: 47.4725\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4358 - val_loss: 47.3838\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4947 - val_loss: 47.5196\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4794 - val_loss: 47.2681\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.5263 - val_loss: 47.4285\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4881 - val_loss: 47.3846\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4546 - val_loss: 47.2570\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4782 - val_loss: 47.3247\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4614 - val_loss: 47.4093\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4825 - val_loss: 47.4222\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4854 - val_loss: 47.4785\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4883 - val_loss: 47.2431\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4838 - val_loss: 47.5391\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4674 - val_loss: 47.4569\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4612 - val_loss: 47.4603\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4939 - val_loss: 47.1887\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4792 - val_loss: 47.3641\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4578 - val_loss: 47.4208\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4728 - val_loss: 47.3299\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4827 - val_loss: 47.3385\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4661 - val_loss: 47.4304\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4698 - val_loss: 47.4857\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4522 - val_loss: 47.3450\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4597 - val_loss: 47.3887\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4633 - val_loss: 47.3515\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4894 - val_loss: 47.2928\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5035 - val_loss: 47.4088\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.4918 - val_loss: 47.4481\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4936 - val_loss: 47.3360\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4470 - val_loss: 47.5077\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4584 - val_loss: 47.3775\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4746 - val_loss: 47.5377\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5239 - val_loss: 47.5013\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5325 - val_loss: 47.3718\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4835 - val_loss: 47.3827\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4859 - val_loss: 47.3503\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4698 - val_loss: 47.3047\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4618 - val_loss: 47.3971\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.4598 - val_loss: 47.4846\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4937 - val_loss: 47.4191\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4699 - val_loss: 47.4243\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4805 - val_loss: 47.3723\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5119 - val_loss: 47.4033\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.4790 - val_loss: 47.3848\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4784 - val_loss: 47.4449\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4646 - val_loss: 47.3245\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4933 - val_loss: 47.4645\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4887 - val_loss: 47.3112\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4683 - val_loss: 47.3907\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4761 - val_loss: 47.5478\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4852 - val_loss: 47.2845\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.4351 - val_loss: 47.4103\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5118 - val_loss: 47.5159\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4632 - val_loss: 47.3513\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4725 - val_loss: 47.3855\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4525 - val_loss: 47.3412\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4658 - val_loss: 47.3546\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4947 - val_loss: 47.4781\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4774 - val_loss: 47.4245\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4604 - val_loss: 47.4742\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 80us/step - loss: 24.5274 - val_loss: 47.3847\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4434 - val_loss: 47.4509\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4911 - val_loss: 47.4976\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4606 - val_loss: 47.2066\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4593 - val_loss: 47.4326\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5035 - val_loss: 47.4173\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4469 - val_loss: 47.4372\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4923 - val_loss: 47.3428\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4661 - val_loss: 47.3316\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.4376 - val_loss: 47.4958\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.5384 - val_loss: 47.3601\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4542 - val_loss: 47.4378\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4583 - val_loss: 47.3840\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4897 - val_loss: 47.6004\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.5769 - val_loss: 47.3093\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4534 - val_loss: 47.5456\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4890 - val_loss: 47.5459\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4376 - val_loss: 47.3936\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4617 - val_loss: 47.3352\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4587 - val_loss: 47.3617\n",
      "\n",
      "Mean Squared Error for iteration11: 45.243288476765585\n",
      "\n",
      "Iteration:  12\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4615 - val_loss: 47.3427\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4591 - val_loss: 47.5485\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.4765 - val_loss: 47.3540\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.4390 - val_loss: 47.4008\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4806 - val_loss: 47.5356\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.5165 - val_loss: 47.3690\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4576 - val_loss: 47.3830\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4392 - val_loss: 47.4876\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4519 - val_loss: 47.4489\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4859 - val_loss: 47.4437\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4604 - val_loss: 47.4625\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4507 - val_loss: 47.3718\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.4456 - val_loss: 47.4761\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4651 - val_loss: 47.3595\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4723 - val_loss: 47.3259\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4686 - val_loss: 47.5239\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4603 - val_loss: 47.3549\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4694 - val_loss: 47.5458\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4397 - val_loss: 47.4636\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4754 - val_loss: 47.3232\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4456 - val_loss: 47.3521\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4945 - val_loss: 47.4107\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4993 - val_loss: 47.3667\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4356 - val_loss: 47.3586\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4550 - val_loss: 47.4942\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4772 - val_loss: 47.4384\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4606 - val_loss: 47.5091\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4980 - val_loss: 47.1682\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4487 - val_loss: 47.5327\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4489 - val_loss: 47.5672\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.4433 - val_loss: 47.3688\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4695 - val_loss: 47.3889\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4718 - val_loss: 47.3849\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4327 - val_loss: 47.4221\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4768 - val_loss: 47.4105\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4681 - val_loss: 47.3279\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4677 - val_loss: 47.4216\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4481 - val_loss: 47.4036\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4452 - val_loss: 47.4876\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4604 - val_loss: 47.5046\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.5145 - val_loss: 47.5029\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4683 - val_loss: 47.3642\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4969 - val_loss: 47.2973\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4586 - val_loss: 47.4903\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4494 - val_loss: 47.4207\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4533 - val_loss: 47.5347\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4536 - val_loss: 47.3989\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 111us/step - loss: 24.4920 - val_loss: 47.2661\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.4599 - val_loss: 47.5766\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.4571 - val_loss: 47.5138\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4404 - val_loss: 47.4467\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.4430 - val_loss: 47.3316\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4614 - val_loss: 47.3447\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4581 - val_loss: 47.5059\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4796 - val_loss: 47.3887\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4464 - val_loss: 47.4792\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.4520 - val_loss: 47.3078\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.4263 - val_loss: 47.3542\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4625 - val_loss: 47.4572\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4369 - val_loss: 47.4239\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4525 - val_loss: 47.4397\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4328 - val_loss: 47.4934\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4514 - val_loss: 47.3516\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4662 - val_loss: 47.3834\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4673 - val_loss: 47.4230\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4805 - val_loss: 47.3925\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.5071 - val_loss: 47.3659\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4165 - val_loss: 47.3114\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.5241 - val_loss: 47.6181\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4505 - val_loss: 47.3802\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4388 - val_loss: 47.4998\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5064 - val_loss: 47.2934\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4821 - val_loss: 47.4764\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4502 - val_loss: 47.4947\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4306 - val_loss: 47.4183\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4725 - val_loss: 47.4288\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4626 - val_loss: 47.4364\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4367 - val_loss: 47.3758\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4360 - val_loss: 47.5005\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.5021 - val_loss: 47.3890\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4040 - val_loss: 47.3652\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4441 - val_loss: 47.4800\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4264 - val_loss: 47.3888\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4632 - val_loss: 47.4880\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4616 - val_loss: 47.5012\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4594 - val_loss: 47.4826\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4886 - val_loss: 47.2654\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4688 - val_loss: 47.4759\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4199 - val_loss: 47.4897\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4883 - val_loss: 47.5416\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4184 - val_loss: 47.4029\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4646 - val_loss: 47.5374\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.4804 - val_loss: 47.5678\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4860 - val_loss: 47.4130\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4757 - val_loss: 47.4964\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4142 - val_loss: 47.4119\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4587 - val_loss: 47.2313\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.5269 - val_loss: 47.4675\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4917 - val_loss: 47.4449\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4573 - val_loss: 47.4201\n",
      "\n",
      "Mean Squared Error for iteration12: 45.234665294457024\n",
      "\n",
      "Iteration:  13\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4313 - val_loss: 47.4458\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4891 - val_loss: 47.5212\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.4291 - val_loss: 47.4820\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4516 - val_loss: 47.5313\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4195 - val_loss: 47.4053\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4557 - val_loss: 47.5162\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4976 - val_loss: 47.3368\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4435 - val_loss: 47.4028\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4664 - val_loss: 47.4030\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4269 - val_loss: 47.4394\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4700 - val_loss: 47.3405\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4517 - val_loss: 47.6305\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4572 - val_loss: 47.5858\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4560 - val_loss: 47.2728\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4850 - val_loss: 47.5001\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.4470 - val_loss: 47.5398\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4457 - val_loss: 47.4153\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4532 - val_loss: 47.4202\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.5080 - val_loss: 47.4723\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4193 - val_loss: 47.4303\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4067 - val_loss: 47.5539\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4518 - val_loss: 47.3584\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4819 - val_loss: 47.4367\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4641 - val_loss: 47.4411\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.4864 - val_loss: 47.5737\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4297 - val_loss: 47.4126\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4500 - val_loss: 47.4305\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4586 - val_loss: 47.3912\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4527 - val_loss: 47.3730\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4855 - val_loss: 47.5853\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4227 - val_loss: 47.4182\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4499 - val_loss: 47.5201\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4182 - val_loss: 47.4290\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4490 - val_loss: 47.3334\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 24.4181 - val_loss: 47.4239\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4764 - val_loss: 47.4477\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4719 - val_loss: 47.5425\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.4408 - val_loss: 47.3754\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4608 - val_loss: 47.3698\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4743 - val_loss: 47.4178\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4323 - val_loss: 47.4393\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4188 - val_loss: 47.4939\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4780 - val_loss: 47.5448\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4472 - val_loss: 47.3578\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4388 - val_loss: 47.3746\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4316 - val_loss: 47.4380\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4093 - val_loss: 47.4782\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4498 - val_loss: 47.4855\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4351 - val_loss: 47.4236\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4177 - val_loss: 47.4749\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4457 - val_loss: 47.4718\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3995 - val_loss: 47.4517\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4270 - val_loss: 47.4353\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4399 - val_loss: 47.5178\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4479 - val_loss: 47.2771\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.4259 - val_loss: 47.4836\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4274 - val_loss: 47.3424\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4016 - val_loss: 47.4994\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4298 - val_loss: 47.4967\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4372 - val_loss: 47.5146\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4229 - val_loss: 47.4535\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4349 - val_loss: 47.3143\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4051 - val_loss: 47.4759\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4295 - val_loss: 47.4342\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.4235 - val_loss: 47.4335\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.4166 - val_loss: 47.3981\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4186 - val_loss: 47.4241\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4178 - val_loss: 47.4706\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4425 - val_loss: 47.4495\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4398 - val_loss: 47.5099\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4442 - val_loss: 47.4075\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4401 - val_loss: 47.3431\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4188 - val_loss: 47.3728\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4580 - val_loss: 47.4647\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4318 - val_loss: 47.5149\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4125 - val_loss: 47.2882\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4422 - val_loss: 47.4064\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4533 - val_loss: 47.2539\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4406 - val_loss: 47.4913\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4632 - val_loss: 47.3406\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4370 - val_loss: 47.4016\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4174 - val_loss: 47.3849\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4000 - val_loss: 47.4245\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.4109 - val_loss: 47.5197\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4212 - val_loss: 47.3322\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4342 - val_loss: 47.3242\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.4643 - val_loss: 47.4793\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4816 - val_loss: 47.1646\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4256 - val_loss: 47.3846\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3970 - val_loss: 47.2995\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4185 - val_loss: 47.4317\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4252 - val_loss: 47.3539\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4610 - val_loss: 47.2709\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.4423 - val_loss: 47.2301\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4061 - val_loss: 47.3728\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4240 - val_loss: 47.3828\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3960 - val_loss: 47.3915\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4228 - val_loss: 47.3223\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4724 - val_loss: 47.4430\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4524 - val_loss: 47.3094\n",
      "\n",
      "Mean Squared Error for iteration13: 45.0110600832439\n",
      "\n",
      "Iteration:  14\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4503 - val_loss: 47.1772\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4823 - val_loss: 47.2972\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4090 - val_loss: 47.4098\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4107 - val_loss: 47.2711\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4222 - val_loss: 47.2021\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4205 - val_loss: 47.3444\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4267 - val_loss: 47.2650\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4298 - val_loss: 47.3069\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4391 - val_loss: 47.3907\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4441 - val_loss: 47.2548\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4376 - val_loss: 47.4045\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 77us/step - loss: 24.4465 - val_loss: 47.3152\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4558 - val_loss: 47.3426\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4273 - val_loss: 47.2648\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4858 - val_loss: 47.4061\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3799 - val_loss: 47.2471\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4042 - val_loss: 47.2337\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4163 - val_loss: 47.2712\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4221 - val_loss: 47.2884\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4091 - val_loss: 47.2626\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4056 - val_loss: 47.3343\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4223 - val_loss: 47.4025\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4444 - val_loss: 47.2776\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4009 - val_loss: 47.2597\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4271 - val_loss: 47.3254\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4223 - val_loss: 47.2110\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4232 - val_loss: 47.3514\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4355 - val_loss: 47.5130\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.4480 - val_loss: 47.3355\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.4968 - val_loss: 47.2246\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4479 - val_loss: 47.2680\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4349 - val_loss: 47.3121\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3820 - val_loss: 47.2411\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4364 - val_loss: 47.3017\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3883 - val_loss: 47.3393\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4158 - val_loss: 47.3043\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4112 - val_loss: 47.2575\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3982 - val_loss: 47.2012\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.4247 - val_loss: 47.2023\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3824 - val_loss: 47.2577\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3933 - val_loss: 47.3987\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3948 - val_loss: 47.2606\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4226 - val_loss: 47.1957\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4278 - val_loss: 47.2710\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3873 - val_loss: 47.2895\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4034 - val_loss: 47.2829\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3992 - val_loss: 47.3338\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4780 - val_loss: 47.3831\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3936 - val_loss: 47.2430\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3940 - val_loss: 47.1759\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3918 - val_loss: 47.2792\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4106 - val_loss: 47.2041\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4400 - val_loss: 47.1523\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4151 - val_loss: 47.2390\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3716 - val_loss: 47.3402\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4149 - val_loss: 47.3637\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.4137 - val_loss: 47.3116\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3933 - val_loss: 47.3354\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4412 - val_loss: 47.1184\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4010 - val_loss: 47.3603\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4323 - val_loss: 47.2894\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4071 - val_loss: 47.2261\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3900 - val_loss: 47.2352\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4043 - val_loss: 47.2581\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4104 - val_loss: 47.2688\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4090 - val_loss: 47.2231\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4012 - val_loss: 47.2282\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4087 - val_loss: 47.2223\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4306 - val_loss: 47.3422\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4299 - val_loss: 47.2946\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3879 - val_loss: 47.2259\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4215 - val_loss: 47.1377\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3853 - val_loss: 47.1923\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.4191 - val_loss: 47.4383\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.3981 - val_loss: 47.3391\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4137 - val_loss: 47.1686\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4170 - val_loss: 47.3084\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4248 - val_loss: 47.3166\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4610 - val_loss: 47.1789\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3914 - val_loss: 47.3728\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4606 - val_loss: 47.1388\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4076 - val_loss: 47.2081\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4125 - val_loss: 47.2286\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4141 - val_loss: 47.2976\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4071 - val_loss: 47.2430\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4359 - val_loss: 47.2037\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4296 - val_loss: 47.2966\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.4430 - val_loss: 47.1792\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4195 - val_loss: 47.3231\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.4589 - val_loss: 47.2259\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4418 - val_loss: 47.2769\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4223 - val_loss: 47.3064\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3884 - val_loss: 47.3092\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4579 - val_loss: 47.1661\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3853 - val_loss: 47.2709\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4444 - val_loss: 47.3790\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3898 - val_loss: 47.2209\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4143 - val_loss: 47.2183\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3708 - val_loss: 47.2110\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3998 - val_loss: 47.2399\n",
      "\n",
      "Mean Squared Error for iteration14: 44.943393448649815\n",
      "\n",
      "Iteration:  15\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4169 - val_loss: 47.3113\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3887 - val_loss: 47.1831\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4008 - val_loss: 47.2313\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4360 - val_loss: 47.3361\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4719 - val_loss: 47.1178\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.4272 - val_loss: 47.2835\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4273 - val_loss: 47.2521\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4166 - val_loss: 47.3459\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3724 - val_loss: 47.2981\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4233 - val_loss: 47.3445\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.3857 - val_loss: 47.1598\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3901 - val_loss: 47.2590\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4201 - val_loss: 47.2511\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3699 - val_loss: 47.2073\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4641 - val_loss: 47.1391\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4115 - val_loss: 47.3702\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4537 - val_loss: 47.3767\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3794 - val_loss: 47.2823\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4325 - val_loss: 47.2453\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.4262 - val_loss: 47.2602\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4896 - val_loss: 47.3359\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4062 - val_loss: 47.1788\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3751 - val_loss: 47.2159\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3825 - val_loss: 47.2647\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4362 - val_loss: 47.1745\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3959 - val_loss: 47.2811\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4063 - val_loss: 47.1142\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3961 - val_loss: 47.2970\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3996 - val_loss: 47.2686\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4171 - val_loss: 47.3074\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4430 - val_loss: 47.1991\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3943 - val_loss: 47.1856\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3983 - val_loss: 47.2849\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3843 - val_loss: 47.2021\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3795 - val_loss: 47.2457\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4771 - val_loss: 47.2740\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4330 - val_loss: 47.2791\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4250 - val_loss: 47.2934\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4060 - val_loss: 47.1677\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4030 - val_loss: 47.2507\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4245 - val_loss: 47.2240\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4492 - val_loss: 47.1993\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4081 - val_loss: 47.0268\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3721 - val_loss: 47.3097\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.4048 - val_loss: 47.2933\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 166us/step - loss: 24.3969 - val_loss: 47.2829\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 211us/step - loss: 24.4215 - val_loss: 47.2079\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 136us/step - loss: 24.4092 - val_loss: 47.2220\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 133us/step - loss: 24.3805 - val_loss: 47.2137\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 156us/step - loss: 24.4061 - val_loss: 47.2240\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 106us/step - loss: 24.4448 - val_loss: 47.2299\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3801 - val_loss: 47.2759\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3839 - val_loss: 47.2065\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3881 - val_loss: 47.2299\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3661 - val_loss: 47.2341\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 117us/step - loss: 24.3840 - val_loss: 47.2657\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 169us/step - loss: 24.4241 - val_loss: 47.1958\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 149us/step - loss: 24.4879 - val_loss: 47.4523\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.4069 - val_loss: 47.1869\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3821 - val_loss: 47.1931\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3912 - val_loss: 47.2053\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.3928 - val_loss: 47.1425\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4143 - val_loss: 47.1310\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4516 - val_loss: 47.1341\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3936 - val_loss: 47.2015\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3955 - val_loss: 47.0778\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 77us/step - loss: 24.4069 - val_loss: 47.3031\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3866 - val_loss: 47.2521\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3928 - val_loss: 47.1957\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3910 - val_loss: 47.2744\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3885 - val_loss: 47.2062\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3840 - val_loss: 47.2178\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3696 - val_loss: 47.1724\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4435 - val_loss: 47.2526\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4011 - val_loss: 47.1320\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3981 - val_loss: 47.2187\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3711 - val_loss: 47.0900\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3784 - val_loss: 47.0512\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3925 - val_loss: 47.1897\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3830 - val_loss: 47.2894\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3844 - val_loss: 47.3736\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4151 - val_loss: 47.3746\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.4824 - val_loss: 47.0458\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.5363 - val_loss: 47.2985\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3804 - val_loss: 47.2237\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3901 - val_loss: 47.0498\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3773 - val_loss: 47.0500\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3644 - val_loss: 47.2470\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3951 - val_loss: 47.2755\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3625 - val_loss: 47.2732\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.3934 - val_loss: 47.2809\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3597 - val_loss: 47.1482\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4345 - val_loss: 47.2575\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4195 - val_loss: 47.2674\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3699 - val_loss: 47.2673\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4323 - val_loss: 47.1623\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3856 - val_loss: 47.2300\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3657 - val_loss: 47.2130\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3776 - val_loss: 47.2838\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3886 - val_loss: 47.1427\n",
      "\n",
      "Mean Squared Error for iteration15: 44.96926557836111\n",
      "\n",
      "Iteration:  16\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.4502 - val_loss: 47.0948\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.4294 - val_loss: 47.2592\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4482 - val_loss: 47.0011\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3921 - val_loss: 47.1806\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.4167 - val_loss: 47.2401\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3904 - val_loss: 47.2925\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4301 - val_loss: 47.1559\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3933 - val_loss: 47.0701\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3554 - val_loss: 47.2127\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4175 - val_loss: 47.4460\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.4043 - val_loss: 47.1634\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3734 - val_loss: 47.2424\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3941 - val_loss: 47.1487\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3574 - val_loss: 47.1580\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4033 - val_loss: 47.2162\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4767 - val_loss: 47.3139\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4035 - val_loss: 47.0778\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3569 - val_loss: 47.2105\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3849 - val_loss: 47.2616\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3498 - val_loss: 47.2088\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3756 - val_loss: 47.1947\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3888 - val_loss: 47.3239\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.4108 - val_loss: 47.1470\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3986 - val_loss: 47.2071\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3824 - val_loss: 47.2827\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3723 - val_loss: 47.1987\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3984 - val_loss: 47.1985\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3514 - val_loss: 47.2541\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3786 - val_loss: 47.2925\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4311 - val_loss: 47.0999\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3787 - val_loss: 47.1376\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3682 - val_loss: 47.1963\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.4211 - val_loss: 47.4178\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3769 - val_loss: 47.1129\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3767 - val_loss: 47.1218\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3801 - val_loss: 47.2721\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.4364 - val_loss: 47.1749\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4322 - val_loss: 47.1487\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4068 - val_loss: 47.1442\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3839 - val_loss: 47.2955\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4220 - val_loss: 47.2326\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3771 - val_loss: 47.2766\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3734 - val_loss: 47.2555\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.3932 - val_loss: 47.1392\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4125 - val_loss: 47.2177\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.3847 - val_loss: 47.2016\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3702 - val_loss: 47.1965\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3799 - val_loss: 47.1794\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3518 - val_loss: 47.0818\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3663 - val_loss: 47.1931\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4414 - val_loss: 47.2299\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3730 - val_loss: 47.1686\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.3732 - val_loss: 47.0774\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4039 - val_loss: 47.2740\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4148 - val_loss: 47.2634\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3845 - val_loss: 47.0292\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3921 - val_loss: 47.2508\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3913 - val_loss: 47.3704\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3861 - val_loss: 47.1890\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3890 - val_loss: 47.2197\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4143 - val_loss: 47.1567\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.3611 - val_loss: 47.3184\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3889 - val_loss: 47.1825\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4285 - val_loss: 47.2013\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3520 - val_loss: 47.1069\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3690 - val_loss: 47.3170\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3923 - val_loss: 47.1402\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4321 - val_loss: 47.2815\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.4056 - val_loss: 47.0239\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3943 - val_loss: 47.1796\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3823 - val_loss: 47.2693\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3602 - val_loss: 47.0886\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3688 - val_loss: 47.0766\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4616 - val_loss: 47.1478\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3522 - val_loss: 47.2059\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3619 - val_loss: 47.1851\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4155 - val_loss: 47.2610\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3502 - val_loss: 47.2123\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3938 - val_loss: 46.9844\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3852 - val_loss: 47.2364\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3651 - val_loss: 47.1895\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3931 - val_loss: 47.0478\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3577 - val_loss: 47.2527\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3868 - val_loss: 47.2106\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3807 - val_loss: 47.1878\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3763 - val_loss: 47.1431\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4079 - val_loss: 47.3015\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3859 - val_loss: 46.9526\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3887 - val_loss: 47.2104\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3516 - val_loss: 47.2320\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3718 - val_loss: 47.1023\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.3502 - val_loss: 47.1802\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.3660 - val_loss: 47.2028\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3613 - val_loss: 47.3513\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3717 - val_loss: 47.2143\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3798 - val_loss: 47.0518\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3657 - val_loss: 47.2077\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3846 - val_loss: 47.2209\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3607 - val_loss: 47.2696\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3759 - val_loss: 47.2912\n",
      "\n",
      "Mean Squared Error for iteration16: 45.16793136693827\n",
      "\n",
      "Iteration:  17\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3542 - val_loss: 47.1656\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3899 - val_loss: 47.1386\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3442 - val_loss: 47.1669\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3968 - val_loss: 47.2578\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3896 - val_loss: 47.3051\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3904 - val_loss: 47.1116\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3609 - val_loss: 47.0876\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3845 - val_loss: 47.1190\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.4186 - val_loss: 47.3995\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3439 - val_loss: 47.1039\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3472 - val_loss: 47.1476\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3965 - val_loss: 47.1270\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3488 - val_loss: 47.2562\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3782 - val_loss: 47.2467\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3814 - val_loss: 47.0836\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3658 - val_loss: 47.1225\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4023 - val_loss: 47.1694\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4535 - val_loss: 47.1772\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3813 - val_loss: 47.2091\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3913 - val_loss: 47.1234\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.3636 - val_loss: 47.1657\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4028 - val_loss: 47.2055\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3739 - val_loss: 47.1423\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.4179 - val_loss: 47.1201\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3828 - val_loss: 47.3468\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3475 - val_loss: 47.1940\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3551 - val_loss: 47.2090\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3468 - val_loss: 47.1316\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3728 - val_loss: 47.2190\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4314 - val_loss: 47.2059\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3513 - val_loss: 47.2309\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3603 - val_loss: 47.0744\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3727 - val_loss: 47.2036\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3583 - val_loss: 47.0820\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3898 - val_loss: 47.2261\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3805 - val_loss: 47.1227\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 111us/step - loss: 24.3604 - val_loss: 47.1881\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3563 - val_loss: 47.1828\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3600 - val_loss: 47.1531\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3738 - val_loss: 47.1701\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3750 - val_loss: 47.2679\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.3641 - val_loss: 47.1556\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3594 - val_loss: 47.1225\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3472 - val_loss: 47.1151\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3915 - val_loss: 47.3182\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3666 - val_loss: 47.2521\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.3579 - val_loss: 47.2524\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3494 - val_loss: 47.0540\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3647 - val_loss: 47.1646\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3616 - val_loss: 47.1708\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3904 - val_loss: 47.1295\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3589 - val_loss: 47.0742\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3970 - val_loss: 47.0879\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3849 - val_loss: 47.1578\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3498 - val_loss: 47.3050\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3480 - val_loss: 47.1833\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3487 - val_loss: 47.0457\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3762 - val_loss: 47.0192\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3852 - val_loss: 47.2793\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3486 - val_loss: 47.2966\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3385 - val_loss: 47.0802\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3616 - val_loss: 47.0807\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3656 - val_loss: 47.1275\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4122 - val_loss: 47.2302\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3295 - val_loss: 47.1577\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3507 - val_loss: 47.1383\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3511 - val_loss: 47.0761\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4150 - val_loss: 47.2698\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.3543 - val_loss: 47.1184\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3907 - val_loss: 47.1077\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4770 - val_loss: 47.2862\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3709 - val_loss: 47.1735\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4102 - val_loss: 47.1014\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3676 - val_loss: 47.2348\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3447 - val_loss: 47.1434\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3496 - val_loss: 47.1343\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3821 - val_loss: 47.2748\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3600 - val_loss: 47.1607\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3650 - val_loss: 47.0902\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3933 - val_loss: 47.1527\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3558 - val_loss: 47.1306\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.4036 - val_loss: 47.1165\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3728 - val_loss: 47.1535\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3650 - val_loss: 47.0279\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3165 - val_loss: 47.1230\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3440 - val_loss: 47.1035\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3654 - val_loss: 47.2532\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3777 - val_loss: 47.2221\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3641 - val_loss: 47.0695\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.3601 - val_loss: 47.1223\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.3665 - val_loss: 47.0145\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3390 - val_loss: 47.1986\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.3967 - val_loss: 47.3660\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4006 - val_loss: 46.9944\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3716 - val_loss: 47.1363\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3422 - val_loss: 47.1692\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3341 - val_loss: 47.1329\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.4069 - val_loss: 47.2526\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 24.3901 - val_loss: 47.1675\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3475 - val_loss: 47.1227\n",
      "\n",
      "Mean Squared Error for iteration17: 44.90347959329814\n",
      "\n",
      "Iteration:  18\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3713 - val_loss: 47.1056\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3556 - val_loss: 47.0700\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3582 - val_loss: 47.0715\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3762 - val_loss: 47.3119\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3507 - val_loss: 47.2240\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3663 - val_loss: 47.1085\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4056 - val_loss: 47.1234\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.4031 - val_loss: 47.2531\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3848 - val_loss: 47.1423\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3925 - val_loss: 46.9783\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3230 - val_loss: 47.2122\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.3868 - val_loss: 47.1060\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3416 - val_loss: 47.1897\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3771 - val_loss: 47.1790\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3673 - val_loss: 47.2645\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3471 - val_loss: 47.2356\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3368 - val_loss: 47.0666\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3437 - val_loss: 47.0302\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3594 - val_loss: 47.2076\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3581 - val_loss: 47.1784\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3314 - val_loss: 47.1030\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3518 - val_loss: 47.0830\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3679 - val_loss: 47.3267\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4022 - val_loss: 47.1311\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.4156 - val_loss: 47.3268\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.3413 - val_loss: 47.1278\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3782 - val_loss: 47.0431\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3310 - val_loss: 47.1591\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3938 - val_loss: 47.0549\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3327 - val_loss: 47.1462\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3429 - val_loss: 47.2721\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3675 - val_loss: 47.3082\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3547 - val_loss: 47.1674\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3342 - val_loss: 47.0944\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3456 - val_loss: 47.0761\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.3705 - val_loss: 47.1847\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3597 - val_loss: 47.0849\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3920 - val_loss: 47.2065\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3600 - val_loss: 47.0983\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3486 - val_loss: 47.1237\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3397 - val_loss: 47.1975\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3950 - val_loss: 47.2687\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3397 - val_loss: 47.0293\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3324 - val_loss: 47.0281\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4072 - val_loss: 47.3641\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3722 - val_loss: 46.9475\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3844 - val_loss: 47.1513\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3541 - val_loss: 47.2411\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3562 - val_loss: 47.1664\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3596 - val_loss: 47.1610\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3534 - val_loss: 47.1778\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3554 - val_loss: 47.0604\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3427 - val_loss: 47.0538\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3386 - val_loss: 47.2159\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3482 - val_loss: 47.0744\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3893 - val_loss: 47.0593\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3492 - val_loss: 47.1661\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.3651 - val_loss: 47.1876\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3162 - val_loss: 47.1695\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3690 - val_loss: 47.2293\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3998 - val_loss: 47.0839\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3640 - val_loss: 47.0454\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3981 - val_loss: 47.2671\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3656 - val_loss: 47.0685\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3613 - val_loss: 47.2096\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3765 - val_loss: 47.0723\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3887 - val_loss: 47.1031\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3436 - val_loss: 47.2239\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3512 - val_loss: 47.2022\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.3476 - val_loss: 47.0875\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.3260 - val_loss: 47.0253\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3694 - val_loss: 47.1179\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.4148 - val_loss: 47.3152\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3722 - val_loss: 47.1392\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3664 - val_loss: 47.0914\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 81us/step - loss: 24.3352 - val_loss: 47.0949\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3430 - val_loss: 47.1797\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3428 - val_loss: 47.1011\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3641 - val_loss: 47.1886\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.3638 - val_loss: 46.9910\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3465 - val_loss: 47.0217\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3708 - val_loss: 47.2075\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3510 - val_loss: 47.1486\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3747 - val_loss: 47.3287\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4205 - val_loss: 47.0407\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3490 - val_loss: 47.2005\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3403 - val_loss: 47.1658\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3987 - val_loss: 47.2779\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3433 - val_loss: 47.1816\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3226 - val_loss: 47.1199\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3776 - val_loss: 47.0779\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3471 - val_loss: 47.1404\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3415 - val_loss: 47.0213\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3511 - val_loss: 47.1499\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.3651 - val_loss: 47.1853\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3741 - val_loss: 47.1195\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3981 - val_loss: 47.1553\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3627 - val_loss: 46.9881\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3435 - val_loss: 47.0531\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3723 - val_loss: 47.2581\n",
      "\n",
      "Mean Squared Error for iteration18: 45.261526793941556\n",
      "\n",
      "Iteration:  19\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3965 - val_loss: 47.0870\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3501 - val_loss: 47.1619\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3691 - val_loss: 47.1224\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3530 - val_loss: 47.1238\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3684 - val_loss: 47.1903\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3698 - val_loss: 47.0558\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3422 - val_loss: 47.1036\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3468 - val_loss: 47.0935\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3878 - val_loss: 47.0096\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3791 - val_loss: 47.1926\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3392 - val_loss: 47.1507\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3474 - val_loss: 47.2467\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3421 - val_loss: 47.0448\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3235 - val_loss: 47.1128\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.3309 - val_loss: 47.1510\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3259 - val_loss: 47.1390\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3071 - val_loss: 47.0704\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3787 - val_loss: 47.2878\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3225 - val_loss: 47.1453\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 125us/step - loss: 24.3860 - val_loss: 47.0260\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3118 - val_loss: 47.1628\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3631 - val_loss: 47.1520\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3648 - val_loss: 47.0636\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3533 - val_loss: 47.1488\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3670 - val_loss: 47.1401\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3371 - val_loss: 47.1457\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3485 - val_loss: 47.0000\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3381 - val_loss: 47.2000\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3466 - val_loss: 47.1063\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4016 - val_loss: 47.0795\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3510 - val_loss: 47.0887\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3423 - val_loss: 47.2190\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3877 - val_loss: 46.9110\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3167 - val_loss: 47.0399\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3364 - val_loss: 47.1899\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3398 - val_loss: 47.1806\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3372 - val_loss: 47.1729\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3666 - val_loss: 47.1814\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3146 - val_loss: 47.1730\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3260 - val_loss: 47.0711\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3441 - val_loss: 47.1799\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3454 - val_loss: 47.1170\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3372 - val_loss: 47.1514\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3272 - val_loss: 46.9551\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3512 - val_loss: 47.1285\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3067 - val_loss: 47.1310\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3389 - val_loss: 47.1795\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3445 - val_loss: 47.0812\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3364 - val_loss: 47.1359\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3049 - val_loss: 47.1984\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3201 - val_loss: 47.1267\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4048 - val_loss: 47.1837\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.3267 - val_loss: 47.2066\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3468 - val_loss: 47.2042\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3415 - val_loss: 47.0495\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3308 - val_loss: 46.9847\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3825 - val_loss: 47.1715\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3767 - val_loss: 47.1578\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3477 - val_loss: 47.0286\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.3283 - val_loss: 47.2296\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3720 - val_loss: 47.1867\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3346 - val_loss: 47.0762\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3405 - val_loss: 47.0940\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3536 - val_loss: 47.1689\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3734 - val_loss: 46.9027\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3486 - val_loss: 47.2229\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3164 - val_loss: 47.0913\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4154 - val_loss: 47.1315\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3058 - val_loss: 47.1072\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3288 - val_loss: 47.1379\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3697 - val_loss: 46.9921\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3348 - val_loss: 47.2477\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3613 - val_loss: 47.1574\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3709 - val_loss: 47.2883\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3424 - val_loss: 47.0583\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3660 - val_loss: 47.0231\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3825 - val_loss: 47.0162\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3441 - val_loss: 47.0745\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.4354 - val_loss: 47.2224\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3308 - val_loss: 47.0018\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3236 - val_loss: 47.0499\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3340 - val_loss: 47.1422\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3475 - val_loss: 47.1731\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.4111 - val_loss: 47.0725\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3755 - val_loss: 47.0868\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3717 - val_loss: 47.1733\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3085 - val_loss: 47.1492\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.3250 - val_loss: 47.2315\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.3812 - val_loss: 47.2508\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3354 - val_loss: 46.9709\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3808 - val_loss: 47.0088\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3456 - val_loss: 47.0344\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3508 - val_loss: 47.0805\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3599 - val_loss: 47.2108\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3296 - val_loss: 47.1251\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3055 - val_loss: 47.1740\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3633 - val_loss: 47.1355\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3300 - val_loss: 47.1990\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3020 - val_loss: 47.1525\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3719 - val_loss: 47.0760\n",
      "\n",
      "Mean Squared Error for iteration19: 45.07935083810904\n",
      "\n",
      "Iteration:  20\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3062 - val_loss: 47.1500\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.3326 - val_loss: 47.0683\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.3803 - val_loss: 47.2204\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.3849 - val_loss: 47.0648\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3749 - val_loss: 46.9796\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3416 - val_loss: 47.1053\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3294 - val_loss: 47.2238\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3206 - val_loss: 47.0446\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3631 - val_loss: 47.1011\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3481 - val_loss: 47.0858\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3190 - val_loss: 47.0886\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3711 - val_loss: 47.0359\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3388 - val_loss: 47.1127\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3760 - val_loss: 47.1546\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3743 - val_loss: 47.2050\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3446 - val_loss: 47.1633\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3280 - val_loss: 47.1616\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3317 - val_loss: 47.0298\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3241 - val_loss: 47.2773\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3294 - val_loss: 47.0656\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3139 - val_loss: 47.1278\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3416 - val_loss: 47.0643\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3341 - val_loss: 46.9936\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3343 - val_loss: 47.1927\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3312 - val_loss: 47.1070\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3361 - val_loss: 47.1594\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3241 - val_loss: 47.0919\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3256 - val_loss: 47.1519\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3146 - val_loss: 46.9924\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 80us/step - loss: 24.3239 - val_loss: 47.0441\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3018 - val_loss: 47.2246\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3417 - val_loss: 47.1911\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3183 - val_loss: 47.0738\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3150 - val_loss: 47.0462\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3460 - val_loss: 47.1918\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3418 - val_loss: 47.1788\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3221 - val_loss: 46.9967\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3149 - val_loss: 47.0494\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3194 - val_loss: 47.0509\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3029 - val_loss: 47.0908\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3221 - val_loss: 47.2415\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3459 - val_loss: 47.0597\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3324 - val_loss: 47.1516\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3295 - val_loss: 47.1313\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3647 - val_loss: 47.0716\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3179 - val_loss: 47.1144\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3396 - val_loss: 47.1181\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3159 - val_loss: 47.1305\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.3465 - val_loss: 47.0972\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3187 - val_loss: 47.1138\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3351 - val_loss: 47.2154\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2970 - val_loss: 47.0910\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3102 - val_loss: 47.0694\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3626 - val_loss: 46.8983\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3242 - val_loss: 47.1662\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3517 - val_loss: 47.1800\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3231 - val_loss: 47.2296\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3299 - val_loss: 47.0332\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3648 - val_loss: 47.1277\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3270 - val_loss: 47.1114\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3552 - val_loss: 47.0871\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3776 - val_loss: 47.1357\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3778 - val_loss: 47.1814\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3372 - val_loss: 47.1541\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3284 - val_loss: 47.1634\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3040 - val_loss: 46.9528\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3281 - val_loss: 47.1617\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3007 - val_loss: 47.0957\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3301 - val_loss: 46.9204\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3154 - val_loss: 47.1212\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2977 - val_loss: 47.1347\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3189 - val_loss: 47.1439\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3518 - val_loss: 47.1726\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2945 - val_loss: 47.1016\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3601 - val_loss: 47.1127\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3079 - val_loss: 47.1070\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.4318 - val_loss: 46.9859\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3548 - val_loss: 47.1575\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3216 - val_loss: 47.1667\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3082 - val_loss: 47.1509\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.3322 - val_loss: 47.2319\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3567 - val_loss: 47.1812\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3134 - val_loss: 47.1388\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3601 - val_loss: 47.0633\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3119 - val_loss: 47.0610\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3416 - val_loss: 47.1730\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3353 - val_loss: 47.0407\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.4090 - val_loss: 46.9772\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3050 - val_loss: 47.1806\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.3178 - val_loss: 47.1250\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3105 - val_loss: 47.2191\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3441 - val_loss: 47.1531\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.3570 - val_loss: 47.2195\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.3308 - val_loss: 47.1032\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.3011 - val_loss: 47.0040\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3030 - val_loss: 47.0766\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3526 - val_loss: 47.2966\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3120 - val_loss: 47.0275\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3148 - val_loss: 46.9651\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3104 - val_loss: 47.1684\n",
      "\n",
      "Mean Squared Error for iteration20: 45.15624381527442\n",
      "\n",
      "Iteration:  21\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3128 - val_loss: 47.1795\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3171 - val_loss: 47.0609\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.3098 - val_loss: 47.1495\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3184 - val_loss: 47.0908\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3687 - val_loss: 47.1712\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3079 - val_loss: 47.0574\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 72us/step - loss: 24.3735 - val_loss: 46.8748\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3447 - val_loss: 47.0126\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3772 - val_loss: 47.2511\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3080 - val_loss: 47.2149\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3387 - val_loss: 47.0331\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3396 - val_loss: 47.1614\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2998 - val_loss: 47.2048\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3195 - val_loss: 47.1027\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3169 - val_loss: 47.1435\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3234 - val_loss: 47.0370\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3049 - val_loss: 47.1045\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3498 - val_loss: 47.0245\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3083 - val_loss: 47.0264\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3516 - val_loss: 47.0609\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3245 - val_loss: 47.2067\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3667 - val_loss: 46.9901\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3557 - val_loss: 47.1068\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3012 - val_loss: 47.2468\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3611 - val_loss: 47.2638\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3249 - val_loss: 47.1044\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3247 - val_loss: 47.1078\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3478 - val_loss: 46.9632\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3113 - val_loss: 47.0850\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3228 - val_loss: 47.1245\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3324 - val_loss: 47.0589\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.3404 - val_loss: 47.0791\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.3110 - val_loss: 47.0977\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3166 - val_loss: 47.1767\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.3322 - val_loss: 47.0739\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3264 - val_loss: 47.1088\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3460 - val_loss: 47.1856\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.3111 - val_loss: 47.0531\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3068 - val_loss: 47.0823\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3020 - val_loss: 47.0271\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3225 - val_loss: 47.1363\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2898 - val_loss: 47.1316\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3146 - val_loss: 47.1147\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3048 - val_loss: 47.0805\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3215 - val_loss: 47.0904\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3248 - val_loss: 47.1469\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3031 - val_loss: 47.0880\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.3420 - val_loss: 47.1300\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3518 - val_loss: 47.1001\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3197 - val_loss: 47.0830\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3538 - val_loss: 46.9931\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3064 - val_loss: 47.0140\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3131 - val_loss: 47.2153\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3758 - val_loss: 47.2084\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3575 - val_loss: 47.0621\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3456 - val_loss: 47.1108\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3650 - val_loss: 46.9909\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2820 - val_loss: 47.0922\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3426 - val_loss: 47.1165\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2983 - val_loss: 47.2060\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3181 - val_loss: 47.1492\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2841 - val_loss: 47.1893\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3289 - val_loss: 47.1190\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3117 - val_loss: 47.0317\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3126 - val_loss: 47.2300\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3112 - val_loss: 47.1735\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3080 - val_loss: 47.0725\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3915 - val_loss: 47.0624\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3978 - val_loss: 47.1859\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.3332 - val_loss: 47.2421\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3362 - val_loss: 47.0702\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3244 - val_loss: 46.9900\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3411 - val_loss: 47.1117\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3124 - val_loss: 47.0142\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3267 - val_loss: 47.1602\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3015 - val_loss: 47.1905\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3122 - val_loss: 47.0629\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3070 - val_loss: 47.0061\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3013 - val_loss: 47.1100\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2919 - val_loss: 47.1042\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.3561 - val_loss: 47.1901\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2772 - val_loss: 47.0821\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.3484 - val_loss: 47.2247\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3011 - val_loss: 47.0978\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 76us/step - loss: 24.3218 - val_loss: 47.1096\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2825 - val_loss: 47.0133\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3577 - val_loss: 47.0614\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3818 - val_loss: 47.2931\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2744 - val_loss: 47.0744\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3183 - val_loss: 46.9249\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2951 - val_loss: 47.0214\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2914 - val_loss: 47.1499\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3040 - val_loss: 47.1285\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2830 - val_loss: 47.1005\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3295 - val_loss: 47.1180\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2892 - val_loss: 47.0557\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3007 - val_loss: 47.0462\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3161 - val_loss: 47.1471\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3070 - val_loss: 46.9716\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3067 - val_loss: 47.0952\n",
      "\n",
      "Mean Squared Error for iteration21: 45.103947739090984\n",
      "\n",
      "Iteration:  22\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 68us/step - loss: 24.3276 - val_loss: 47.1166\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3622 - val_loss: 47.2687\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3106 - val_loss: 47.0605\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3593 - val_loss: 47.2593\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2922 - val_loss: 47.0042\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3264 - val_loss: 47.0894\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3029 - val_loss: 47.1547\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3929 - val_loss: 47.0846\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3506 - val_loss: 47.1010\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3281 - val_loss: 47.0494\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2887 - val_loss: 47.1814\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2931 - val_loss: 47.0754\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3652 - val_loss: 47.0153\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3162 - val_loss: 47.2094\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2853 - val_loss: 47.1805\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3467 - val_loss: 47.1633\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2989 - val_loss: 47.0731\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2892 - val_loss: 46.9960\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3175 - val_loss: 47.0320\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2840 - val_loss: 47.0516\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2912 - val_loss: 47.1405\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3232 - val_loss: 47.1381\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3260 - val_loss: 47.0614\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3034 - val_loss: 47.1135\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2793 - val_loss: 47.0555\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3452 - val_loss: 47.1292\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3064 - val_loss: 46.9704\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.3046 - val_loss: 47.0083\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.3075 - val_loss: 47.1492\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3405 - val_loss: 46.9537\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3619 - val_loss: 47.2087\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3185 - val_loss: 47.0822\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3116 - val_loss: 47.0765\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3513 - val_loss: 47.0856\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3266 - val_loss: 47.0761\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3269 - val_loss: 47.1133\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3062 - val_loss: 47.1083\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.2957 - val_loss: 47.1562\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3104 - val_loss: 47.1333\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3178 - val_loss: 47.0291\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2898 - val_loss: 47.1209\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2904 - val_loss: 47.0919\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2844 - val_loss: 47.0975\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3189 - val_loss: 47.1550\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2993 - val_loss: 47.0171\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3267 - val_loss: 47.0271\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2939 - val_loss: 47.0908\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3381 - val_loss: 47.2578\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3146 - val_loss: 46.9650\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3026 - val_loss: 47.1727\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2982 - val_loss: 47.0633\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3369 - val_loss: 47.1012\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2855 - val_loss: 46.9883\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2921 - val_loss: 47.1190\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.3371 - val_loss: 47.1119\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.3166 - val_loss: 47.0328\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3067 - val_loss: 47.0358\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2792 - val_loss: 47.1154\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3100 - val_loss: 47.1022\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3412 - val_loss: 47.0436\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3380 - val_loss: 46.9363\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.3327 - val_loss: 47.2096\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2996 - val_loss: 46.9943\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2941 - val_loss: 47.1253\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2853 - val_loss: 47.0514\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2908 - val_loss: 47.0074\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3080 - val_loss: 47.1203\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3305 - val_loss: 47.0429\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3262 - val_loss: 47.0154\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2947 - val_loss: 47.0699\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2956 - val_loss: 47.1488\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3076 - val_loss: 47.3310\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2746 - val_loss: 47.0642\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.3006 - val_loss: 47.0234\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2845 - val_loss: 47.1584\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2867 - val_loss: 47.0403\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3027 - val_loss: 46.9720\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2904 - val_loss: 47.1968\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3181 - val_loss: 47.1118\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2970 - val_loss: 47.0950\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2929 - val_loss: 47.0912\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3034 - val_loss: 46.9861\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2775 - val_loss: 47.0541\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3814 - val_loss: 47.0248\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.2857 - val_loss: 47.1720\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3523 - val_loss: 47.0208\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3046 - val_loss: 47.2436\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3015 - val_loss: 47.1322\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2804 - val_loss: 47.0193\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3139 - val_loss: 47.1918\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3367 - val_loss: 46.9570\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3209 - val_loss: 47.1009\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3455 - val_loss: 47.0894\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3350 - val_loss: 47.0038\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3132 - val_loss: 47.0814\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3063 - val_loss: 47.2368\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2986 - val_loss: 47.0596\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3146 - val_loss: 47.1406\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2751 - val_loss: 46.9946\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3292 - val_loss: 47.1948\n",
      "\n",
      "Mean Squared Error for iteration22: 45.27588030993926\n",
      "\n",
      "Iteration:  23\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2831 - val_loss: 47.0167\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3254 - val_loss: 47.0653\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.2962 - val_loss: 47.0168\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3306 - val_loss: 47.1080\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3206 - val_loss: 47.0312\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.3117 - val_loss: 47.0262\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2979 - val_loss: 47.0497\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3005 - val_loss: 47.0398\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2889 - val_loss: 47.1272\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.3055 - val_loss: 47.1036\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2994 - val_loss: 47.1024\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2916 - val_loss: 47.2534\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3168 - val_loss: 46.9608\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2865 - val_loss: 47.0169\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3096 - val_loss: 47.1018\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2950 - val_loss: 47.1931\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3030 - val_loss: 47.0814\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2893 - val_loss: 47.0117\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.3045 - val_loss: 47.1027\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2953 - val_loss: 47.1531\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2830 - val_loss: 47.0255\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2970 - val_loss: 47.0872\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3153 - val_loss: 47.0650\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3189 - val_loss: 46.9931\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2766 - val_loss: 47.0622\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3014 - val_loss: 47.1411\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3128 - val_loss: 47.1569\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2960 - val_loss: 47.0068\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.2874 - val_loss: 47.0952\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3103 - val_loss: 47.2089\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2788 - val_loss: 47.1261\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2847 - val_loss: 47.0628\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3081 - val_loss: 47.0841\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2971 - val_loss: 47.0342\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2718 - val_loss: 47.0798\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3102 - val_loss: 47.0434\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3046 - val_loss: 47.0348\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3053 - val_loss: 47.0653\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.3196 - val_loss: 47.2784\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2746 - val_loss: 47.1396\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3120 - val_loss: 47.0011\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2834 - val_loss: 47.0770\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2996 - val_loss: 47.0472\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2864 - val_loss: 47.1585\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2975 - val_loss: 47.0973\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3191 - val_loss: 47.0612\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2839 - val_loss: 46.9371\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3280 - val_loss: 47.1174\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3317 - val_loss: 46.9291\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2812 - val_loss: 47.1177\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.2776 - val_loss: 47.1887\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2915 - val_loss: 46.9613\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2956 - val_loss: 47.0549\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2835 - val_loss: 47.0332\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3270 - val_loss: 47.0534\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3001 - val_loss: 47.3712\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2929 - val_loss: 47.1222\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3258 - val_loss: 47.1176\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3316 - val_loss: 46.9985\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2714 - val_loss: 47.1228\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2850 - val_loss: 47.0528\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2906 - val_loss: 47.0671\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2674 - val_loss: 47.1198\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 106us/step - loss: 24.2805 - val_loss: 47.1637\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3185 - val_loss: 47.0496\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.3053 - val_loss: 47.1742\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3058 - val_loss: 47.0445\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2910 - val_loss: 47.0361\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2948 - val_loss: 47.1033\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2954 - val_loss: 46.9424\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3275 - val_loss: 47.1078\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2734 - val_loss: 47.0431\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3132 - val_loss: 46.9887\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2753 - val_loss: 47.1374\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2775 - val_loss: 46.9997\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2666 - val_loss: 47.0532\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2962 - val_loss: 47.0957\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2740 - val_loss: 47.0279\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2912 - val_loss: 47.0335\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.3305 - val_loss: 47.1412\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2895 - val_loss: 47.1047\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3132 - val_loss: 46.9186\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2935 - val_loss: 47.1809\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2654 - val_loss: 47.1234\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3238 - val_loss: 46.9129\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2504 - val_loss: 47.0961\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2725 - val_loss: 47.2065\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2512 - val_loss: 47.0218\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2773 - val_loss: 47.0461\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2401 - val_loss: 47.1525\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2792 - val_loss: 47.1713\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2915 - val_loss: 47.0397\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2407 - val_loss: 47.0040\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2635 - val_loss: 47.0245\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2483 - val_loss: 47.1399\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3394 - val_loss: 47.3280\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.3751 - val_loss: 46.8347\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.3222 - val_loss: 47.0682\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2604 - val_loss: 47.0607\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2687 - val_loss: 47.1235\n",
      "\n",
      "Mean Squared Error for iteration23: 45.26040439718657\n",
      "\n",
      "Iteration:  24\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2449 - val_loss: 47.1277\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2780 - val_loss: 47.0609\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2460 - val_loss: 47.0718\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3071 - val_loss: 47.0519\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2343 - val_loss: 47.1261\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2429 - val_loss: 47.0778\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2567 - val_loss: 47.1538\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2483 - val_loss: 47.1052\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2465 - val_loss: 47.0480\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.3067 - val_loss: 47.1106\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2654 - val_loss: 47.0441\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2453 - val_loss: 47.0531\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2630 - val_loss: 47.1457\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2575 - val_loss: 47.0832\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3200 - val_loss: 46.9471\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 72us/step - loss: 24.2641 - val_loss: 47.1315\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2710 - val_loss: 47.0778\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2556 - val_loss: 47.0357\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2750 - val_loss: 47.1078\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2620 - val_loss: 47.0320\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2570 - val_loss: 47.1320\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2566 - val_loss: 47.1310\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2388 - val_loss: 47.2900\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2998 - val_loss: 47.0567\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2686 - val_loss: 46.9940\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2367 - val_loss: 47.0694\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2865 - val_loss: 47.0238\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2869 - val_loss: 47.1321\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2746 - val_loss: 47.2183\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2404 - val_loss: 47.0037\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2487 - val_loss: 47.1758\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2447 - val_loss: 47.1221\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3153 - val_loss: 46.9721\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2327 - val_loss: 47.1469\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.3323 - val_loss: 47.1363\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2484 - val_loss: 47.0574\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2668 - val_loss: 47.1998\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2380 - val_loss: 47.0797\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2826 - val_loss: 46.8857\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2401 - val_loss: 47.0278\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2763 - val_loss: 47.2315\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.2374 - val_loss: 47.0689\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3021 - val_loss: 47.2790\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2454 - val_loss: 47.1014\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2769 - val_loss: 47.0407\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2863 - val_loss: 46.9104\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2846 - val_loss: 47.0075\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2364 - val_loss: 47.0154\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3576 - val_loss: 47.2608\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2781 - val_loss: 47.0611\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2650 - val_loss: 47.0147\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2765 - val_loss: 47.0855\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.3076 - val_loss: 47.1556\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2611 - val_loss: 47.0995\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.2997 - val_loss: 46.9890\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2240 - val_loss: 47.0439\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2673 - val_loss: 47.0276\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2628 - val_loss: 47.0815\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2320 - val_loss: 47.1513\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2640 - val_loss: 47.0222\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2459 - val_loss: 47.0636\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2502 - val_loss: 47.0691\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2903 - val_loss: 47.2148\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2454 - val_loss: 47.0724\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2641 - val_loss: 47.1560\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2635 - val_loss: 46.9084\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.2595 - val_loss: 47.1380\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2421 - val_loss: 47.0993\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2441 - val_loss: 47.0481\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2602 - val_loss: 47.0210\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2657 - val_loss: 46.9911\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2425 - val_loss: 47.1348\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2570 - val_loss: 47.2225\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2409 - val_loss: 47.0989\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2496 - val_loss: 47.0567\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2138 - val_loss: 46.9962\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2293 - val_loss: 47.0249\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2662 - val_loss: 47.1785\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.3148 - val_loss: 47.0542\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.3596 - val_loss: 47.0930\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2663 - val_loss: 46.9267\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2308 - val_loss: 47.0525\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2537 - val_loss: 47.0619\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2719 - val_loss: 47.1716\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2480 - val_loss: 47.0459\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2370 - val_loss: 47.0843\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2460 - val_loss: 47.2015\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2436 - val_loss: 47.0329\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2433 - val_loss: 47.1062\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2488 - val_loss: 47.0452\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2504 - val_loss: 47.0661\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2476 - val_loss: 47.0280\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.3312 - val_loss: 47.0531\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.2486 - val_loss: 47.0568\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2494 - val_loss: 47.1487\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2289 - val_loss: 47.2093\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2434 - val_loss: 47.1091\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2304 - val_loss: 47.0268\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.2398 - val_loss: 47.0584\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2411 - val_loss: 47.0746\n",
      "\n",
      "Mean Squared Error for iteration24: 45.34065175367527\n",
      "\n",
      "Iteration:  25\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2418 - val_loss: 47.1438\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2417 - val_loss: 47.0595\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2438 - val_loss: 46.9725\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2580 - val_loss: 47.0377\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2624 - val_loss: 46.9731\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2339 - val_loss: 47.0332\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2728 - val_loss: 46.9401\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2566 - val_loss: 47.0001\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2783 - val_loss: 47.0054\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.2997 - val_loss: 47.2015\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2325 - val_loss: 47.0817\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2220 - val_loss: 47.0179\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2562 - val_loss: 47.1096\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2690 - val_loss: 46.9163\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2555 - val_loss: 47.2306\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2387 - val_loss: 47.0457\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2594 - val_loss: 46.9767\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2641 - val_loss: 47.0745\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2270 - val_loss: 47.0313\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2260 - val_loss: 47.0878\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2734 - val_loss: 46.9854\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.3145 - val_loss: 47.1246\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2503 - val_loss: 47.1922\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2603 - val_loss: 46.9729\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2463 - val_loss: 47.0199\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2775 - val_loss: 47.1771\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2745 - val_loss: 47.0464\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2353 - val_loss: 46.9947\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2928 - val_loss: 46.8545\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2639 - val_loss: 47.1408\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2556 - val_loss: 47.1268\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.2402 - val_loss: 47.1381\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2461 - val_loss: 47.0340\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2359 - val_loss: 47.0545\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2108 - val_loss: 47.0400\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2447 - val_loss: 47.0920\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2403 - val_loss: 47.0450\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2225 - val_loss: 46.9829\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2582 - val_loss: 47.0319\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2580 - val_loss: 47.0671\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2380 - val_loss: 46.9631\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2770 - val_loss: 47.2339\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2431 - val_loss: 47.0842\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.2767 - val_loss: 46.9650\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.2237 - val_loss: 47.0441\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2221 - val_loss: 47.0735\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2203 - val_loss: 47.0106\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2535 - val_loss: 47.0414\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2517 - val_loss: 47.1329\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2433 - val_loss: 47.1120\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2344 - val_loss: 47.0091\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2607 - val_loss: 46.9507\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2171 - val_loss: 47.0617\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2286 - val_loss: 47.0864\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2287 - val_loss: 47.0760\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2325 - val_loss: 47.0575\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2285 - val_loss: 47.0782\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2538 - val_loss: 47.0453\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2264 - val_loss: 47.0553\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2310 - val_loss: 47.0135\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2211 - val_loss: 47.0448\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2614 - val_loss: 47.1824\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2236 - val_loss: 47.0334\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2345 - val_loss: 47.0339\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2594 - val_loss: 47.1502\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2082 - val_loss: 46.9430\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2271 - val_loss: 47.0017\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2344 - val_loss: 47.1471\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2324 - val_loss: 47.0365\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2183 - val_loss: 47.0126\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.2526 - val_loss: 47.0467\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2537 - val_loss: 46.9754\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2314 - val_loss: 47.0855\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2454 - val_loss: 46.9564\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2176 - val_loss: 46.9948\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2127 - val_loss: 47.1023\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.2546 - val_loss: 47.1316\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2108 - val_loss: 47.0994\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2090 - val_loss: 47.0259\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2232 - val_loss: 47.0445\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2342 - val_loss: 47.1246\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2368 - val_loss: 46.9962\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2373 - val_loss: 46.9695\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2301 - val_loss: 47.1484\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2558 - val_loss: 47.0500\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2204 - val_loss: 46.9939\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2120 - val_loss: 47.0415\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2340 - val_loss: 47.0969\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2608 - val_loss: 47.0260\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.2188 - val_loss: 46.9785\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.2215 - val_loss: 47.0447\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2366 - val_loss: 47.1675\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2621 - val_loss: 47.2920\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2175 - val_loss: 47.0531\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2222 - val_loss: 47.0763\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2037 - val_loss: 46.9190\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2213 - val_loss: 46.9066\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2382 - val_loss: 46.9613\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.2491 - val_loss: 46.9473\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2613 - val_loss: 47.0827\n",
      "\n",
      "Mean Squared Error for iteration25: 45.349101079289454\n",
      "\n",
      "Iteration:  26\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2453 - val_loss: 47.0038\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2335 - val_loss: 47.1652\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2355 - val_loss: 47.1242\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2065 - val_loss: 47.0038\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2456 - val_loss: 47.0592\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1963 - val_loss: 47.1238\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2376 - val_loss: 47.1696\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2784 - val_loss: 47.0664\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2693 - val_loss: 47.1114\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2982 - val_loss: 47.0062\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2711 - val_loss: 46.8586\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2611 - val_loss: 47.1333\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2537 - val_loss: 47.1338\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2099 - val_loss: 47.0892\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2290 - val_loss: 47.0302\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2071 - val_loss: 47.0329\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1986 - val_loss: 46.9814\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2871 - val_loss: 46.9015\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2505 - val_loss: 47.1615\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2123 - val_loss: 47.0937\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2076 - val_loss: 47.1231\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2885 - val_loss: 47.0597\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2478 - val_loss: 47.1458\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2770 - val_loss: 47.1334\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2488 - val_loss: 47.2020\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2611 - val_loss: 46.9568\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1980 - val_loss: 47.0409\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2246 - val_loss: 47.1150\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2033 - val_loss: 47.0709\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2259 - val_loss: 47.0325\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2536 - val_loss: 47.1430\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2411 - val_loss: 46.9458\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2447 - val_loss: 47.0896\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2318 - val_loss: 47.0495\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.2665 - val_loss: 47.1077\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2357 - val_loss: 47.1151\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2254 - val_loss: 46.8867\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2188 - val_loss: 47.1285\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2439 - val_loss: 47.0885\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2264 - val_loss: 47.1937\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2312 - val_loss: 47.0146\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2284 - val_loss: 46.9950\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2164 - val_loss: 46.9879\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2502 - val_loss: 47.1227\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.2237 - val_loss: 47.0465\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2102 - val_loss: 47.1737\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.3057 - val_loss: 46.9056\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.2694 - val_loss: 47.1366\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2086 - val_loss: 47.0684\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.2041 - val_loss: 46.9796\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2695 - val_loss: 47.0729\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2112 - val_loss: 47.1127\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2154 - val_loss: 47.0531\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2330 - val_loss: 47.0290\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2425 - val_loss: 47.1249\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2060 - val_loss: 46.9964\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2281 - val_loss: 47.0177\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2430 - val_loss: 47.0635\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2344 - val_loss: 47.1097\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2268 - val_loss: 47.2068\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2654 - val_loss: 47.1756\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2332 - val_loss: 47.0045\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2508 - val_loss: 46.9180\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2054 - val_loss: 46.9911\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2707 - val_loss: 47.0565\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2263 - val_loss: 47.2300\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2606 - val_loss: 47.0593\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.2488 - val_loss: 47.1927\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2329 - val_loss: 47.0311\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2374 - val_loss: 47.0332\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2351 - val_loss: 46.9804\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.2361 - val_loss: 47.1280\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2103 - val_loss: 47.0109\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2458 - val_loss: 47.2039\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1883 - val_loss: 47.0884\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2226 - val_loss: 47.1154\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2146 - val_loss: 47.1370\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2352 - val_loss: 46.9567\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2003 - val_loss: 47.1369\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 24.2302 - val_loss: 47.0156\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2368 - val_loss: 46.9935\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2588 - val_loss: 47.2291\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2225 - val_loss: 47.2701\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2573 - val_loss: 46.9677\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1923 - val_loss: 47.0291\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2424 - val_loss: 47.1632\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2154 - val_loss: 47.0150\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2027 - val_loss: 47.0591\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1979 - val_loss: 46.9790\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2034 - val_loss: 47.0819\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1951 - val_loss: 47.1403\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2026 - val_loss: 47.1264\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2010 - val_loss: 46.9927\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2191 - val_loss: 47.1124\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2297 - val_loss: 47.1378\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2548 - val_loss: 46.9411\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2867 - val_loss: 47.2379\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2566 - val_loss: 46.9405\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2232 - val_loss: 47.1069\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2240 - val_loss: 47.1036\n",
      "\n",
      "Mean Squared Error for iteration26: 45.3749128792963\n",
      "\n",
      "Iteration:  27\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2050 - val_loss: 46.9444\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1957 - val_loss: 47.0641\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2319 - val_loss: 47.2436\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2737 - val_loss: 47.2086\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2163 - val_loss: 46.9336\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2010 - val_loss: 47.0656\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2164 - val_loss: 47.0237\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2136 - val_loss: 47.2544\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2021 - val_loss: 47.0607\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2208 - val_loss: 47.1367\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2286 - val_loss: 47.0250\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.2479 - val_loss: 46.9846\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1960 - val_loss: 47.0276\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2472 - val_loss: 47.0576\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1992 - val_loss: 47.1376\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2898 - val_loss: 47.1944\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2594 - val_loss: 47.0647\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2078 - val_loss: 47.0402\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2113 - val_loss: 47.1095\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2138 - val_loss: 47.0228\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1942 - val_loss: 47.1166\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2678 - val_loss: 47.1560\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1931 - val_loss: 47.1029\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.1788 - val_loss: 47.0500\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 87us/step - loss: 24.2042 - val_loss: 47.1191\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1898 - val_loss: 47.0224\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2178 - val_loss: 46.9026\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2313 - val_loss: 47.0812\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1978 - val_loss: 47.1876\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2047 - val_loss: 47.1641\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3069 - val_loss: 47.1209\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2048 - val_loss: 46.9686\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2269 - val_loss: 46.9126\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.2415 - val_loss: 47.2180\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1828 - val_loss: 47.0453\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2274 - val_loss: 46.9888\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2012 - val_loss: 46.9859\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1828 - val_loss: 47.0833\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.2042 - val_loss: 47.0741\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2436 - val_loss: 47.1207\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2266 - val_loss: 47.2256\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2069 - val_loss: 47.1812\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1976 - val_loss: 47.0967\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.2108 - val_loss: 47.0063\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.3203 - val_loss: 47.0253\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2000 - val_loss: 46.9786\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1843 - val_loss: 47.0493\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2465 - val_loss: 47.0535\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2186 - val_loss: 47.0362\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2118 - val_loss: 47.1299\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1882 - val_loss: 47.1116\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2239 - val_loss: 46.9804\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2481 - val_loss: 47.0679\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1884 - val_loss: 47.0050\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2175 - val_loss: 47.1980\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2217 - val_loss: 47.0777\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2265 - val_loss: 47.1261\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1913 - val_loss: 47.0923\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2470 - val_loss: 47.1279\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2131 - val_loss: 47.0189\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1728 - val_loss: 47.0486\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1884 - val_loss: 47.1169\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2627 - val_loss: 47.1713\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2054 - val_loss: 47.1315\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1851 - val_loss: 47.0108\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2210 - val_loss: 47.1627\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1861 - val_loss: 47.1160\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2022 - val_loss: 46.9887\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1984 - val_loss: 47.0143\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 24.1979 - val_loss: 47.1974\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2061 - val_loss: 46.9929\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2327 - val_loss: 47.0022\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1910 - val_loss: 47.0478\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2767 - val_loss: 47.0457\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2150 - val_loss: 47.1510\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2091 - val_loss: 47.0745\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1846 - val_loss: 47.0770\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2234 - val_loss: 47.1151\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1937 - val_loss: 47.0291\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1907 - val_loss: 47.0321\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1900 - val_loss: 47.0213\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2162 - val_loss: 47.3111\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1975 - val_loss: 47.0359\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1899 - val_loss: 47.0018\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2396 - val_loss: 47.1200\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1918 - val_loss: 47.0765\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1902 - val_loss: 47.0742\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1842 - val_loss: 47.1087\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2335 - val_loss: 47.2481\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1679 - val_loss: 47.0645\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2298 - val_loss: 46.9240\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1810 - val_loss: 47.0447\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2502 - val_loss: 47.1247\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2371 - val_loss: 47.1087\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1905 - val_loss: 47.0906\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2037 - val_loss: 47.1743\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1732 - val_loss: 47.0884\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2108 - val_loss: 47.2388\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2415 - val_loss: 46.9604\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2194 - val_loss: 47.1246\n",
      "\n",
      "Mean Squared Error for iteration27: 45.406901144860676\n",
      "\n",
      "Iteration:  28\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1770 - val_loss: 47.1278\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 81us/step - loss: 24.2237 - val_loss: 47.1267\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2015 - val_loss: 47.0548\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2024 - val_loss: 47.0332\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2089 - val_loss: 47.0885\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2250 - val_loss: 47.0256\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2571 - val_loss: 47.1687\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2413 - val_loss: 47.1261\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2350 - val_loss: 47.0370\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1959 - val_loss: 47.0284\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2126 - val_loss: 46.9443\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1779 - val_loss: 47.1310\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2184 - val_loss: 47.1261\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2472 - val_loss: 47.1086\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2214 - val_loss: 47.1499\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2436 - val_loss: 47.2026\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2011 - val_loss: 47.0050\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1909 - val_loss: 47.1365\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1962 - val_loss: 46.9604\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2315 - val_loss: 47.0938\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1997 - val_loss: 47.0105\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1850 - val_loss: 47.1127\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2182 - val_loss: 46.9386\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1779 - val_loss: 47.0701\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1818 - val_loss: 47.1732\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2228 - val_loss: 47.2286\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2112 - val_loss: 46.9206\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2382 - val_loss: 47.0987\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1737 - val_loss: 47.1089\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2227 - val_loss: 47.0381\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1903 - val_loss: 47.1213\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1692 - val_loss: 46.9380\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1920 - val_loss: 47.1013\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1736 - val_loss: 47.0060\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1968 - val_loss: 47.1783\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1819 - val_loss: 47.0451\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1906 - val_loss: 47.1699\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2100 - val_loss: 47.0304\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2006 - val_loss: 47.1223\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1853 - val_loss: 47.1119\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1965 - val_loss: 46.9277\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1759 - val_loss: 47.0910\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1734 - val_loss: 47.0989\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.2134 - val_loss: 47.0276\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2538 - val_loss: 47.1983\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2064 - val_loss: 47.0807\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.2080 - val_loss: 47.0061\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1567 - val_loss: 46.9914\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2279 - val_loss: 47.1299\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1715 - val_loss: 47.0095\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2182 - val_loss: 47.1520\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2236 - val_loss: 47.0066\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2038 - val_loss: 46.9563\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1967 - val_loss: 47.1634\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2456 - val_loss: 47.1394\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1700 - val_loss: 47.1211\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2143 - val_loss: 46.9665\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2087 - val_loss: 46.9104\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1964 - val_loss: 47.1380\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.1793 - val_loss: 47.1481\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2675 - val_loss: 47.2577\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2089 - val_loss: 46.9366\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1907 - val_loss: 46.9402\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2401 - val_loss: 47.0130\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1815 - val_loss: 47.1227\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1606 - val_loss: 47.0338\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2013 - val_loss: 47.0508\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1608 - val_loss: 47.0728\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1952 - val_loss: 47.1215\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1643 - val_loss: 47.0984\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2040 - val_loss: 46.9772\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2317 - val_loss: 47.0743\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1910 - val_loss: 47.0765\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1705 - val_loss: 47.0803\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2140 - val_loss: 47.0426\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2104 - val_loss: 46.9188\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2714 - val_loss: 47.1491\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2125 - val_loss: 47.0916\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1748 - val_loss: 47.0942\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.1814 - val_loss: 47.2029\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1902 - val_loss: 47.0688\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2054 - val_loss: 47.0167\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1914 - val_loss: 47.0747\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2019 - val_loss: 47.1831\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1820 - val_loss: 47.1192\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2077 - val_loss: 46.9064\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2071 - val_loss: 47.1574\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2255 - val_loss: 47.2551\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2067 - val_loss: 47.0408\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.1457 - val_loss: 47.0559\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1686 - val_loss: 46.9711\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1939 - val_loss: 47.1269\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1826 - val_loss: 47.0773\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1872 - val_loss: 46.9897\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1861 - val_loss: 47.0731\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.1869 - val_loss: 47.0977\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2032 - val_loss: 47.0249\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2440 - val_loss: 47.0234\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1756 - val_loss: 47.0438\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2040 - val_loss: 47.2273\n",
      "\n",
      "Mean Squared Error for iteration28: 45.51930023164778\n",
      "\n",
      "Iteration:  29\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1975 - val_loss: 47.0853\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1819 - val_loss: 47.1161\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2364 - val_loss: 47.2266\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1757 - val_loss: 47.0512\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1932 - val_loss: 46.9431\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1658 - val_loss: 46.9828\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.1865 - val_loss: 47.0766\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1631 - val_loss: 47.0683\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2325 - val_loss: 47.0746\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1835 - val_loss: 47.0043\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2173 - val_loss: 47.1172\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2211 - val_loss: 46.9560\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1989 - val_loss: 47.1396\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1804 - val_loss: 47.1668\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1620 - val_loss: 47.0037\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2146 - val_loss: 46.9787\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1993 - val_loss: 47.1787\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1972 - val_loss: 47.0332\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1963 - val_loss: 46.9191\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1989 - val_loss: 47.1147\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2011 - val_loss: 47.0472\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1743 - val_loss: 47.0555\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1876 - val_loss: 47.0031\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2352 - val_loss: 47.0578\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1894 - val_loss: 46.9686\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2194 - val_loss: 47.1434\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1896 - val_loss: 47.0261\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1869 - val_loss: 47.0216\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2008 - val_loss: 47.1501\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.1749 - val_loss: 47.0352\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 67us/step - loss: 24.1973 - val_loss: 47.0787\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.1773 - val_loss: 46.9683\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.1825 - val_loss: 47.0662\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2181 - val_loss: 47.0766\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1647 - val_loss: 46.9888\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1614 - val_loss: 47.0914\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1971 - val_loss: 47.1402\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1930 - val_loss: 47.1472\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2230 - val_loss: 47.0871\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1535 - val_loss: 46.9535\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2784 - val_loss: 47.1099\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1605 - val_loss: 47.0205\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2245 - val_loss: 46.9598\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1641 - val_loss: 47.1552\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2029 - val_loss: 47.0242\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2014 - val_loss: 47.0047\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2177 - val_loss: 47.0960\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1601 - val_loss: 47.0783\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1470 - val_loss: 47.1567\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.1658 - val_loss: 47.0951\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2118 - val_loss: 47.2185\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.2083 - val_loss: 47.0625\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1817 - val_loss: 47.0020\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1808 - val_loss: 46.9283\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1946 - val_loss: 47.2531\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2348 - val_loss: 47.0522\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.1543 - val_loss: 47.0118\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1855 - val_loss: 47.0645\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1998 - val_loss: 46.9789\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2006 - val_loss: 47.0753\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1974 - val_loss: 47.1135\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.2029 - val_loss: 47.0126\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1988 - val_loss: 47.1582\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1977 - val_loss: 46.9815\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1859 - val_loss: 47.0456\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1701 - val_loss: 47.1033\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1828 - val_loss: 47.0403\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1502 - val_loss: 47.0499\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1794 - val_loss: 47.0737\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1846 - val_loss: 47.0763\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1847 - val_loss: 46.9538\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1989 - val_loss: 46.9258\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1367 - val_loss: 47.0805\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1916 - val_loss: 47.0938\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1581 - val_loss: 47.0401\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1801 - val_loss: 47.0226\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1811 - val_loss: 47.1606\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.2374 - val_loss: 47.1013\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1612 - val_loss: 47.0428\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2035 - val_loss: 47.0094\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1797 - val_loss: 47.1293\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1753 - val_loss: 47.0043\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1999 - val_loss: 46.9608\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1926 - val_loss: 47.0434\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1636 - val_loss: 46.9706\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2022 - val_loss: 47.0836\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1835 - val_loss: 47.1020\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2202 - val_loss: 47.2285\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1632 - val_loss: 47.0182\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1994 - val_loss: 46.8273\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1977 - val_loss: 47.1057\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2060 - val_loss: 47.1310\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1832 - val_loss: 47.0378\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1812 - val_loss: 47.0975\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.2408 - val_loss: 46.8977\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1746 - val_loss: 47.0657\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2070 - val_loss: 47.0959\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1914 - val_loss: 46.9807\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1515 - val_loss: 47.1067\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1647 - val_loss: 46.9966\n",
      "\n",
      "Mean Squared Error for iteration29: 45.093611372853246\n",
      "\n",
      "Iteration:  30\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.2475 - val_loss: 47.2060\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1576 - val_loss: 47.0530\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1923 - val_loss: 46.9351\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1489 - val_loss: 46.9890\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1696 - val_loss: 47.1287\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1740 - val_loss: 46.9703\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1892 - val_loss: 47.1263\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1986 - val_loss: 47.1293\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1586 - val_loss: 47.0245\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2038 - val_loss: 47.0131\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1695 - val_loss: 47.0504\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2124 - val_loss: 47.0557\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2002 - val_loss: 46.9737\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1778 - val_loss: 47.1500\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1623 - val_loss: 47.1044\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1627 - val_loss: 47.0162\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.1936 - val_loss: 46.9027\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2389 - val_loss: 47.1423\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1738 - val_loss: 47.0385\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1748 - val_loss: 46.9544\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1844 - val_loss: 46.8787\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2371 - val_loss: 47.0701\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1480 - val_loss: 47.0861\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2448 - val_loss: 47.1430\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1572 - val_loss: 47.0921\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2169 - val_loss: 47.0403\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2241 - val_loss: 46.8491\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2302 - val_loss: 47.1466\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1882 - val_loss: 47.0451\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1865 - val_loss: 47.0029\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1699 - val_loss: 46.9674\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1560 - val_loss: 46.9221\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.2033 - val_loss: 46.9394\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.1954 - val_loss: 47.0951\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1929 - val_loss: 46.9994\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1898 - val_loss: 47.0306\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1549 - val_loss: 47.2527\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2152 - val_loss: 47.1037\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.2218 - val_loss: 47.1973\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.1812 - val_loss: 47.1092\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1598 - val_loss: 47.0465\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1493 - val_loss: 46.9477\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2400 - val_loss: 47.1839\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1745 - val_loss: 46.9551\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2308 - val_loss: 46.8583\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2239 - val_loss: 47.0334\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1817 - val_loss: 47.0511\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1941 - val_loss: 47.0542\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1576 - val_loss: 47.0214\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1713 - val_loss: 47.0438\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2173 - val_loss: 46.9923\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1497 - val_loss: 47.0347\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2051 - val_loss: 47.1616\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2054 - val_loss: 46.9340\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1560 - val_loss: 47.0569\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1826 - val_loss: 46.9253\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1589 - val_loss: 46.9864\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1485 - val_loss: 46.9905\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1922 - val_loss: 47.0676\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1775 - val_loss: 47.1074\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1689 - val_loss: 47.0029\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1649 - val_loss: 47.0363\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2079 - val_loss: 47.0563\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1726 - val_loss: 47.0116\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1841 - val_loss: 46.9205\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1894 - val_loss: 47.0498\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1959 - val_loss: 46.9802\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1621 - val_loss: 46.9583\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2033 - val_loss: 46.9770\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1823 - val_loss: 47.0967\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1810 - val_loss: 47.0906\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1730 - val_loss: 46.9498\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2038 - val_loss: 47.1744\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2098 - val_loss: 47.0772\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1835 - val_loss: 47.0081\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1516 - val_loss: 47.0179\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1786 - val_loss: 47.1414\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1811 - val_loss: 46.9316\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1645 - val_loss: 47.0509\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1505 - val_loss: 47.1660\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1788 - val_loss: 47.1033\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1673 - val_loss: 46.9300\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1958 - val_loss: 46.9276\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2040 - val_loss: 46.9927\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.1517 - val_loss: 47.0643\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1724 - val_loss: 47.0969\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1705 - val_loss: 46.9788\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1937 - val_loss: 47.1023\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1818 - val_loss: 47.0497\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1903 - val_loss: 47.0329\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1586 - val_loss: 47.0686\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1785 - val_loss: 47.1167\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1876 - val_loss: 46.9775\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2411 - val_loss: 47.1543\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1645 - val_loss: 47.0432\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2357 - val_loss: 46.9260\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1780 - val_loss: 47.0972\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2093 - val_loss: 47.0690\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2279 - val_loss: 47.0443\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1823 - val_loss: 47.0384\n",
      "\n",
      "Mean Squared Error for iteration30: 45.20518874173627\n",
      "\n",
      "Iteration:  31\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1478 - val_loss: 47.0699\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2157 - val_loss: 46.9589\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1846 - val_loss: 46.9544\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1689 - val_loss: 47.1134\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2158 - val_loss: 46.9011\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.2034 - val_loss: 47.2895\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1872 - val_loss: 47.1282\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1713 - val_loss: 46.9080\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1703 - val_loss: 47.0529\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1396 - val_loss: 47.0242\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.1781 - val_loss: 46.9711\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1823 - val_loss: 46.9222\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1560 - val_loss: 47.1233\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.2228 - val_loss: 47.2765\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1664 - val_loss: 47.0037\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1879 - val_loss: 47.0239\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1670 - val_loss: 47.0254\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1974 - val_loss: 47.1761\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1809 - val_loss: 46.8588\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1817 - val_loss: 47.0261\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1883 - val_loss: 46.9199\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1629 - val_loss: 47.0357\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1853 - val_loss: 47.1837\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1545 - val_loss: 47.1661\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2168 - val_loss: 47.0607\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2041 - val_loss: 47.0385\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2208 - val_loss: 47.0635\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1399 - val_loss: 46.9963\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1667 - val_loss: 46.9856\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1597 - val_loss: 47.0616\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1648 - val_loss: 47.0655\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1772 - val_loss: 47.1257\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1542 - val_loss: 47.1278\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1875 - val_loss: 46.9583\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1614 - val_loss: 47.0310\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1901 - val_loss: 46.9081\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2014 - val_loss: 47.1601\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1641 - val_loss: 47.0414\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1909 - val_loss: 47.0919\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1716 - val_loss: 46.9553\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1442 - val_loss: 46.9126\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.1765 - val_loss: 47.1042\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1821 - val_loss: 47.0078\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1908 - val_loss: 47.1028\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1749 - val_loss: 46.9306\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.2101 - val_loss: 47.1162\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1701 - val_loss: 47.1725\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1782 - val_loss: 46.9890\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1622 - val_loss: 46.9772\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1776 - val_loss: 47.0346\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.2009 - val_loss: 47.0230\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1707 - val_loss: 46.9988\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1539 - val_loss: 47.0092\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1744 - val_loss: 46.9004\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1935 - val_loss: 47.1908\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1565 - val_loss: 46.9660\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1597 - val_loss: 46.9585\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1741 - val_loss: 47.1603\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1606 - val_loss: 47.1107\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1858 - val_loss: 46.8971\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.2016 - val_loss: 47.1600\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1582 - val_loss: 46.9474\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1893 - val_loss: 46.9659\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1428 - val_loss: 47.0733\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1657 - val_loss: 47.0187\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1740 - val_loss: 47.1257\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2089 - val_loss: 46.8604\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2495 - val_loss: 46.9583\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1813 - val_loss: 47.0338\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1970 - val_loss: 47.1094\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1731 - val_loss: 46.9199\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1530 - val_loss: 47.0271\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1952 - val_loss: 47.1032\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.2121 - val_loss: 47.0234\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1415 - val_loss: 47.0130\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1843 - val_loss: 46.9872\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2440 - val_loss: 47.1531\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2021 - val_loss: 46.9645\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1601 - val_loss: 46.9535\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2092 - val_loss: 47.1276\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1347 - val_loss: 46.9843\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.2588 - val_loss: 47.2295\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1834 - val_loss: 46.8942\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.1369 - val_loss: 46.9289\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1449 - val_loss: 47.0968\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1322 - val_loss: 47.0320\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1925 - val_loss: 47.1694\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1638 - val_loss: 46.9165\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.2092 - val_loss: 47.0216\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1575 - val_loss: 46.9529\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1670 - val_loss: 46.9993\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1541 - val_loss: 47.0541\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1599 - val_loss: 47.0986\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1809 - val_loss: 46.8933\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1320 - val_loss: 46.9859\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1805 - val_loss: 47.0793\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1503 - val_loss: 46.9718\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1884 - val_loss: 47.0280\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1433 - val_loss: 47.0771\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1684 - val_loss: 46.9942\n",
      "\n",
      "Mean Squared Error for iteration31: 45.12969201394098\n",
      "\n",
      "Iteration:  32\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 68us/step - loss: 24.1417 - val_loss: 47.0515\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1619 - val_loss: 47.0877\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1718 - val_loss: 46.9086\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1449 - val_loss: 46.9966\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1603 - val_loss: 46.9051\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1776 - val_loss: 46.9651\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2087 - val_loss: 47.0294\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1593 - val_loss: 47.1807\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1306 - val_loss: 47.0671\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1848 - val_loss: 46.9828\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1576 - val_loss: 47.0514\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1981 - val_loss: 47.1277\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1513 - val_loss: 46.9872\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1988 - val_loss: 46.9911\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1368 - val_loss: 46.9546\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1616 - val_loss: 47.1435\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1945 - val_loss: 46.9859\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1425 - val_loss: 47.0016\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.1874 - val_loss: 47.1268\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1643 - val_loss: 46.8735\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1321 - val_loss: 46.9442\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1515 - val_loss: 47.0936\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1810 - val_loss: 47.0309\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1388 - val_loss: 46.9924\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1608 - val_loss: 47.0741\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1815 - val_loss: 46.9777\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1260 - val_loss: 47.0417\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2496 - val_loss: 47.1089\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1738 - val_loss: 46.8339\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1455 - val_loss: 46.9848\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1495 - val_loss: 47.0972\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1689 - val_loss: 47.1588\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2237 - val_loss: 47.1069\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1443 - val_loss: 47.0221\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1574 - val_loss: 46.9116\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1574 - val_loss: 46.9982\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1499 - val_loss: 46.9214\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1666 - val_loss: 47.1019\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1489 - val_loss: 47.0138\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1778 - val_loss: 47.0850\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1694 - val_loss: 46.9477\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1322 - val_loss: 47.1138\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1673 - val_loss: 47.0572\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1674 - val_loss: 46.8477\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1690 - val_loss: 47.0888\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1664 - val_loss: 47.0005\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2378 - val_loss: 47.0403\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2079 - val_loss: 47.0626\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1390 - val_loss: 47.0324\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1691 - val_loss: 47.1174\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1234 - val_loss: 46.9216\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.1469 - val_loss: 46.9395\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1858 - val_loss: 47.0725\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1331 - val_loss: 47.0735\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1693 - val_loss: 46.9897\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1548 - val_loss: 47.1496\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2191 - val_loss: 46.8837\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1222 - val_loss: 47.0011\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1652 - val_loss: 46.9534\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1383 - val_loss: 47.0143\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1616 - val_loss: 47.1675\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1541 - val_loss: 46.9744\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1610 - val_loss: 46.8243\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.2177 - val_loss: 47.1178\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1348 - val_loss: 47.1041\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 80us/step - loss: 24.1633 - val_loss: 47.0484\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1528 - val_loss: 46.9074\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1388 - val_loss: 46.9883\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1598 - val_loss: 47.0418\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1472 - val_loss: 47.1031\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1703 - val_loss: 47.0700\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1928 - val_loss: 47.0109\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1779 - val_loss: 47.1146\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1681 - val_loss: 46.9905\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1916 - val_loss: 47.0970\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 24.1538 - val_loss: 46.9084\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1475 - val_loss: 46.9602\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1958 - val_loss: 47.0616\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1824 - val_loss: 47.0231\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1739 - val_loss: 47.0180\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1749 - val_loss: 46.9839\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1966 - val_loss: 47.0521\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1546 - val_loss: 47.0587\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1739 - val_loss: 46.9190\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1671 - val_loss: 47.0954\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1742 - val_loss: 47.1953\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1399 - val_loss: 46.8835\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1770 - val_loss: 46.9584\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1601 - val_loss: 47.0149\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2363 - val_loss: 46.7114\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1878 - val_loss: 47.0532\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1764 - val_loss: 47.0058\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1452 - val_loss: 47.0541\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1228 - val_loss: 46.9646\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1460 - val_loss: 46.9573\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1370 - val_loss: 47.0886\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1285 - val_loss: 46.9848\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1813 - val_loss: 46.9066\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2051 - val_loss: 47.1781\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.2051 - val_loss: 46.8006\n",
      "\n",
      "Mean Squared Error for iteration32: 45.037755477561404\n",
      "\n",
      "Iteration:  33\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1782 - val_loss: 47.0087\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1931 - val_loss: 47.0522\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1361 - val_loss: 46.9954\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1278 - val_loss: 47.0178\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2010 - val_loss: 46.9951\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1179 - val_loss: 46.9897\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1432 - val_loss: 46.9966\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.1746 - val_loss: 47.1467\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1533 - val_loss: 46.9095\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.2570 - val_loss: 47.1773\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1783 - val_loss: 46.9055\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1373 - val_loss: 46.9437\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1774 - val_loss: 46.9410\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1683 - val_loss: 46.9484\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1536 - val_loss: 47.0215\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2001 - val_loss: 47.0103\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1671 - val_loss: 46.9983\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1391 - val_loss: 46.9739\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1452 - val_loss: 47.1173\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1660 - val_loss: 46.9803\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1331 - val_loss: 47.0113\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1446 - val_loss: 46.9799\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1416 - val_loss: 46.9282\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1624 - val_loss: 46.9086\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1873 - val_loss: 46.9585\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1550 - val_loss: 46.9474\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1372 - val_loss: 46.9795\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1693 - val_loss: 47.0978\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1504 - val_loss: 47.0023\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1282 - val_loss: 47.0857\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1361 - val_loss: 47.0170\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1218 - val_loss: 46.9882\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1482 - val_loss: 46.8822\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1376 - val_loss: 47.0084\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1560 - val_loss: 47.1041\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1356 - val_loss: 47.0293\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1283 - val_loss: 46.9450\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1436 - val_loss: 46.8314\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1552 - val_loss: 46.9899\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1609 - val_loss: 46.9389\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.1446 - val_loss: 46.9499\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1234 - val_loss: 47.0562\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.1762 - val_loss: 47.1183\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1375 - val_loss: 47.0418\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1322 - val_loss: 46.9113\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1572 - val_loss: 46.9059\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1150 - val_loss: 46.9639\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1369 - val_loss: 46.9627\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1892 - val_loss: 47.1202\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1074 - val_loss: 46.9785\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1646 - val_loss: 46.8960\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1310 - val_loss: 46.8611\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1921 - val_loss: 46.9524\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1657 - val_loss: 47.0755\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1646 - val_loss: 46.9973\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1527 - val_loss: 46.9762\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1684 - val_loss: 46.9573\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1351 - val_loss: 47.0484\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1915 - val_loss: 46.9020\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1415 - val_loss: 46.9296\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2195 - val_loss: 47.0134\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1309 - val_loss: 47.0210\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1560 - val_loss: 46.8417\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1440 - val_loss: 47.0416\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1323 - val_loss: 46.9970\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1338 - val_loss: 46.8838\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1568 - val_loss: 46.8062\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1582 - val_loss: 47.0901\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1306 - val_loss: 46.9730\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1597 - val_loss: 47.0703\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1422 - val_loss: 46.9165\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.2398 - val_loss: 47.1124\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1315 - val_loss: 46.8846\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1351 - val_loss: 46.8858\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1612 - val_loss: 46.9190\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1434 - val_loss: 46.9793\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1749 - val_loss: 46.8266\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1524 - val_loss: 47.1539\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1449 - val_loss: 46.9480\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1672 - val_loss: 46.9948\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1696 - val_loss: 47.0568\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1371 - val_loss: 46.8900\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1518 - val_loss: 47.0312\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1651 - val_loss: 46.8987\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1162 - val_loss: 46.9934\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.1147 - val_loss: 46.9488\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2200 - val_loss: 47.0195\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1275 - val_loss: 46.9095\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1297 - val_loss: 47.0233\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1440 - val_loss: 46.8886\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1588 - val_loss: 46.8985\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1752 - val_loss: 47.0074\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1450 - val_loss: 46.9636\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1567 - val_loss: 46.9297\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1791 - val_loss: 47.0291\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1682 - val_loss: 46.8726\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1164 - val_loss: 46.8833\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.1193 - val_loss: 46.9972\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.1623 - val_loss: 47.0430\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1387 - val_loss: 47.0090\n",
      "\n",
      "Mean Squared Error for iteration33: 45.15856432968037\n",
      "\n",
      "Iteration:  34\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1313 - val_loss: 46.9780\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1898 - val_loss: 46.9492\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1631 - val_loss: 46.9872\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1723 - val_loss: 46.9483\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1571 - val_loss: 47.0156\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.2078 - val_loss: 46.9191\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1136 - val_loss: 46.9528\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1391 - val_loss: 46.8858\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1648 - val_loss: 47.0829\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1700 - val_loss: 46.8571\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1321 - val_loss: 47.0087\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2284 - val_loss: 46.9813\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1436 - val_loss: 46.8775\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1408 - val_loss: 46.9090\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1389 - val_loss: 46.9729\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1475 - val_loss: 46.9932\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1412 - val_loss: 46.9899\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1254 - val_loss: 46.9832\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1700 - val_loss: 46.8876\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 75us/step - loss: 24.1519 - val_loss: 46.9010\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1706 - val_loss: 47.0342\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1536 - val_loss: 46.9861\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1637 - val_loss: 46.8603\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1485 - val_loss: 47.0405\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1692 - val_loss: 46.7763\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1282 - val_loss: 46.9906\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1424 - val_loss: 47.0194\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1413 - val_loss: 46.8673\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1266 - val_loss: 46.9066\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1836 - val_loss: 46.9468\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.2046 - val_loss: 46.9778\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1544 - val_loss: 46.8919\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1952 - val_loss: 46.9190\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1361 - val_loss: 46.9263\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1641 - val_loss: 46.9898\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1460 - val_loss: 46.8828\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1616 - val_loss: 46.9192\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1905 - val_loss: 46.7908\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1415 - val_loss: 47.0390\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1518 - val_loss: 47.0421\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1194 - val_loss: 46.9888\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1347 - val_loss: 46.8584\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 24.1347 - val_loss: 46.8697\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1217 - val_loss: 46.9809\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1181 - val_loss: 46.9380\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1357 - val_loss: 46.9179\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1669 - val_loss: 46.9781\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1966 - val_loss: 46.7731\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1444 - val_loss: 46.9493\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1462 - val_loss: 46.9291\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1516 - val_loss: 46.8108\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1253 - val_loss: 46.9355\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1632 - val_loss: 46.9116\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1343 - val_loss: 47.0351\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1485 - val_loss: 47.0121\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1715 - val_loss: 46.7549\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1906 - val_loss: 46.9980\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1696 - val_loss: 46.9888\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1238 - val_loss: 46.8878\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1212 - val_loss: 46.8669\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1706 - val_loss: 46.9045\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1271 - val_loss: 47.0216\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1778 - val_loss: 46.9288\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1173 - val_loss: 46.8318\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1533 - val_loss: 46.9013\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1155 - val_loss: 46.8848\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1389 - val_loss: 46.8296\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1705 - val_loss: 47.1205\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1489 - val_loss: 46.8990\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1237 - val_loss: 46.8643\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1991 - val_loss: 46.9694\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1345 - val_loss: 46.9029\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1226 - val_loss: 46.8607\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1227 - val_loss: 46.9208\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1395 - val_loss: 46.9118\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1297 - val_loss: 46.9699\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1205 - val_loss: 46.9395\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1399 - val_loss: 46.8299\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1667 - val_loss: 47.0150\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1420 - val_loss: 46.9735\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1660 - val_loss: 46.8642\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1448 - val_loss: 46.8536\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1140 - val_loss: 46.9023\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1340 - val_loss: 46.9104\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1908 - val_loss: 47.0238\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1238 - val_loss: 46.9743\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.1374 - val_loss: 46.9545\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1768 - val_loss: 46.7960\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1927 - val_loss: 46.8978\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1786 - val_loss: 46.9640\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1839 - val_loss: 46.9941\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1511 - val_loss: 46.9492\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1313 - val_loss: 46.7794\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1394 - val_loss: 46.8380\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1986 - val_loss: 46.8797\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2213 - val_loss: 46.8763\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.1308 - val_loss: 46.8875\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.1403 - val_loss: 46.9935\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1400 - val_loss: 46.8867\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1398 - val_loss: 46.9722\n",
      "\n",
      "Mean Squared Error for iteration34: 45.27829554371088\n",
      "\n",
      "Iteration:  35\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1743 - val_loss: 46.7420\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1373 - val_loss: 46.8851\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1626 - val_loss: 47.0029\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1201 - val_loss: 46.8731\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1084 - val_loss: 46.8495\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1266 - val_loss: 46.8827\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1869 - val_loss: 46.9347\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1550 - val_loss: 46.9555\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1088 - val_loss: 46.9604\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1419 - val_loss: 46.8841\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1498 - val_loss: 46.9233\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1507 - val_loss: 46.8033\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1477 - val_loss: 47.0899\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.2032 - val_loss: 46.9611\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1385 - val_loss: 46.9409\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1881 - val_loss: 46.9500\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1429 - val_loss: 46.8292\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1629 - val_loss: 46.9274\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1786 - val_loss: 46.8662\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1560 - val_loss: 47.0087\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1330 - val_loss: 47.0787\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1336 - val_loss: 46.8509\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1171 - val_loss: 46.8406\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1428 - val_loss: 46.9909\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1387 - val_loss: 46.9217\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1254 - val_loss: 46.8869\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1369 - val_loss: 46.8806\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1087 - val_loss: 46.9502\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1243 - val_loss: 46.8586\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1454 - val_loss: 46.9906\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 24.1404 - val_loss: 46.8878\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1205 - val_loss: 46.7876\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1663 - val_loss: 46.7329\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1491 - val_loss: 46.9473\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1514 - val_loss: 46.8734\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1368 - val_loss: 46.9376\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1459 - val_loss: 46.9286\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1293 - val_loss: 46.8654\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1458 - val_loss: 46.9346\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1761 - val_loss: 46.9363\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1233 - val_loss: 46.8461\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1533 - val_loss: 46.8546\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1871 - val_loss: 46.8823\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1258 - val_loss: 46.9770\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1423 - val_loss: 46.9093\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1434 - val_loss: 46.7629\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1320 - val_loss: 46.9426\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1241 - val_loss: 47.0405\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1508 - val_loss: 47.0098\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1338 - val_loss: 46.7791\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1387 - val_loss: 46.7189\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1159 - val_loss: 46.9860\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1060 - val_loss: 46.8958\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1501 - val_loss: 46.8534\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1564 - val_loss: 46.9369\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1076 - val_loss: 46.9433\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1116 - val_loss: 46.8800\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1717 - val_loss: 46.8999\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1506 - val_loss: 46.7570\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1198 - val_loss: 46.8663\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1133 - val_loss: 46.9181\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1428 - val_loss: 46.9250\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1356 - val_loss: 46.9058\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1155 - val_loss: 47.0326\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1455 - val_loss: 46.8039\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0982 - val_loss: 46.8217\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1251 - val_loss: 46.8176\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1335 - val_loss: 46.9947\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1012 - val_loss: 46.9016\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1511 - val_loss: 46.8825\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1050 - val_loss: 46.7722\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1297 - val_loss: 46.7908\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1001 - val_loss: 46.8948\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1243 - val_loss: 46.8184\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.1134 - val_loss: 46.8680\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.1175 - val_loss: 46.8139\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1114 - val_loss: 46.7654\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1657 - val_loss: 46.8564\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1605 - val_loss: 46.9923\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1279 - val_loss: 46.8950\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1250 - val_loss: 46.9456\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1134 - val_loss: 46.8321\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1859 - val_loss: 46.7872\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1391 - val_loss: 46.9118\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1348 - val_loss: 46.9201\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1183 - val_loss: 46.8804\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.1019 - val_loss: 46.8738\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1224 - val_loss: 46.9344\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1688 - val_loss: 46.9309\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1611 - val_loss: 46.7124\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1344 - val_loss: 46.8865\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0985 - val_loss: 46.9339\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1122 - val_loss: 46.9387\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1191 - val_loss: 46.7096\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1077 - val_loss: 46.8542\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1569 - val_loss: 46.9532\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1308 - val_loss: 46.9193\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1529 - val_loss: 46.7516\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1011 - val_loss: 46.8960\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1487 - val_loss: 46.8453\n",
      "\n",
      "Mean Squared Error for iteration35: 45.440701762246775\n",
      "\n",
      "Iteration:  36\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1371 - val_loss: 46.8613\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1197 - val_loss: 46.9244\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1465 - val_loss: 46.9098\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1489 - val_loss: 46.9310\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1168 - val_loss: 46.7751\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1286 - val_loss: 46.7618\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1171 - val_loss: 46.9244\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1489 - val_loss: 46.8790\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1431 - val_loss: 46.8832\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0997 - val_loss: 46.7039\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1319 - val_loss: 46.7289\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1022 - val_loss: 46.9504\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1270 - val_loss: 46.7624\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1122 - val_loss: 46.8765\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1494 - val_loss: 46.8886\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1693 - val_loss: 46.9150\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1520 - val_loss: 46.9591\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0911 - val_loss: 46.7585\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1255 - val_loss: 46.7789\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.1100 - val_loss: 46.9081\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1972 - val_loss: 46.6272\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1779 - val_loss: 46.9406\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1398 - val_loss: 46.7797\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1328 - val_loss: 46.9358\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1435 - val_loss: 46.7293\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0979 - val_loss: 46.7788\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1273 - val_loss: 46.8251\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1241 - val_loss: 46.9127\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1121 - val_loss: 46.8347\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1407 - val_loss: 46.7951\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1340 - val_loss: 46.7929\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1013 - val_loss: 46.8089\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1469 - val_loss: 46.8617\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1236 - val_loss: 46.8335\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1435 - val_loss: 46.8505\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1250 - val_loss: 46.8575\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1295 - val_loss: 46.8886\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0986 - val_loss: 46.9079\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1033 - val_loss: 46.9350\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1226 - val_loss: 46.9243\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1463 - val_loss: 46.7292\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1702 - val_loss: 46.9533\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0955 - val_loss: 46.8256\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1586 - val_loss: 46.7515\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1198 - val_loss: 46.8977\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1269 - val_loss: 46.9091\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1145 - val_loss: 46.7398\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1128 - val_loss: 46.7921\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1450 - val_loss: 46.9067\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1812 - val_loss: 46.6911\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1445 - val_loss: 46.8811\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 85us/step - loss: 24.1713 - val_loss: 46.9638\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1063 - val_loss: 46.7966\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1229 - val_loss: 46.6939\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1221 - val_loss: 46.9100\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1135 - val_loss: 46.7910\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1384 - val_loss: 46.8183\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1336 - val_loss: 46.6805\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0994 - val_loss: 46.8302\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1780 - val_loss: 46.8384\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0894 - val_loss: 46.8094\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1464 - val_loss: 46.8894\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1186 - val_loss: 46.8255\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1112 - val_loss: 46.7702\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.1061 - val_loss: 46.7907\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1399 - val_loss: 46.8514\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1126 - val_loss: 46.7757\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0953 - val_loss: 46.7881\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1370 - val_loss: 46.9321\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1131 - val_loss: 46.9052\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1355 - val_loss: 46.7487\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1136 - val_loss: 46.7595\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1160 - val_loss: 46.9151\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1069 - val_loss: 46.9099\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1279 - val_loss: 46.7044\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.1190 - val_loss: 46.7562\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1121 - val_loss: 46.7703\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1807 - val_loss: 47.0105\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2119 - val_loss: 46.6964\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1255 - val_loss: 46.9259\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1079 - val_loss: 46.8255\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1028 - val_loss: 46.8344\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1369 - val_loss: 46.9214\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1223 - val_loss: 46.8758\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1299 - val_loss: 46.7093\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1340 - val_loss: 46.8366\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1494 - val_loss: 46.8657\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0912 - val_loss: 46.8772\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0948 - val_loss: 46.8715\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1414 - val_loss: 46.9055\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0872 - val_loss: 46.6721\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1084 - val_loss: 46.7768\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1465 - val_loss: 46.9266\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1108 - val_loss: 46.8351\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1351 - val_loss: 46.6753\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1477 - val_loss: 46.8460\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1423 - val_loss: 46.6307\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1164 - val_loss: 46.8297\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0976 - val_loss: 46.8379\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1579 - val_loss: 46.7209\n",
      "\n",
      "Mean Squared Error for iteration36: 45.019824718452185\n",
      "\n",
      "Iteration:  37\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1257 - val_loss: 46.7900\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1507 - val_loss: 46.9146\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1686 - val_loss: 46.8767\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1096 - val_loss: 46.7169\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1470 - val_loss: 46.8547\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1131 - val_loss: 46.6582\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1407 - val_loss: 46.8632\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1093 - val_loss: 46.7117\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1309 - val_loss: 46.8527\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 24.1102 - val_loss: 46.7459\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1180 - val_loss: 46.7263\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1458 - val_loss: 46.7269\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0988 - val_loss: 46.8363\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1099 - val_loss: 46.7817\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1549 - val_loss: 46.9209\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1192 - val_loss: 46.7567\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1231 - val_loss: 46.6653\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1541 - val_loss: 46.8154\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1311 - val_loss: 46.8041\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.1162 - val_loss: 46.7505\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1254 - val_loss: 46.8720\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1079 - val_loss: 46.7739\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1252 - val_loss: 46.7019\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1344 - val_loss: 46.8533\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1076 - val_loss: 46.7642\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1662 - val_loss: 46.7682\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1419 - val_loss: 46.7630\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1141 - val_loss: 46.8512\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 24.1196 - val_loss: 46.7442\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1530 - val_loss: 46.8309\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1122 - val_loss: 46.7181\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1044 - val_loss: 46.7107\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1136 - val_loss: 46.7558\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1038 - val_loss: 46.8800\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1067 - val_loss: 46.8346\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1363 - val_loss: 46.9242\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1636 - val_loss: 46.8712\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1283 - val_loss: 46.7338\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1184 - val_loss: 46.6714\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1984 - val_loss: 46.8895\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1262 - val_loss: 46.7902\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.1264 - val_loss: 46.7894\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1620 - val_loss: 46.5869\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1205 - val_loss: 46.7670\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0960 - val_loss: 46.7917\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1058 - val_loss: 46.8366\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1124 - val_loss: 46.7571\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1005 - val_loss: 46.7945\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1326 - val_loss: 46.6890\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1174 - val_loss: 46.7358\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1218 - val_loss: 46.8054\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1174 - val_loss: 46.8226\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0860 - val_loss: 46.7285\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.1008 - val_loss: 46.6279\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.1409 - val_loss: 46.9291\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1095 - val_loss: 46.7859\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1417 - val_loss: 46.7107\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1346 - val_loss: 46.6676\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1317 - val_loss: 46.8411\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1818 - val_loss: 46.8375\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1317 - val_loss: 46.8665\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1113 - val_loss: 46.7264\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1184 - val_loss: 46.8391\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0945 - val_loss: 46.7534\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0911 - val_loss: 46.7560\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1355 - val_loss: 46.8658\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1041 - val_loss: 46.6958\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1138 - val_loss: 46.6401\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1087 - val_loss: 46.6902\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1166 - val_loss: 46.8155\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0798 - val_loss: 46.8128\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1028 - val_loss: 46.8230\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1560 - val_loss: 46.8225\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0909 - val_loss: 46.8123\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1293 - val_loss: 46.6876\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0953 - val_loss: 46.7456\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.0983 - val_loss: 46.6946\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1212 - val_loss: 46.8145\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1083 - val_loss: 46.7111\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1214 - val_loss: 46.7422\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1063 - val_loss: 46.7526\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0904 - val_loss: 46.7373\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1002 - val_loss: 46.7538\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1262 - val_loss: 46.7946\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1117 - val_loss: 46.7461\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.1379 - val_loss: 46.6906\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1513 - val_loss: 46.7771\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1331 - val_loss: 46.8330\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1062 - val_loss: 46.7535\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1133 - val_loss: 46.7256\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1461 - val_loss: 46.8464\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1157 - val_loss: 46.7144\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1015 - val_loss: 46.6930\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0921 - val_loss: 46.7826\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1143 - val_loss: 46.7478\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1160 - val_loss: 46.7680\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0964 - val_loss: 46.7158\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0970 - val_loss: 46.8451\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 113us/step - loss: 24.1728 - val_loss: 46.8362\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0855 - val_loss: 46.7562\n",
      "\n",
      "Mean Squared Error for iteration37: 45.36762811989209\n",
      "\n",
      "Iteration:  38\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1645 - val_loss: 46.7937\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1704 - val_loss: 46.6938\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0947 - val_loss: 46.6393\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1441 - val_loss: 46.8279\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0973 - val_loss: 46.8429\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.0983 - val_loss: 46.7892\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1220 - val_loss: 46.6346\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1031 - val_loss: 46.5798\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.0820 - val_loss: 46.6931\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0977 - val_loss: 46.8389\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.0976 - val_loss: 46.7984\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1093 - val_loss: 46.8640\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1536 - val_loss: 46.6984\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1115 - val_loss: 46.7420\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0917 - val_loss: 46.6410\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0914 - val_loss: 46.6181\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1348 - val_loss: 46.8732\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0885 - val_loss: 46.8184\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0976 - val_loss: 46.5711\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1284 - val_loss: 46.6854\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0997 - val_loss: 46.7479\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1428 - val_loss: 46.7534\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1139 - val_loss: 46.7030\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1093 - val_loss: 46.6813\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1029 - val_loss: 46.7138\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1249 - val_loss: 46.8138\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1453 - val_loss: 46.8494\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1271 - val_loss: 46.8541\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1434 - val_loss: 46.6135\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0962 - val_loss: 46.6759\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1232 - val_loss: 46.7708\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1024 - val_loss: 46.6203\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1317 - val_loss: 46.8322\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1052 - val_loss: 46.7531\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1108 - val_loss: 46.6891\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0876 - val_loss: 46.6717\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1182 - val_loss: 46.7669\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0996 - val_loss: 46.6769\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1116 - val_loss: 46.7496\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0977 - val_loss: 46.6865\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0985 - val_loss: 46.6654\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1329 - val_loss: 46.7406\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.1065 - val_loss: 46.7923\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0672 - val_loss: 46.6788\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0954 - val_loss: 46.6538\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1232 - val_loss: 46.7786\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1142 - val_loss: 46.6609\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1052 - val_loss: 46.6086\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1411 - val_loss: 46.7628\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0843 - val_loss: 46.7965\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1283 - val_loss: 46.8014\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1381 - val_loss: 46.8742\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0699 - val_loss: 46.6999\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.1121 - val_loss: 46.5995\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1428 - val_loss: 46.7187\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0714 - val_loss: 46.6450\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0889 - val_loss: 46.7443\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0925 - val_loss: 46.8658\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1016 - val_loss: 46.7597\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1592 - val_loss: 46.5926\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0857 - val_loss: 46.7239\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1255 - val_loss: 46.6708\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1153 - val_loss: 46.7061\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0972 - val_loss: 46.7298\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1289 - val_loss: 46.6147\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1025 - val_loss: 46.8353\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1093 - val_loss: 46.6741\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1004 - val_loss: 46.7342\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1331 - val_loss: 46.6685\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1473 - val_loss: 46.8483\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0932 - val_loss: 46.7645\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0916 - val_loss: 46.6653\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0914 - val_loss: 46.5610\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1177 - val_loss: 46.7434\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1065 - val_loss: 46.7659\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.1209 - val_loss: 46.6365\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1340 - val_loss: 46.8552\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1157 - val_loss: 46.6743\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1106 - val_loss: 46.7999\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0774 - val_loss: 46.7304\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1074 - val_loss: 46.6337\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0975 - val_loss: 46.8083\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1169 - val_loss: 46.6043\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.0847 - val_loss: 46.6863\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1032 - val_loss: 46.6698\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1450 - val_loss: 46.7888\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1171 - val_loss: 46.8779\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.0927 - val_loss: 46.6402\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1475 - val_loss: 46.6892\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1301 - val_loss: 46.6532\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0997 - val_loss: 46.6346\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0943 - val_loss: 46.7400\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0889 - val_loss: 46.7496\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1473 - val_loss: 46.6851\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1187 - val_loss: 46.6959\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0821 - val_loss: 46.6261\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0833 - val_loss: 46.7250\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1251 - val_loss: 46.8336\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0872 - val_loss: 46.6694\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0783 - val_loss: 46.6419\n",
      "\n",
      "Mean Squared Error for iteration38: 45.0972671604479\n",
      "\n",
      "Iteration:  39\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0954 - val_loss: 46.7524\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1451 - val_loss: 46.7283\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1011 - val_loss: 46.7364\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1229 - val_loss: 46.7696\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1140 - val_loss: 46.6579\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.0861 - val_loss: 46.7484\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0768 - val_loss: 46.6945\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0907 - val_loss: 46.6589\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1030 - val_loss: 46.6425\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1045 - val_loss: 46.7460\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0759 - val_loss: 46.7154\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1271 - val_loss: 46.8054\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1041 - val_loss: 46.6480\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1363 - val_loss: 46.5747\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0899 - val_loss: 46.6806\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1144 - val_loss: 46.8083\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0783 - val_loss: 46.6747\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1165 - val_loss: 46.8567\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0843 - val_loss: 46.7233\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0934 - val_loss: 46.6787\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0984 - val_loss: 46.7304\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1378 - val_loss: 46.6709\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0708 - val_loss: 46.7065\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1142 - val_loss: 46.6278\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0753 - val_loss: 46.7117\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1068 - val_loss: 46.8496\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1517 - val_loss: 46.5510\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1347 - val_loss: 46.7864\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1210 - val_loss: 46.6412\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0782 - val_loss: 46.6362\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0937 - val_loss: 46.7457\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1224 - val_loss: 46.6680\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.1321 - val_loss: 46.7563\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0880 - val_loss: 46.7088\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1403 - val_loss: 46.6097\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1570 - val_loss: 46.7324\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1469 - val_loss: 46.6805\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1219 - val_loss: 46.8495\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0951 - val_loss: 46.7035\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1055 - val_loss: 46.6302\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0947 - val_loss: 46.5512\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0970 - val_loss: 46.6402\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.0828 - val_loss: 46.6757\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1246 - val_loss: 46.7134\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0992 - val_loss: 46.5790\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1231 - val_loss: 46.7390\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0999 - val_loss: 46.7000\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.0923 - val_loss: 46.7411\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.0993 - val_loss: 46.7444\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1035 - val_loss: 46.6901\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1546 - val_loss: 46.7057\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0940 - val_loss: 46.6483\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1235 - val_loss: 46.6860\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1703 - val_loss: 46.5936\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0833 - val_loss: 46.7139\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1153 - val_loss: 46.8335\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1058 - val_loss: 46.5754\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0765 - val_loss: 46.5594\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1490 - val_loss: 46.8785\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1302 - val_loss: 46.5667\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 80us/step - loss: 24.1065 - val_loss: 46.7259\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1226 - val_loss: 46.6304\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1000 - val_loss: 46.6500\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0893 - val_loss: 46.6853\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.1149 - val_loss: 46.8277\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1057 - val_loss: 46.5996\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1337 - val_loss: 46.6101\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0804 - val_loss: 46.6534\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0730 - val_loss: 46.7036\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1084 - val_loss: 46.7741\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1223 - val_loss: 46.7745\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0783 - val_loss: 46.6664\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1246 - val_loss: 46.6236\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0901 - val_loss: 46.6522\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0829 - val_loss: 46.5584\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0820 - val_loss: 46.6902\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1122 - val_loss: 46.6987\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.1093 - val_loss: 46.6460\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1154 - val_loss: 46.6202\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0776 - val_loss: 46.6281\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1286 - val_loss: 46.7064\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0888 - val_loss: 46.6297\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0851 - val_loss: 46.5340\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1476 - val_loss: 46.8153\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1132 - val_loss: 46.6618\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1082 - val_loss: 46.7184\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1313 - val_loss: 46.4896\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.1223 - val_loss: 46.6990\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0638 - val_loss: 46.7759\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1550 - val_loss: 46.6146\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1822 - val_loss: 46.8422\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.2213 - val_loss: 46.6615\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0783 - val_loss: 46.5444\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0789 - val_loss: 46.6162\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0698 - val_loss: 46.7202\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0939 - val_loss: 46.7053\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1075 - val_loss: 46.7505\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0925 - val_loss: 46.7441\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1028 - val_loss: 46.6311\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1160 - val_loss: 46.5487\n",
      "\n",
      "Mean Squared Error for iteration39: 44.99861713681895\n",
      "\n",
      "Iteration:  40\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0885 - val_loss: 46.7209\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1583 - val_loss: 46.5899\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0927 - val_loss: 46.6838\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0768 - val_loss: 46.7256\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1070 - val_loss: 46.6781\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1077 - val_loss: 46.5778\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0856 - val_loss: 46.6792\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1118 - val_loss: 46.7517\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0762 - val_loss: 46.6549\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0733 - val_loss: 46.6825\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1038 - val_loss: 46.6743\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0785 - val_loss: 46.7030\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0757 - val_loss: 46.7110\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0761 - val_loss: 46.7042\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0890 - val_loss: 46.6106\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1092 - val_loss: 46.6856\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0947 - val_loss: 46.6528\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1004 - val_loss: 46.5894\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0841 - val_loss: 46.7022\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1439 - val_loss: 46.5487\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0949 - val_loss: 46.6438\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.1247 - val_loss: 46.6787\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0888 - val_loss: 46.7026\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1241 - val_loss: 46.6595\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1223 - val_loss: 46.6106\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1214 - val_loss: 46.6944\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1074 - val_loss: 46.6349\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0735 - val_loss: 46.6253\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1174 - val_loss: 46.6451\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1074 - val_loss: 46.6991\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1117 - val_loss: 46.5945\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.1066 - val_loss: 46.5290\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1027 - val_loss: 46.8238\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0820 - val_loss: 46.6725\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0912 - val_loss: 46.7077\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0643 - val_loss: 46.5590\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1084 - val_loss: 46.5864\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.1036 - val_loss: 46.5918\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1033 - val_loss: 46.5362\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0345 - val_loss: 46.7262\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1000 - val_loss: 46.6871\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0973 - val_loss: 46.7224\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0663 - val_loss: 46.7206\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0763 - val_loss: 46.6239\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1011 - val_loss: 46.5254\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1758 - val_loss: 46.8220\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0910 - val_loss: 46.7179\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0760 - val_loss: 46.6032\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1086 - val_loss: 46.6122\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1133 - val_loss: 46.6741\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0820 - val_loss: 46.7159\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1172 - val_loss: 46.5520\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1151 - val_loss: 46.6800\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1507 - val_loss: 46.5388\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1005 - val_loss: 46.5294\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0763 - val_loss: 46.5995\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0935 - val_loss: 46.7041\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0992 - val_loss: 46.5375\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0976 - val_loss: 46.6196\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1339 - val_loss: 46.6354\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0858 - val_loss: 46.7103\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1055 - val_loss: 46.6980\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0982 - val_loss: 46.6898\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1253 - val_loss: 46.7367\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0795 - val_loss: 46.5828\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0886 - val_loss: 46.6669\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.1488 - val_loss: 46.6924\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0807 - val_loss: 46.5944\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1268 - val_loss: 46.4341\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1094 - val_loss: 46.6846\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0785 - val_loss: 46.7142\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1170 - val_loss: 46.6309\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1091 - val_loss: 46.6119\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0998 - val_loss: 46.6274\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1149 - val_loss: 46.6568\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0757 - val_loss: 46.6242\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1196 - val_loss: 46.7450\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1056 - val_loss: 46.5708\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0909 - val_loss: 46.6276\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0913 - val_loss: 46.5877\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0837 - val_loss: 46.6285\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0941 - val_loss: 46.7708\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0711 - val_loss: 46.7041\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0528 - val_loss: 46.5343\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1166 - val_loss: 46.5801\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0554 - val_loss: 46.5661\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0716 - val_loss: 46.7518\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0861 - val_loss: 46.6606\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1188 - val_loss: 46.5564\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0662 - val_loss: 46.5695\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1263 - val_loss: 46.6336\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0662 - val_loss: 46.6182\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0687 - val_loss: 46.6748\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1023 - val_loss: 46.4916\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1033 - val_loss: 46.7511\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0954 - val_loss: 46.6657\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0835 - val_loss: 46.6339\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0802 - val_loss: 46.6341\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0608 - val_loss: 46.6937\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.0606 - val_loss: 46.6229\n",
      "\n",
      "Mean Squared Error for iteration40: 45.162765912077695\n",
      "\n",
      "Iteration:  41\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0885 - val_loss: 46.6459\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0761 - val_loss: 46.7163\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1044 - val_loss: 46.6299\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1366 - val_loss: 46.6549\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1093 - val_loss: 46.6089\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1077 - val_loss: 46.6140\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1187 - val_loss: 46.4882\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1132 - val_loss: 46.7184\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1081 - val_loss: 46.8364\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0902 - val_loss: 46.6572\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0774 - val_loss: 46.4776\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1152 - val_loss: 46.6831\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0993 - val_loss: 46.5164\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0531 - val_loss: 46.6495\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 75us/step - loss: 24.1205 - val_loss: 46.6491\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0924 - val_loss: 46.4895\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0984 - val_loss: 46.5783\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0766 - val_loss: 46.6462\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0938 - val_loss: 46.6350\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0880 - val_loss: 46.5561\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.0828 - val_loss: 46.6125\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0935 - val_loss: 46.5347\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0831 - val_loss: 46.5464\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0670 - val_loss: 46.7365\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0733 - val_loss: 46.5960\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0621 - val_loss: 46.6373\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0598 - val_loss: 46.6855\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0668 - val_loss: 46.6079\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0818 - val_loss: 46.5757\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0666 - val_loss: 46.5301\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0629 - val_loss: 46.5574\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0805 - val_loss: 46.7242\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0752 - val_loss: 46.7047\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0557 - val_loss: 46.6167\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0799 - val_loss: 46.6576\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0908 - val_loss: 46.5892\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0843 - val_loss: 46.5295\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0999 - val_loss: 46.6004\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0870 - val_loss: 46.6472\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1013 - val_loss: 46.7467\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0691 - val_loss: 46.5325\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0765 - val_loss: 46.5186\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1143 - val_loss: 46.6800\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0715 - val_loss: 46.5038\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0797 - val_loss: 46.6341\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0658 - val_loss: 46.5295\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1579 - val_loss: 46.6020\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0908 - val_loss: 46.4881\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1493 - val_loss: 46.7719\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1234 - val_loss: 46.5819\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1054 - val_loss: 46.5334\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1085 - val_loss: 46.7036\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0563 - val_loss: 46.5473\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0856 - val_loss: 46.6206\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1250 - val_loss: 46.5563\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 106us/step - loss: 24.1129 - val_loss: 46.5420\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0695 - val_loss: 46.5810\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0779 - val_loss: 46.6786\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0765 - val_loss: 46.6001\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1007 - val_loss: 46.5867\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0885 - val_loss: 46.6115\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0739 - val_loss: 46.5313\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1092 - val_loss: 46.6810\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1181 - val_loss: 46.6791\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1082 - val_loss: 46.5190\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0732 - val_loss: 46.5117\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0782 - val_loss: 46.7181\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0682 - val_loss: 46.6460\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1087 - val_loss: 46.6930\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0532 - val_loss: 46.6424\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1169 - val_loss: 46.5689\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0747 - val_loss: 46.5050\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1215 - val_loss: 46.7284\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0736 - val_loss: 46.5994\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0768 - val_loss: 46.4976\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0684 - val_loss: 46.5348\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0760 - val_loss: 46.5203\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1095 - val_loss: 46.7429\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1349 - val_loss: 46.6903\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0639 - val_loss: 46.6063\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1599 - val_loss: 46.7036\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0582 - val_loss: 46.5723\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0682 - val_loss: 46.5144\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0879 - val_loss: 46.5676\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1498 - val_loss: 46.5314\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1152 - val_loss: 46.5109\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0663 - val_loss: 46.6017\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1285 - val_loss: 46.6888\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1107 - val_loss: 46.6512\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1109 - val_loss: 46.4590\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0813 - val_loss: 46.5660\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0780 - val_loss: 46.6392\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 72us/step - loss: 24.0710 - val_loss: 46.5833\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0732 - val_loss: 46.6126\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1006 - val_loss: 46.6775\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0530 - val_loss: 46.5807\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0825 - val_loss: 46.5421\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0737 - val_loss: 46.4771\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0930 - val_loss: 46.5431\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0779 - val_loss: 46.5215\n",
      "\n",
      "Mean Squared Error for iteration41: 44.96549861100132\n",
      "\n",
      "Iteration:  42\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.0634 - val_loss: 46.4537\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0817 - val_loss: 46.6422\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0682 - val_loss: 46.5877\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1090 - val_loss: 46.5271\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0782 - val_loss: 46.5816\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0704 - val_loss: 46.7217\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0871 - val_loss: 46.7028\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0867 - val_loss: 46.5806\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0690 - val_loss: 46.5274\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0747 - val_loss: 46.4273\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1320 - val_loss: 46.7138\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0503 - val_loss: 46.6392\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0429 - val_loss: 46.5467\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0707 - val_loss: 46.5679\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1392 - val_loss: 46.7607\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0845 - val_loss: 46.4780\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1064 - val_loss: 46.7060\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0529 - val_loss: 46.4486\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0938 - val_loss: 46.4202\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1006 - val_loss: 46.6110\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1083 - val_loss: 46.6178\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1445 - val_loss: 46.6538\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0784 - val_loss: 46.4684\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0794 - val_loss: 46.6032\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0477 - val_loss: 46.5931\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0603 - val_loss: 46.5607\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0434 - val_loss: 46.5054\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0612 - val_loss: 46.6493\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1102 - val_loss: 46.6328\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0470 - val_loss: 46.4453\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1761 - val_loss: 46.5192\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1896 - val_loss: 46.6904\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0712 - val_loss: 46.6165\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1660 - val_loss: 46.4403\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1012 - val_loss: 46.5439\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0583 - val_loss: 46.5629\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0495 - val_loss: 46.5890\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0794 - val_loss: 46.6754\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1159 - val_loss: 46.5133\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0405 - val_loss: 46.5668\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0712 - val_loss: 46.5348\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0692 - val_loss: 46.7668\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1497 - val_loss: 46.7174\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1374 - val_loss: 46.4307\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0740 - val_loss: 46.4314\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 24.0754 - val_loss: 46.6798\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.0758 - val_loss: 46.6020\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0894 - val_loss: 46.5112\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0732 - val_loss: 46.5798\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0692 - val_loss: 46.6570\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1092 - val_loss: 46.3985\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1334 - val_loss: 46.6950\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0616 - val_loss: 46.6806\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0561 - val_loss: 46.7079\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0676 - val_loss: 46.6326\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.1050 - val_loss: 46.4812\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0665 - val_loss: 46.5930\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0521 - val_loss: 46.6416\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0754 - val_loss: 46.7018\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1185 - val_loss: 46.5121\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1427 - val_loss: 46.4793\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0394 - val_loss: 46.7090\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0705 - val_loss: 46.6500\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0660 - val_loss: 46.5802\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0804 - val_loss: 46.5252\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0650 - val_loss: 46.4931\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0684 - val_loss: 46.4927\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0883 - val_loss: 46.4506\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0596 - val_loss: 46.6336\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.0755 - val_loss: 46.7165\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0773 - val_loss: 46.6488\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1244 - val_loss: 46.5547\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1021 - val_loss: 46.5871\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0579 - val_loss: 46.6143\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1090 - val_loss: 46.6993\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0870 - val_loss: 46.7117\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0724 - val_loss: 46.4662\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0904 - val_loss: 46.4232\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0577 - val_loss: 46.6180\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0667 - val_loss: 46.5537\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0786 - val_loss: 46.6387\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0783 - val_loss: 46.5187\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1605 - val_loss: 46.6681\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0664 - val_loss: 46.7027\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0588 - val_loss: 46.5269\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0745 - val_loss: 46.6012\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0782 - val_loss: 46.5638\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0639 - val_loss: 46.5626\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0842 - val_loss: 46.4442\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0572 - val_loss: 46.5858\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 24.0632 - val_loss: 46.6012\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0705 - val_loss: 46.6660\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0600 - val_loss: 46.5880\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0594 - val_loss: 46.7055\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0669 - val_loss: 46.6021\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0437 - val_loss: 46.4637\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0642 - val_loss: 46.6049\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0663 - val_loss: 46.5982\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0899 - val_loss: 46.5818\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0802 - val_loss: 46.6093\n",
      "\n",
      "Mean Squared Error for iteration42: 45.25511578167543\n",
      "\n",
      "Iteration:  43\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0978 - val_loss: 46.7393\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0811 - val_loss: 46.5315\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1000 - val_loss: 46.6784\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0701 - val_loss: 46.4615\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0797 - val_loss: 46.5506\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.1012 - val_loss: 46.6654\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0842 - val_loss: 46.5631\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0632 - val_loss: 46.6145\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0539 - val_loss: 46.5477\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0812 - val_loss: 46.4718\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0930 - val_loss: 46.4846\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0527 - val_loss: 46.7811\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0680 - val_loss: 46.6358\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0562 - val_loss: 46.5430\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0384 - val_loss: 46.5102\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0705 - val_loss: 46.5091\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.1203 - val_loss: 46.6517\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0665 - val_loss: 46.6272\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0571 - val_loss: 46.5531\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0957 - val_loss: 46.6677\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0691 - val_loss: 46.5319\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0802 - val_loss: 46.6489\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0486 - val_loss: 46.6090\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0732 - val_loss: 46.4703\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0607 - val_loss: 46.6431\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0947 - val_loss: 46.4454\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1073 - val_loss: 46.6255\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0711 - val_loss: 46.6833\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0604 - val_loss: 46.5730\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0995 - val_loss: 46.5375\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1122 - val_loss: 46.5828\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1047 - val_loss: 46.4684\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0705 - val_loss: 46.5593\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0609 - val_loss: 46.5588\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1020 - val_loss: 46.5506\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.0645 - val_loss: 46.5789\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0432 - val_loss: 46.6023\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0941 - val_loss: 46.5024\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1108 - val_loss: 46.5470\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0577 - val_loss: 46.7386\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0375 - val_loss: 46.5230\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0989 - val_loss: 46.3951\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1318 - val_loss: 46.7807\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0846 - val_loss: 46.5500\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0656 - val_loss: 46.5452\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0746 - val_loss: 46.6582\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 88us/step - loss: 24.0514 - val_loss: 46.4924\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1078 - val_loss: 46.6122\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0686 - val_loss: 46.5481\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0719 - val_loss: 46.6972\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0904 - val_loss: 46.5285\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0767 - val_loss: 46.4368\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0620 - val_loss: 46.6533\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0876 - val_loss: 46.7138\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0563 - val_loss: 46.5131\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1002 - val_loss: 46.5472\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0915 - val_loss: 46.4899\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0833 - val_loss: 46.6797\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0735 - val_loss: 46.4835\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0572 - val_loss: 46.6164\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0919 - val_loss: 46.4885\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0568 - val_loss: 46.6244\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0497 - val_loss: 46.5594\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0476 - val_loss: 46.5538\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0835 - val_loss: 46.6862\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0393 - val_loss: 46.5850\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0516 - val_loss: 46.5309\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 173us/step - loss: 24.0930 - val_loss: 46.5472\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0631 - val_loss: 46.4575\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0608 - val_loss: 46.5025\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.0499 - val_loss: 46.5652\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.1178 - val_loss: 46.6773\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.1351 - val_loss: 46.5634\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0566 - val_loss: 46.5538\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0883 - val_loss: 46.4158\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0630 - val_loss: 46.5897\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0434 - val_loss: 46.5497\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0618 - val_loss: 46.4572\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0487 - val_loss: 46.6920\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0694 - val_loss: 46.6227\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0561 - val_loss: 46.5442\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0719 - val_loss: 46.4889\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0736 - val_loss: 46.7021\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0440 - val_loss: 46.6582\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0942 - val_loss: 46.4762\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0745 - val_loss: 46.5014\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0450 - val_loss: 46.6018\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1332 - val_loss: 46.4164\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1914 - val_loss: 46.7966\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0434 - val_loss: 46.6128\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0577 - val_loss: 46.5541\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0467 - val_loss: 46.5721\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0818 - val_loss: 46.6607\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0872 - val_loss: 46.4162\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0928 - val_loss: 46.6339\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0804 - val_loss: 46.6163\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0812 - val_loss: 46.4294\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0868 - val_loss: 46.4479\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0278 - val_loss: 46.5882\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0800 - val_loss: 46.4630\n",
      "\n",
      "Mean Squared Error for iteration43: 45.13625261686509\n",
      "\n",
      "Iteration:  44\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0486 - val_loss: 46.5958\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0771 - val_loss: 46.6814\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0562 - val_loss: 46.4680\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0375 - val_loss: 46.5953\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0576 - val_loss: 46.6216\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0419 - val_loss: 46.5517\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0698 - val_loss: 46.6047\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0981 - val_loss: 46.3857\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0778 - val_loss: 46.5969\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0921 - val_loss: 46.6628\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0648 - val_loss: 46.5537\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0545 - val_loss: 46.5874\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0715 - val_loss: 46.6451\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1183 - val_loss: 46.5728\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0652 - val_loss: 46.3661\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0927 - val_loss: 46.7105\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0582 - val_loss: 46.6176\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0605 - val_loss: 46.5854\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0476 - val_loss: 46.5252\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0465 - val_loss: 46.3955\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0436 - val_loss: 46.6426\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1030 - val_loss: 46.6711\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0611 - val_loss: 46.5410\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 79us/step - loss: 24.0452 - val_loss: 46.4016\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 24.0977 - val_loss: 46.6271\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0625 - val_loss: 46.4477\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0900 - val_loss: 46.4784\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0757 - val_loss: 46.6206\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0861 - val_loss: 46.5652\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0959 - val_loss: 46.5886\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0848 - val_loss: 46.4886\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0547 - val_loss: 46.5608\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.0479 - val_loss: 46.4128\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.1009 - val_loss: 46.6000\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.1059 - val_loss: 46.6138\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 68us/step - loss: 24.1274 - val_loss: 46.6461\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0377 - val_loss: 46.4506\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0925 - val_loss: 46.5170\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 68us/step - loss: 24.0553 - val_loss: 46.5888\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0405 - val_loss: 46.5711\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0626 - val_loss: 46.5914\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0676 - val_loss: 46.5223\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0804 - val_loss: 46.5786\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0443 - val_loss: 46.5252\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0658 - val_loss: 46.5365\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0547 - val_loss: 46.4674\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0917 - val_loss: 46.5370\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0405 - val_loss: 46.6250\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0417 - val_loss: 46.5249\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0715 - val_loss: 46.6616\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0668 - val_loss: 46.6758\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0978 - val_loss: 46.4122\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0688 - val_loss: 46.4871\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0533 - val_loss: 46.4494\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0408 - val_loss: 46.5291\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0698 - val_loss: 46.4555\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0576 - val_loss: 46.6487\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0848 - val_loss: 46.4813\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0438 - val_loss: 46.5304\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0531 - val_loss: 46.5981\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0856 - val_loss: 46.5478\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0670 - val_loss: 46.4820\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0555 - val_loss: 46.5645\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0355 - val_loss: 46.5491\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0760 - val_loss: 46.4865\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0631 - val_loss: 46.5654\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0443 - val_loss: 46.5393\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0482 - val_loss: 46.7276\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0882 - val_loss: 46.6088\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0730 - val_loss: 46.5035\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0892 - val_loss: 46.6550\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0612 - val_loss: 46.5660\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1258 - val_loss: 46.4970\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0304 - val_loss: 46.6189\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0352 - val_loss: 46.5218\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0316 - val_loss: 46.5239\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0591 - val_loss: 46.6189\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0463 - val_loss: 46.4841\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0907 - val_loss: 46.5502\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0731 - val_loss: 46.5470\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.0680 - val_loss: 46.5031\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.0609 - val_loss: 46.6109\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.1037 - val_loss: 46.6384\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0479 - val_loss: 46.5186\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1033 - val_loss: 46.5476\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0665 - val_loss: 46.4036\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0703 - val_loss: 46.5519\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0577 - val_loss: 46.4664\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0393 - val_loss: 46.5834\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0505 - val_loss: 46.4114\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0732 - val_loss: 46.4237\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0509 - val_loss: 46.5704\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0760 - val_loss: 46.4576\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0411 - val_loss: 46.6474\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0606 - val_loss: 46.4542\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0456 - val_loss: 46.5272\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0438 - val_loss: 46.6321\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1188 - val_loss: 46.6913\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0735 - val_loss: 46.4826\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0674 - val_loss: 46.4204\n",
      "\n",
      "Mean Squared Error for iteration44: 44.98117470176498\n",
      "\n",
      "Iteration:  45\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 78us/step - loss: 24.1116 - val_loss: 46.6140\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0370 - val_loss: 46.5604\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0867 - val_loss: 46.4408\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0628 - val_loss: 46.4577\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0369 - val_loss: 46.4551\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1124 - val_loss: 46.5251\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0963 - val_loss: 46.5510\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0448 - val_loss: 46.4966\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0799 - val_loss: 46.4781\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0787 - val_loss: 46.6087\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0495 - val_loss: 46.4611\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0475 - val_loss: 46.5224\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0609 - val_loss: 46.5559\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0694 - val_loss: 46.5531\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 118us/step - loss: 24.0850 - val_loss: 46.4640\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0414 - val_loss: 46.3733\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0531 - val_loss: 46.5182\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1126 - val_loss: 46.3835\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0643 - val_loss: 46.6234\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0436 - val_loss: 46.5745\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1147 - val_loss: 46.3604\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0499 - val_loss: 46.4661\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0593 - val_loss: 46.6447\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0743 - val_loss: 46.4720\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0398 - val_loss: 46.6220\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.1538 - val_loss: 46.5306\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0451 - val_loss: 46.6218\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0738 - val_loss: 46.5711\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0827 - val_loss: 46.4902\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0512 - val_loss: 46.4743\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0465 - val_loss: 46.5298\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0554 - val_loss: 46.5831\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0615 - val_loss: 46.5853\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0728 - val_loss: 46.4539\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0594 - val_loss: 46.4397\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0538 - val_loss: 46.6331\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.0443 - val_loss: 46.5348\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0731 - val_loss: 46.5625\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1072 - val_loss: 46.3017\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0543 - val_loss: 46.3865\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0703 - val_loss: 46.5710\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0470 - val_loss: 46.5774\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0844 - val_loss: 46.5430\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0393 - val_loss: 46.4418\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1122 - val_loss: 46.6017\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0727 - val_loss: 46.3816\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.0803 - val_loss: 46.4363\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0178 - val_loss: 46.4503\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0603 - val_loss: 46.4393\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0521 - val_loss: 46.6087\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0598 - val_loss: 46.4510\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0395 - val_loss: 46.6505\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0623 - val_loss: 46.5167\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0348 - val_loss: 46.4345\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0518 - val_loss: 46.4676\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0485 - val_loss: 46.5353\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0542 - val_loss: 46.5240\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0155 - val_loss: 46.4245\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.0130 - val_loss: 46.5272\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0798 - val_loss: 46.4972\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0542 - val_loss: 46.4651\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0606 - val_loss: 46.5527\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0390 - val_loss: 46.4768\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0622 - val_loss: 46.7145\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0502 - val_loss: 46.5588\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0420 - val_loss: 46.3720\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0250 - val_loss: 46.4326\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0777 - val_loss: 46.5861\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0345 - val_loss: 46.4629\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 24.0451 - val_loss: 46.5830\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0762 - val_loss: 46.5299\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0338 - val_loss: 46.4824\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0474 - val_loss: 46.3922\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0390 - val_loss: 46.4485\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0875 - val_loss: 46.6413\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0272 - val_loss: 46.5575\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0363 - val_loss: 46.4994\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0306 - val_loss: 46.4585\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.0734 - val_loss: 46.4345\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0214 - val_loss: 46.4296\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0305 - val_loss: 46.5521\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0472 - val_loss: 46.3692\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.1177 - val_loss: 46.7236\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0735 - val_loss: 46.3743\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0309 - val_loss: 46.3524\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0715 - val_loss: 46.4723\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1125 - val_loss: 46.6030\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1097 - val_loss: 46.6599\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0594 - val_loss: 46.3215\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0248 - val_loss: 46.5072\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0392 - val_loss: 46.4453\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0737 - val_loss: 46.4099\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0734 - val_loss: 46.6502\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0815 - val_loss: 46.5772\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0996 - val_loss: 46.5093\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0865 - val_loss: 46.4763\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0722 - val_loss: 46.6635\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0714 - val_loss: 46.3923\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0943 - val_loss: 46.4766\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0518 - val_loss: 46.4568\n",
      "\n",
      "Mean Squared Error for iteration45: 45.20204947114032\n",
      "\n",
      "Iteration:  46\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0313 - val_loss: 46.4423\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0770 - val_loss: 46.2980\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0579 - val_loss: 46.4231\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.0285 - val_loss: 46.6916\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0976 - val_loss: 46.3698\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0375 - val_loss: 46.5681\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0757 - val_loss: 46.3995\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0683 - val_loss: 46.5924\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0311 - val_loss: 46.4891\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0517 - val_loss: 46.4263\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0469 - val_loss: 46.4778\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0731 - val_loss: 46.3649\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0634 - val_loss: 46.4494\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0637 - val_loss: 46.6552\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0339 - val_loss: 46.4773\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0715 - val_loss: 46.4837\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0594 - val_loss: 46.4595\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0513 - val_loss: 46.3607\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0584 - val_loss: 46.4201\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0573 - val_loss: 46.4718\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0149 - val_loss: 46.4508\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0288 - val_loss: 46.4692\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.0309 - val_loss: 46.4293\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0563 - val_loss: 46.5838\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0224 - val_loss: 46.5032\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0946 - val_loss: 46.3585\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0302 - val_loss: 46.3297\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0278 - val_loss: 46.5666\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0483 - val_loss: 46.6285\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0722 - val_loss: 46.3076\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1172 - val_loss: 46.4892\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0808 - val_loss: 46.2971\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0996 - val_loss: 46.5605\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0626 - val_loss: 46.5170\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.1124 - val_loss: 46.3752\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.0294 - val_loss: 46.4909\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0881 - val_loss: 46.5931\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0317 - val_loss: 46.4481\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0883 - val_loss: 46.3395\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0702 - val_loss: 46.6274\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0351 - val_loss: 46.5888\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0506 - val_loss: 46.5164\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0399 - val_loss: 46.2798\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0872 - val_loss: 46.4896\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0772 - val_loss: 46.3442\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0466 - val_loss: 46.3558\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.1340 - val_loss: 46.6853\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0564 - val_loss: 46.4491\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 24.0556 - val_loss: 46.5683\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0213 - val_loss: 46.3883\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 24.0996 - val_loss: 46.3466\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0935 - val_loss: 46.4918\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.1013 - val_loss: 46.5402\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0666 - val_loss: 46.2957\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0155 - val_loss: 46.4281\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.0667 - val_loss: 46.3608\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0917 - val_loss: 46.4583\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0494 - val_loss: 46.5358\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0594 - val_loss: 46.3589\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0495 - val_loss: 46.5742\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0580 - val_loss: 46.3908\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0801 - val_loss: 46.3044\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0586 - val_loss: 46.5550\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.0657 - val_loss: 46.6094\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0495 - val_loss: 46.4634\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0476 - val_loss: 46.4484\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0748 - val_loss: 46.3639\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0796 - val_loss: 46.4709\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0265 - val_loss: 46.4419\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0478 - val_loss: 46.3751\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1196 - val_loss: 46.5333\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0443 - val_loss: 46.4567\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0924 - val_loss: 46.3710\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0660 - val_loss: 46.5899\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0738 - val_loss: 46.4055\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0156 - val_loss: 46.4328\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0368 - val_loss: 46.5207\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0174 - val_loss: 46.4437\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0110 - val_loss: 46.3881\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0253 - val_loss: 46.4583\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0829 - val_loss: 46.6571\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0468 - val_loss: 46.4494\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0153 - val_loss: 46.4264\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0937 - val_loss: 46.6121\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0103 - val_loss: 46.3934\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0140 - val_loss: 46.3897\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0305 - val_loss: 46.3680\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1070 - val_loss: 46.3620\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0241 - val_loss: 46.4061\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0135 - val_loss: 46.4885\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0933 - val_loss: 46.2820\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0547 - val_loss: 46.4201\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0316 - val_loss: 46.4737\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 24.0572 - val_loss: 46.4735\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0675 - val_loss: 46.4953\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0347 - val_loss: 46.4262\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0944 - val_loss: 46.5516\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0375 - val_loss: 46.4925\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0654 - val_loss: 46.4206\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0278 - val_loss: 46.4822\n",
      "\n",
      "Mean Squared Error for iteration46: 45.244874206519526\n",
      "\n",
      "Iteration:  47\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.0297 - val_loss: 46.3925\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0517 - val_loss: 46.2942\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0302 - val_loss: 46.4161\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0860 - val_loss: 46.3398\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0414 - val_loss: 46.6066\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0808 - val_loss: 46.4252\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0430 - val_loss: 46.4442\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0313 - val_loss: 46.4641\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0631 - val_loss: 46.4185\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0522 - val_loss: 46.3961\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0389 - val_loss: 46.5284\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0419 - val_loss: 46.4320\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0272 - val_loss: 46.4369\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0362 - val_loss: 46.3860\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0555 - val_loss: 46.3222\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0099 - val_loss: 46.4546\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0140 - val_loss: 46.4875\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0466 - val_loss: 46.4949\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0420 - val_loss: 46.5844\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0400 - val_loss: 46.3067\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0479 - val_loss: 46.3302\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0734 - val_loss: 46.5927\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0486 - val_loss: 46.5217\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0478 - val_loss: 46.3762\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0379 - val_loss: 46.3764\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0329 - val_loss: 46.4496\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0778 - val_loss: 46.7146\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0340 - val_loss: 46.2807\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0381 - val_loss: 46.3565\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0586 - val_loss: 46.4796\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0167 - val_loss: 46.4409\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0300 - val_loss: 46.2812\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 77us/step - loss: 24.0106 - val_loss: 46.3990\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0585 - val_loss: 46.3800\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0928 - val_loss: 46.4150\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0587 - val_loss: 46.6361\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0712 - val_loss: 46.4615\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.0550 - val_loss: 46.4342\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 24.1178 - val_loss: 46.3993\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0012 - val_loss: 46.4272\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0377 - val_loss: 46.4505\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0225 - val_loss: 46.4140\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0433 - val_loss: 46.3913\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0110 - val_loss: 46.4383\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0197 - val_loss: 46.5418\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0233 - val_loss: 46.5016\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0594 - val_loss: 46.4198\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 23.9940 - val_loss: 46.4039\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0432 - val_loss: 46.4828\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0781 - val_loss: 46.4450\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0882 - val_loss: 46.3348\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0287 - val_loss: 46.5144\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0246 - val_loss: 46.6011\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0212 - val_loss: 46.3979\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0460 - val_loss: 46.3512\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0252 - val_loss: 46.3597\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0420 - val_loss: 46.4184\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0273 - val_loss: 46.3660\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0463 - val_loss: 46.3849\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0152 - val_loss: 46.5504\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0117 - val_loss: 46.5839\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0470 - val_loss: 46.4098\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0030 - val_loss: 46.4054\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0109 - val_loss: 46.4639\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0040 - val_loss: 46.4661\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0046 - val_loss: 46.4526\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0523 - val_loss: 46.3078\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0107 - val_loss: 46.4024\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0232 - val_loss: 46.4991\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0574 - val_loss: 46.5227\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0161 - val_loss: 46.3194\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0138 - val_loss: 46.4744\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0185 - val_loss: 46.3885\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0446 - val_loss: 46.3482\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1231 - val_loss: 46.4525\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0380 - val_loss: 46.4900\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0033 - val_loss: 46.4080\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0553 - val_loss: 46.3807\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0035 - val_loss: 46.4166\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0047 - val_loss: 46.4467\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0178 - val_loss: 46.3680\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0130 - val_loss: 46.3685\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0439 - val_loss: 46.3272\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 24.0370 - val_loss: 46.4890\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0446 - val_loss: 46.5964\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0207 - val_loss: 46.5062\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0881 - val_loss: 46.4215\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0320 - val_loss: 46.4760\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0289 - val_loss: 46.4136\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0348 - val_loss: 46.3174\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0399 - val_loss: 46.3303\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0564 - val_loss: 46.5419\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0262 - val_loss: 46.3768\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0138 - val_loss: 46.3391\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0306 - val_loss: 46.3274\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0037 - val_loss: 46.4314\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0257 - val_loss: 46.4322\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0775 - val_loss: 46.5074\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0040 - val_loss: 46.3789\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0423 - val_loss: 46.4159\n",
      "\n",
      "Mean Squared Error for iteration47: 45.12453308255089\n",
      "\n",
      "Iteration:  48\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0770 - val_loss: 46.3472\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9941 - val_loss: 46.4118\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0379 - val_loss: 46.4110\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0332 - val_loss: 46.3702\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0183 - val_loss: 46.3577\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0282 - val_loss: 46.4301\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0412 - val_loss: 46.4978\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0045 - val_loss: 46.4110\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9955 - val_loss: 46.4108\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 74us/step - loss: 24.0132 - val_loss: 46.4897\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0998 - val_loss: 46.5030\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0023 - val_loss: 46.3926\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0526 - val_loss: 46.3880\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0218 - val_loss: 46.2547\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0259 - val_loss: 46.4534\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0727 - val_loss: 46.3450\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0144 - val_loss: 46.4258\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0252 - val_loss: 46.4240\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0678 - val_loss: 46.3523\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 69us/step - loss: 24.0256 - val_loss: 46.3572\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0050 - val_loss: 46.3288\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0395 - val_loss: 46.3632\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0106 - val_loss: 46.3048\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0053 - val_loss: 46.3195\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0399 - val_loss: 46.3112\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0140 - val_loss: 46.4089\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0598 - val_loss: 46.4664\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0186 - val_loss: 46.2939\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 24.0045 - val_loss: 46.4863\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0719 - val_loss: 46.3483\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0612 - val_loss: 46.3507\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0467 - val_loss: 46.5753\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0698 - val_loss: 46.3475\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0599 - val_loss: 46.3923\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0210 - val_loss: 46.4266\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0253 - val_loss: 46.4272\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0262 - val_loss: 46.4353\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0167 - val_loss: 46.3783\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.1019 - val_loss: 46.3590\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 24.0865 - val_loss: 46.2816\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0294 - val_loss: 46.3706\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0803 - val_loss: 46.4916\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0033 - val_loss: 46.3355\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0659 - val_loss: 46.2577\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0161 - val_loss: 46.4531\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0749 - val_loss: 46.5067\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9960 - val_loss: 46.3827\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0236 - val_loss: 46.2976\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0344 - val_loss: 46.4136\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0173 - val_loss: 46.4188\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0232 - val_loss: 46.4112\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0119 - val_loss: 46.4264\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9879 - val_loss: 46.3486\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0428 - val_loss: 46.3247\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0543 - val_loss: 46.2444\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0540 - val_loss: 46.4185\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0652 - val_loss: 46.2795\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0033 - val_loss: 46.3855\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0264 - val_loss: 46.4216\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0387 - val_loss: 46.2926\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 24.0378 - val_loss: 46.5038\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0267 - val_loss: 46.3751\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9906 - val_loss: 46.4292\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0085 - val_loss: 46.3640\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9967 - val_loss: 46.4293\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0393 - val_loss: 46.3539\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0285 - val_loss: 46.4122\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0103 - val_loss: 46.4760\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0190 - val_loss: 46.2600\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0042 - val_loss: 46.2800\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0418 - val_loss: 46.3687\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0138 - val_loss: 46.3548\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0521 - val_loss: 46.2549\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 24.0423 - val_loss: 46.4000\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0519 - val_loss: 46.3117\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0225 - val_loss: 46.4413\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0059 - val_loss: 46.3943\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0840 - val_loss: 46.2605\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0539 - val_loss: 46.2957\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0218 - val_loss: 46.4785\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0479 - val_loss: 46.3431\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0314 - val_loss: 46.3672\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0409 - val_loss: 46.3024\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0421 - val_loss: 46.3842\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 24.0125 - val_loss: 46.4552\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0085 - val_loss: 46.4697\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0611 - val_loss: 46.2123\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 73us/step - loss: 24.0429 - val_loss: 46.4091\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0518 - val_loss: 46.2575\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0715 - val_loss: 46.4240\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0563 - val_loss: 46.3856\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0127 - val_loss: 46.2803\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0106 - val_loss: 46.3998\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0575 - val_loss: 46.4842\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0291 - val_loss: 46.3410\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0104 - val_loss: 46.4434\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0314 - val_loss: 46.3574\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9906 - val_loss: 46.2950\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0239 - val_loss: 46.4130\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0057 - val_loss: 46.2399\n",
      "\n",
      "Mean Squared Error for iteration48: 45.09243102056604\n",
      "\n",
      "Iteration:  49\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 23.9939 - val_loss: 46.3366\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0104 - val_loss: 46.3224\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9950 - val_loss: 46.3566\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0420 - val_loss: 46.2747\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0500 - val_loss: 46.4427\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0229 - val_loss: 46.3879\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 24.0556 - val_loss: 46.2515\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9993 - val_loss: 46.3124\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0191 - val_loss: 46.2407\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0267 - val_loss: 46.2841\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9923 - val_loss: 46.3698\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9993 - val_loss: 46.2817\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9947 - val_loss: 46.4211\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0241 - val_loss: 46.4133\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9875 - val_loss: 46.3916\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0987 - val_loss: 46.3401\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0285 - val_loss: 46.3460\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0543 - val_loss: 46.2992\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.0176 - val_loss: 46.3913\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 24.0145 - val_loss: 46.3054\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0339 - val_loss: 46.2907\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0198 - val_loss: 46.3397\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0615 - val_loss: 46.3056\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0168 - val_loss: 46.3925\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0113 - val_loss: 46.3314\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9939 - val_loss: 46.4009\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9871 - val_loss: 46.2801\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0229 - val_loss: 46.3084\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0186 - val_loss: 46.4300\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.0138 - val_loss: 46.4114\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0309 - val_loss: 46.2682\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0228 - val_loss: 46.2874\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0187 - val_loss: 46.3823\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0308 - val_loss: 46.4286\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9793 - val_loss: 46.3127\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0557 - val_loss: 46.2687\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0008 - val_loss: 46.3318\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0784 - val_loss: 46.3242\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0235 - val_loss: 46.2471\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0240 - val_loss: 46.3359\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0503 - val_loss: 46.4744\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9930 - val_loss: 46.3987\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0177 - val_loss: 46.3424\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 23.9982 - val_loss: 46.3825\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0031 - val_loss: 46.2942\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0004 - val_loss: 46.3784\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0443 - val_loss: 46.2173\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0511 - val_loss: 46.5026\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0014 - val_loss: 46.2406\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0188 - val_loss: 46.4028\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.0187 - val_loss: 46.2592\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0275 - val_loss: 46.2919\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0225 - val_loss: 46.2731\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0696 - val_loss: 46.3742\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0184 - val_loss: 46.2806\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0467 - val_loss: 46.4458\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0163 - val_loss: 46.3382\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0048 - val_loss: 46.2650\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9755 - val_loss: 46.3303\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9971 - val_loss: 46.4231\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0053 - val_loss: 46.3186\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0227 - val_loss: 46.3593\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0586 - val_loss: 46.4591\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 108us/step - loss: 23.9795 - val_loss: 46.3413\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 72us/step - loss: 24.0001 - val_loss: 46.3073\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0073 - val_loss: 46.1515\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0146 - val_loss: 46.1841\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0771 - val_loss: 46.5158\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0273 - val_loss: 46.2272\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0021 - val_loss: 46.2663\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0195 - val_loss: 46.1858\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0311 - val_loss: 46.2948\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0174 - val_loss: 46.2720\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 24.0190 - val_loss: 46.3569\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0414 - val_loss: 46.4029\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0114 - val_loss: 46.3303\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0292 - val_loss: 46.4260\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9880 - val_loss: 46.3027\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0298 - val_loss: 46.3862\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0265 - val_loss: 46.2796\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0728 - val_loss: 46.2367\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0251 - val_loss: 46.2610\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0101 - val_loss: 46.1570\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0197 - val_loss: 46.3144\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0377 - val_loss: 46.4253\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0564 - val_loss: 46.3039\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9984 - val_loss: 46.3218\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0138 - val_loss: 46.2537\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0176 - val_loss: 46.3454\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0404 - val_loss: 46.3402\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0082 - val_loss: 46.2474\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9907 - val_loss: 46.3486\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0183 - val_loss: 46.2698\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0408 - val_loss: 46.3418\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0277 - val_loss: 46.4231\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0101 - val_loss: 46.3946\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 24.0095 - val_loss: 46.2221\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0078 - val_loss: 46.2829\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0270 - val_loss: 46.3408\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 24.0174 - val_loss: 46.4590\n",
      "\n",
      "Mean Squared Error for iteration49: 45.19115006964412\n",
      "\n",
      "Iteration:  50\n",
      "Train on 576 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0193 - val_loss: 46.2991\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0095 - val_loss: 46.1758\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0563 - val_loss: 46.3328\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0129 - val_loss: 46.2731\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0449 - val_loss: 46.2943\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0187 - val_loss: 46.2890\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0024 - val_loss: 46.2887\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0034 - val_loss: 46.2699\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 104us/step - loss: 24.0441 - val_loss: 46.2190\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0168 - val_loss: 46.2833\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0249 - val_loss: 46.3287\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9790 - val_loss: 46.2817\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0038 - val_loss: 46.2831\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0429 - val_loss: 46.3301\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0270 - val_loss: 46.2211\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0118 - val_loss: 46.3620\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0566 - val_loss: 46.2235\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0031 - val_loss: 46.2855\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 24.0393 - val_loss: 46.3928\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.1306 - val_loss: 46.1301\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 23.9979 - val_loss: 46.3798\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0340 - val_loss: 46.3700\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0051 - val_loss: 46.2108\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0118 - val_loss: 46.2139\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0488 - val_loss: 46.0797\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9784 - val_loss: 46.3440\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 23.9925 - val_loss: 46.3879\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9925 - val_loss: 46.3417\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0029 - val_loss: 46.2614\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0058 - val_loss: 46.1709\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 23.9873 - val_loss: 46.2765\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0056 - val_loss: 46.2058\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0413 - val_loss: 46.4768\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0126 - val_loss: 46.2570\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0141 - val_loss: 46.3782\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 24.0079 - val_loss: 46.2147\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 23.9957 - val_loss: 46.2621\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0296 - val_loss: 46.2058\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 24.0262 - val_loss: 46.4312\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0068 - val_loss: 46.2697\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 24.0000 - val_loss: 46.2108\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 75us/step - loss: 24.0033 - val_loss: 46.2814\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0472 - val_loss: 46.2974\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0273 - val_loss: 46.1848\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9981 - val_loss: 46.3146\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0040 - val_loss: 46.2710\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 23.9826 - val_loss: 46.2363\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 23.9966 - val_loss: 46.2129\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9995 - val_loss: 46.2610\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9763 - val_loss: 46.1787\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0128 - val_loss: 46.2815\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0162 - val_loss: 46.3209\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 24.0135 - val_loss: 46.2747\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 24.0054 - val_loss: 46.2032\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9976 - val_loss: 46.1549\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0030 - val_loss: 46.2586\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0126 - val_loss: 46.2611\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0076 - val_loss: 46.2925\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 23.9868 - val_loss: 46.2265\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 70us/step - loss: 23.9958 - val_loss: 46.2445\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 24.0320 - val_loss: 46.2970\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 24.0221 - val_loss: 46.2499\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0031 - val_loss: 46.0535\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 24.0114 - val_loss: 46.2034\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 24.0211 - val_loss: 46.3115\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0105 - val_loss: 46.2066\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 23.9897 - val_loss: 46.2641\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0055 - val_loss: 46.2074\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9775 - val_loss: 46.2868\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0084 - val_loss: 46.3158\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 23.9680 - val_loss: 46.2956\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9880 - val_loss: 46.1571\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9759 - val_loss: 46.2518\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0297 - val_loss: 46.2170\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9723 - val_loss: 46.1867\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9900 - val_loss: 46.2223\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 23.9773 - val_loss: 46.2251\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 23.9778 - val_loss: 46.1867\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.0265 - val_loss: 46.2756\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9682 - val_loss: 46.1365\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 23.9939 - val_loss: 46.1652\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0445 - val_loss: 46.1722\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 23.9974 - val_loss: 46.1256\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9651 - val_loss: 46.2614\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 71us/step - loss: 23.9828 - val_loss: 46.2904\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 23.9582 - val_loss: 46.2413\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9612 - val_loss: 46.1765\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 24.0334 - val_loss: 45.9649\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9966 - val_loss: 46.2562\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 72us/step - loss: 24.0044 - val_loss: 46.2983\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9975 - val_loss: 46.1448\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 23.9918 - val_loss: 46.2999\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 24.0654 - val_loss: 46.1527\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 23.9949 - val_loss: 46.2592\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 24.1001 - val_loss: 46.4434\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 23.9483 - val_loss: 46.2156\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 23.9993 - val_loss: 46.0901\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 24.0391 - val_loss: 46.0299\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 23.9984 - val_loss: 46.2922\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 23.9910 - val_loss: 46.1006\n",
      "\n",
      "Mean Squared Error for iteration50: 45.09440933592454\n"
     ]
    }
   ],
   "source": [
    "mse_total = []\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    print('\\nIteration: ', i+1)\n",
    "    model.fit(X_train_norm, y_train, validation_split=0.2, epochs=100)\n",
    "    predict_yhat = model.predict(X_test_norm)\n",
    "    mse = mean_squared_error(y_test, predict_yhat)\n",
    "    print('\\n''Mean Squared Error for iteration{}: {}'.format(i+1, mse))\n",
    "    mse_total.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mse_total)\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model MSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Part A - Mean of MSE and Std. Dev.\n",
    "\n",
    "The mean MSE value is 50.59490996401415 and standard deviation for MSE is 1.0276902495212634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Part B - Mean of MSE and Std. Dev.\n",
    "\n",
    "The mean MSE value is 46.317387506891215 and standard deviation for MSE is 0.17963278002251176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean MSE value is 45.24129840451905 and standard deviation for MSE is 0.1993572807166656\n"
     ]
    }
   ],
   "source": [
    "#Calculating mean and standard deviation of MSE - Part C\n",
    "mean = np.mean(mse_total)\n",
    "std_dev=np.std(mse_total)\n",
    "\n",
    "print('The mean MSE value is {} and standard deviation for MSE is {}'.format(mean, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
